% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 21: Queueing Networks}
\author{}
\begin{document}
\maketitle

\section{Migration Processes}

\begin{cor}
Consider an M/M/s queue with Poisson$(\lambda)$ arrivals and each server having exponential service time exp$(\mu)$ service. If $\lambda > s \mu$, then the output process in steady state is Poisson$(\lambda)$.
\end{cor}
\begin{proof}
Let $X(t)$ denote the number of customers in the system at time $t$. Since M/M/s process is a birth and death process, it follows from the previous proposition that $\{X(t),~t \geq 0\}$ is time reversible. Now going forward in time, the time instants at which $X(t)$ increases by 1 are the arrival instants of a Poisson process. Hence, by time reversibility, the time Points at which $X(t)$ increases by 1 when we go backwards in time also constitutes a Poisson process. But these instants are exactly the departure instants of the forward process. Hence the result.
\end{proof}
\begin{lem}
For an ergodic M/M/1 queue in steady state, the following are true.
\begin{enumerate}
\item The number of customers present in the system at time $t$ is independent of the sequence of past departures.
\item For FCFS discipline, the waiting time spent in the system (waiting in the queue plus the service time) by a customer is independent of the departure process prior to its departure.
\end{enumerate} 
\end{lem} 
\begin{proof}
Proofs follow by looking at reversed process.
\begin{enumerate}
\item Since the arrival process is Poisson, the future arrivals are independent of the number of customers in the system at the current instant. Looking backwards in time, future arrivals are the past departures. Hence by time reversibility, the number of customers currently in the system are also independent of the past departures.
\item Consider the case when a customer arrives into the system at time $T_1$. The customer leaves at time $T_2>T_1$.  Since the service discipline is assumed first come first serve and the arrival is Poisson, it is seen that the waiting time $T_2-T_1$ is independent of the customers coming after $T_1$. Looking at the reversed process, we see that $T_2-T_1$  will be independent of the arrivals after $T_2$ for the reversed process. But this is just the departure process before time $T_2$. Hence the result.
\end{enumerate}
\end{proof}

\section{Tandem Queues}
Time reversibility of M/M/s queues can be used to study what is called as a tandem, or sequential queueing system. For instance, consider a two server queueing system. Service time of server $i$ is distributed exponential$(\mu_i)$. Customers arrive according to a Poisson$(\lambda)$ process to the server 1. After being served at server 1, customers join server  2 for its service. Assume there is infinite waiting room at both servers. Since the departure process of server 1 is Poisson, as discussed previously, the arrival process to server 2 is also Poisson$(\lambda)$. Time reversibility concept can be used to give a much more stronger result.

\begin{thm}
For the ergodic tandem queue in steady state, the following are true.
\begin{enumerate}
\item The number of customers $N_1, N_2$ present at server 1 and server 2 respectively, are independent, and
\begin{align*}
Pr\{N_1 = n_1, N_2 = n_2)\}={\rho_1}^{n_1}(1-\rho_1){\rho_2}^{n_2}(1-\rho_2).
\end{align*}
\item For FCFS discipline, the waiting time at server 1 is independent of the waiting time at server 2.
\end{enumerate}
\end{thm}
\begin{proof}
Proofs follow by looking at reversed process.
\begin{enumerate}
\item By part 1 of previous lemma, we have that the number of customers at server 1 is independent of the past departures of server 1. But past departures are same as the arrival to server 2. Thus follows the independence of the number of customers in both servers. The formula for the joint density follows from the independence and the formula for the limiting probabilities of an M/M/1 queue. 
\item By part 2 of the previous lemma, the waiting time of a customer at server 1 is independent of the past departures happening at server 1. But the past departures at server 1, in conjunction with the service times at server 2, determine customer's waiting time at server 2. Hence the result follows.
\end{enumerate}
\end{proof}
%\subsection{Applications of the Reversed Chain to Queueing Theory}
%The reversed chain can be quite a useful concept even when the process is not time reversible. To see this, we start with the following theorem. 
%\begin{thm}
%Let $Q$ denote the rate matrix for an irreducible continuous-time Markov chain. If we can find $Q^*$ of the same size as $Q$ and a vector $P \geq 0$ such that $\sum_i P_i =1$ and 
%\begin{align*}
%P_{i}q_{ij}^*=P_jq_{ji}^*, ~ i \neq j,
%\end{align*} and
%\begin{align*}
%\sum_{j \neq i}q_{ij}=\sum_{j \neq i}q_{ij}^*, i \geq 0,
%\end{align*} 
%then $Q^*$ is the rate matrix for the reversed Markov chain and $P_i$ are the limiting probabilities for both  chains.
%\end{thm}
%\begin{proof}
%Exercise.
%\end{proof}
\section{Network of Queues}
\begin{thm}
Consider irreducible Markov chain with transition matrix $P$. If one can find non-negative vector $\alpha$ and other transition matrix $P^*$ such that $\sum_j \alpha_j =1$ and $\alpha_iP_{ij}=\alpha_jP^*_{ji}$ then $\alpha$ is the stationary probability vector and $P^*$ is the transition matrix for the reversed chain.
\end{thm}
\begin{proof}
Summing $\alpha_iP_{ij}=\alpha_jP_{ji}^*$ over $i$ gives, $\sum_{i}\alpha_iP_{ij}=\alpha_j$. Hence $\alpha_i$s are the stationary probabilities of the forward and reverse process. Since $P_{ji}^*=\frac{\alpha_iP_{ij}}{\alpha_j}$, $P_{ij}^*$ are the transition probabilities of the reverse chain.
\end{proof} 

\begin{thm}
Let $Q$ denote the rate matrix for an irreducible Markov process. If we can find $Q^*$ of the same size as $Q$ and a vector $\pi \geq 0$ such that $\sum_i \pi_i =1$ and for $i \neq j \in I$, we have
\begin{align*}
\pi_{i}Q_{ij} =\pi_jQ_{ji}^*, \text{ and } &
\sum_{j \neq i}Q_{ij} =\sum_{j \neq i}Q_{ij}^*,
\end{align*} 
then $Q^*$ is the rate matrix for the reversed Markov chain and $\pi_i$ are the limiting probabilities for both  processes.
\end{thm}
%\begin{proof}
%Exercise.
%\end{proof}
\subsection{Jackson Network}
Consider a system of $k$ servers with independent service times distributed exponentially with rate $\mu_i$ for server $i \in [k]$.  
To each server, customers arrive from outside the system, according to Poisson$(r_i)$. Once a customer is served at server $i$, the customer joins server $j$ with probability $P_{ij}$, $\sum_{i}P_{ij} \leq 1$. The probability of the customer departing the system is $1-\sum_{j}P_{ij}$. If we denote $\lambda_i$ as the total rate at which the customers join $i$, then $\lambda_i$s can be obtained as a solution to,
\begin{align*}
\lambda_j=r_j+\sum_{i=1}^{k}\lambda_i P_{ij},~ j=1, \dots k.
\end{align*}
The model can be analysed by a stationary continuous-time Markov chain $\{X(t) \in \N_0^k: t \in \R\}$ with states $X(t) = (X_1(t),X_2(t), \dots X_k(k)) \in \N_0^k$, 
where $X_i(t)$ denotes the number of customers in server $i$ at time $t$. 
From the tandem queue results, we expect the customers at each server to be independent random variables. 
Let $n \in \N_0^k$, and we are interested in knowing the the joint probability,
\begin{align*}
\pi(n) &\triangleq \Pr\{X(t) = n\} = \Pr\{X_1(t) = n_1, \dots, X_k(t) = n_k\}.
\end{align*}
We write the marginal probabilities as 
\eq{
\pi(m_i) &\triangleq \Pr\{X_i(t) = m_i\} = \sum_{n \in \N_0^k: n_i = m_i }\pi(n).
}
%where $Pr(n_i)$ is the limiting probability that there are $n_i$ customers to serve at server $i$. 
Since, $X(t)$ is a CTMC, only a single transition takes place in any infinitesimal time interval. 
Let $X(t) = n$, then the possible transitions from this Markov process at time $t$ and the associated rates are the following.  
\begin{enumerate}[i\_]
\item An external arrival takes place in queue $i$, with rate 
\eq{
Q(n,n+e_i) &= r_i.
}
\item If $n_i > 0$, a service completes and a customer departs from queue $i$ exiting the system, with rate 
\eq{
Q(n,n-e_i) &= \mu_i(1 - \sum_{j=1}^kP_{ij}).
}
\item If $n_i > 0$, a service completes and a customer departs from queue $i$ and joins queue $j$, with rate 
\eq{
Q(n,n-e_i+e_j) &= \mu_iP_{ij}.
}
\end{enumerate}

\begin{thm}
If $\lambda_i < \mu_i$ for each $i \in [k]$, then the reversed stochastic process is a network process of the same type as original. 
That is, the reversed process $X(-t)$ is a CTMC with the generator matrix $Q^{\ast}$ given by the following.  
\begin{enumerate}[(i)]
\item The system has external Poisson arrivals to queue $i$ at rate $\lambda_i(1-\sum_jP_{ij})$. 
That is, 
\eq{
Q^{\ast}(n,n+e_i) &= \lambda_i(1-\sum_jP_{ij}).
}
\item The service times at queue $i$ are \textit{iid} exponential with rate $\mu_i$. 
\item  A departure from queue $j$ goes to queue $i$ with probability $\bar{P}_{ji}$ as given by
\begin{align*}
\bar{P}_{ji}=\frac{\lambda_i P_{ij}}{\lambda_j}.
\end{align*}
That is, if $n_j > 0$, then a customer joins queue $i$ after a service completion from queue $j$ with rate 
\eq{
Q^{\ast}(n,n-e_j+e_i) &= \mu_j\bar{P}_{ji} = \frac{\mu_j}{\lambda_j}\lambda_iP_{ij}.
}
The corresponding rate of customer departing the system after service completion from server $j$ is
\eq{
Q^{\ast}(n, n-e_j) &= \mu_j(1-\sum_{i=1}^k\bar{P}_{ji}) = \frac{\mu_j}{\lambda_j}(\lambda_j - \sum_{i=1}^k\lambda_iP_{ij}) = \frac{\mu_j}{\lambda_j}r_j.
}
\item For $\rho_i \triangleq \lambda_i/\mu_i$, the stationary distribution satisfies
\begin{align*}
\pi(n)= \prod_{i=1}^k\pi_i(n_i) = \prod_{i=1}^k\rho_i^{n_i}(1-\rho_i).
\end{align*}
\end{enumerate}
\end{thm}
\begin{proof} 
It suffices to check that the detailed balanced conditions hold with the candidate generator matrix $Q^{\ast}$ for the reversed process and the candidate distribution $\pi$. 
We first focus at the detailed balance equations associated with the external arrivals 
\eq{
\pi(n)Q(n,n+e_i) &= \pi(n+e_i)Q^{\ast}(n+e_i,n).
}
This equation holds since for each $n_i \in \N_0$, we have 
\eq{
\pi_i(n_i+e_i) &= \rho_i\pi_i(n_i).
}
Second for $n_i > 0$, we look at the detailed balanced equations corresponding the exits from the system from queue $i$,
\eq{
\pi(n)Q(n,n-e_i) &= \pi(n-e_i)Q^{\ast}(n-e_i,n). 
}
This equation holds since for each $n_i \in \N$, we have 
\eq{
\pi_i(n_i-e_i)\mu_i(1-\sum_{i=1}^kP_{ij}) &= \lambda_i(1-\sum_{i=1}^kP_{ij})\pi_i(n_i).
}
Finally for $n_i > 0$, we look at the detailed balanced equations corresponding the transitions where a customer joins queues $j$ after getting service from queue $i$,
\eq{
\pi(n)Q(n,n-e_i+e_j) &= \pi(n-e_i+e_j)Q^{\ast}(n-e_i+e_j,n). 
}
This equation holds since for each $n_i \in \N$, we have 
\eq{
\pi_i(n_i)\pi_j(n_j)\mu_iP_{ij} &= \pi_i(n_i-1)\pi_j(n_j+1)\mu_j\bar{P}_{ji}.
}
\end{proof}
% Consider transitions resulting from an outside arrival. Consider states $\underline{n}=(n_1,n_2 \hdots n_i, \hdots n_k)$ and $\underline{n}'=(n_1,\hdots n_i+1,\hdots n_k)$. Now
% \begin{align*}
% q_{n,n'}=r_i,
% \end{align*}
% and, if the conjecture is true,
% \begin{align*}
% q_{n',n}^*&=\mu_i(1-\sum_{j}\bar{P}_{ij})\\
% &=\mu_i \frac{(\lambda_i-\sum_{j}\lambda_j {P}_{ji})}{\lambda_i}\\
% &=\frac{\mu_i r_i}{\lambda_i}
% \end{align*}
% and 
% \begin{align*}
% P(\underline{n})=\Pi_j P_j(n_j),~ P(\underline{n}')=P_i(n_i+1)\Pi_{j \neq i}P_j(n_j).
% \end{align*}
% Hence, from the previous theorem, we need that
% \begin{align*}
%  r_i \Pi_j P_j(n_j)= \frac{\mu_i r_i}{\lambda_i}\Pi_{j \neq i}P_j(n_j).
% \end{align*} 
% That is,
% \begin{align*}
% P_i(n+1)= \frac{\lambda_i}{\mu_i}P_i(n)={(\frac{\lambda_i}{\mu_i})}^{n+1}P_i(0),
% \end{align*}
% and using the fact that 
% \begin{align*}
%\sum_{n=0}^{\infty}P_i(n)=1
% \end{align*}
% yields 
% \begin{align*}
%P_i(n) ={(\frac{\lambda_i}{\mu_i})}^n(1-\frac{\lambda_i}{\mu_i}).
%\end{align*}
%Thus $\frac{\lambda_i}{\mu_i}< 1$ and $P_i$ must be as given before for the conjecture to be true. Now consider transitions that result from a departure from server $j$ going to server $i$. That is, let $\underline{n}=(n_1,n_2 \hdots n_i, \hdots n_k)$ and $\underline{n}'=(n_1,\hdots n_i+1,\hdots n_j+1,\hdots n_k)$, where $n_j >0$. Since
%\begin{align*}
%q_{n,n'}=\mu_j P_{ji}
%\end{align*}
%and the conjecture yields
%\begin{align*}
%q_{n,n'}^*=\mu_i \bar{P}_{ij}.
%\end{align*}
%We need to show that 
%\begin{align*}
%P(\underline{n})\mu_j P_{ji}= P(\underline{n}')\mu_i \bar{P}_{ij}
%\end{align*}
%or, equivalently,
%\begin{align*}
%\lambda_j P_{ji}=\lambda_i \bar{P}_{ij}.
%\end{align*}
%which is the definition of $\bar{P}_{ij}$.
%The last lecture ended with a conjecture on the network of queues. First we state the following theorem.
%\begin{thm}
%Assuming that $\lambda_i < \mu_i$, for all $i$, in steady state, the number of customers at service $i$ are independent and the limiting probabilities are given by
%\begin{equation*}
%Pr(n_1,n_2, \hdots n_k) = \Pi_{i=1}^{k}{(\frac{\lambda_i}{\mu_i})}^n (1-\frac{\lambda_i}{\mu_i}).
%\end{equation*}
%\end{thm}
%Also, form the reversed chain, we have the following.
\begin{cor}
The process of customers departing the system from the server $i$, $i=1,2 \hdots k,$ are independent Poisson processes having respective rates $\lambda_i (1-\sum_j P_{ij} )$.
\end{cor}
\begin{proof}
We have already shown that in the reverse process, customers arrive to server $i$ from outside the system according to independent Poisson processes having rates $\lambda_i(1-\sum_{i}P_{ij}),~ i \geq 1$. Since an arrival from outside corresponds to a departure out of the system from server $i$ in the forward process, the result follows. 
\end{proof}


\section{The Erlang Loss Formula}
consider a queueing system in which there are $k$ servers and customers arrive according to a Poisson process with rate $\lambda$. If an arriving customers find all the $k$ servers the customer is lost (do not eneter the system). The service times of server are assumed to be distributed according to some general distribution $G$. Assume that $G$ has a density $g$. Let $\lambda(t)$ denote the hazard rate function. That is,
\begin{equation*}
\lambda(t)=\frac{g(t)}{\bar{G}(t)}
\end{equation*}
is the instantaneous probability density that a $t$ unit old service will end. Assume that the states are ordered. i.e. $\underline{x}=(x_1,x_2, \hdots x_n)$, $x_1 \leq x_2 \leq \hdots x_n$, where $x_n$ denote the service time of $n^{\text{th}}$ customer ($n \leq k$). The process of successive states will be a Markov process in the sense that the conditional distribution of any future state, given the present state and all the past states, will depend only on the present. Even though the process is not a continuous-time Markov chain, we can extend and use the theory so far developed to analyse the process.
\begin{cor}
The reverse process is also a $k-$ server loss system with service distribution $G$ in which arrivals occur according to a Poisson process with rate $\lambda$. The state at any time represents the ordered residual service times of customers in service currently.
\end{cor}  
\begin{proof}
We shall prove the above conjecture and obtain the limiting distribution. For any state $\underline{x}=(x_1,x_2 \hdots x_i, \hdots x_n)$ and $e_i(\underline{x})=(x_1,x_2 \hdots x_{i-1},x_{i+1} \hdots x_n)$. In the original process when the state is $\underline{x}$ it will instantaneously go to $e_i(\underline{x})$ with a probability density equal to $\lambda(x_i)$. Similarly, in the reversed process, we see that if the state is $e_i(\underline{x})$, then it will instantaneously go to $\underline{x}$ if a customer having service time $x_i$ instantaneously arrives. So,
\begin{flalign*}
\text{Forward}:~ \underline{x} \rightarrow e_i(\underline{x})~ \text{w.p. intensity~}\lambda(x_i);\\
\text{Reverse}:~ e_i(\underline{x}) \rightarrow ~ \underline{x} \text{with joint prob. intensity~}\lambda g(x_i).
\end{flalign*}
Hence if $p(\underline{x})$ represents the limiting density, in accordance with Theorem 1.5 (Lecture 14), we would need that
\begin{equation*}
p(\underline{x})\lambda(x_i)=P(e_i(\underline{x}))\lambda g(x_i),
\end{equation*}
or, since $\lambda(x_i)=g(x_i)/\bar{G}(x_i)$,
\begin{equation*}
p(\underline{x})=p(e_i(\underline{x}))\lambda G(x_i).
\end{equation*}
Letting  $i=1$ and iterating the above yields,
\begin{flalign*}
p(\underline{x})&=\lambda G(x_1)p(e_1(\underline{x}))\\
=&\lambda G(x_1)\lambda G(x_2)p(e_1(e_1(\underline{x})))\\
& \vdots\\
&=\Pi_{i=1}^{n}\lambda \bar{G}(x_i)P(\phi),
\end{flalign*}
where $P(\phi)$ is the limiting probability that the system is empty. Integrating over vector $\underline{x}$ yields
\begin{flalign*}
Pr(n~\text{in the system})&=P(\phi)\lambda^n {\int \int \hdots \int}_{x_1 \leq x_2 \hdots x_n}\Pi_{i=1}^n\bar{G}(x_i)dx_1 dx_2 \hdots dx_n\\
&=P(\phi)\frac{\lambda^n}{n!} {\int \int \hdots \int}_{x_1, x_2, \hdots x_n}\Pi_{i=1}^n\bar{G}(x_i)dx_1 dx_2 \hdots dx_n\\
&=P(\phi)\frac{{(\lambda E[S])}^n}{n!}, n 1,2 \hdots k, 
\end{flalign*}
where $E[S]=\int \bar{G}(x) dx$ is the mean service time. Upon using the fact that
\begin{equation*}
P(\phi)+\sum_{n=1}^k Pr(n~\text{in the system})=1.
\end{equation*}
we obtain
\begin{equation}
\label{EquilibriumDistribution}
 Pr(n~ \text{in the system})= \frac{ {(\lambda E[S])}^n /(n!)}{\sum_{i=0}^n {(\lambda E[S])}^i/(i!)}.
\end{equation}
This can be written as
\begin{equation*}
 Pr(\underline{x})= \frac{ \lambda^n \Pi_{i=1}^n \bar{G}(x_i)}{\sum_{i=0}^n {(\lambda E[S])}^i/(i!)}.
\end{equation*}
Observe that the conditional distribution of the ordered ages given that there are $n$ customers in the system is
\begin{flalign*}
Pr(\underline{x}|n~ \text{in the system})&=\frac{p(\underline{x}}{Pr(n~ \text{in the system})}\\
&n! \Pi_{i=1}^{n}\frac{\bar{G}(x_i)}{E[S]}.
\end{flalign*}
As $\bar{G}(x)/E[S]$ is just the density of the equilibrium distribution of $G$, if the conjecture is valid, the limiting distribution of the number of customers in the sysytem depends on $G$ only through its mean and given that there are $n$ customers in the system the ages are independnet and identically distributed according to the equilibrium distribution of $G$. To complete the proof of the conjecture, we must consider the transitions of the forward process from $\underline{x}$ to $(0,\underline{x})$ when $n<k$. Now 
\begin{flalign*}
& \text{Forward:} \underline{x} \rightarrow (0,\underline{x}) ~\text{with instantaneous density }~ \lambda;\\
&\text{Reverse:} (0,\underline{x})  \rightarrow  \underline{x} ~  \underline{x} ~\text{with probability  } 1.
\end{flalign*} 
Hence in conjunction with Theorem 1.5 (Lecture 14) we must verify that 
\begin{equation*}
p(\underline{x})\lambda=p(0,\underline{x}),
\end{equation*}
which follows from ref{EquilibriumDistribution} since $\bar{G}(0)=1$.
\end{proof}
We have thus proven the following:
\begin{thm}
The limiting distribution of the number of customers in the system is given by
\begin{equation}
Pr(n ~ \text{in the system}) = Pr(n~ \text{in the system})= \frac{ {(\lambda E[S])}^n /(n!)}{\sum_{i=0}^n {(\lambda E[S])}^i/(i!)}.
\end{equation}
and given that there are $n$ in the system the ages (or the residual times) of these $n$ are independent and identically distributed according to the equilibrium distribution of $G$.
\end{thm}
The model considered is often called the Erlang loss system and \ref{EquilibriumDistribution} is called is called the Erlang loss formula. By using the reversed process, we also have the following corollary.
\begin{cor}
In Erlang loss model the departure process (including both customers completing service and those that are lost) is a Poisson process at rate $\lambda$. 
\end{cor}
\begin{proof}
The corollary follows as in the reversed process arrivals of all customers (including that are lost) constitutes a Poisson process. 
\end{proof}
\end{document}
