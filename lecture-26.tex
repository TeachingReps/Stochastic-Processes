% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 26: Martingale Concentration Inequalities}
\author{}

\begin{document}
\maketitle
\section{Introduction}

\begin{lem}
\label{StoppingTimeBound}
If $\{X_n: n \in \N \}$ is  a submartingale and $N$ is a stopping time such that $\Pr\{N \leq n\}=1$ then
\begin{align*}
 \E X_1 \leq \E X_N \leq \E X_n.
\end{align*}
\end{lem}
\begin{proof}
It follows from optional stopping theorem that since $N$ is bounded, $\E[X_N] \geq \E[X_1]$. 
Now, since $N$ is a stopping time, we see that for $\{N = k\}$
\begin{align*}
\E[X_n|X_1, \ldots, X_N, N = k ]&= \E[X_n|X_1, \ldots, X_k, N=k] = \E[X_n|X_1, \ldots, X_k] \geq X_k = X_N.
\end{align*}
%where $(a)$ follows from the fact that $N$ is  a stopping time. 
Result follows by taking expectation on both sides.
\end{proof}

\begin{thm}[Kolmogorov's inequality for submartingales] If $\{X_n: n \in \N \}$ is a submartingale, then
\begin{align*}
\Pr\{\max\{X_1,X_2, \ldots, X_n\}>a\} &\leq \frac{\E[X_n]}{a},\text{ for } a>0.
\end{align*}
\end{thm}
\begin{proof}
We define a stopping time 
\begin{align*}
N &= \min\{i \in [n]: X_i >a\} \wedge n \leq n.
\end{align*} %, and define it to equal $n$ if $Z_i \leq a$ for %all $i=1, \hdots n$. 
It follows that, $\{\max\{X_1, \ldots, X_n\}>a\} = \{X_N > a\}$. 
Using this fact and Markov inequality, we get
\begin{align*}
\Pr\{\max\{X_1, \ldots, X_n\}>a\} &=\Pr\{X_N>a\} \leq \frac{\E[X_N]}{a}.
\end{align*}
Since $N \leq n$ is a bounded stopping time, result follows from the previous Lemma \ref{StoppingTimeBound}.
\end{proof}
\begin{cor}
\label{MartingaleBoundCor}
Let $\{X_n : n \in \N \}$ be a martingale. Then, for $a>0$ the following hold.
\begin{align*}
\Pr\{\max\{|X_1|, \ldots, |X_n|\}>a\} &\leq \frac{\E[|X_n|]}{a},\\
\Pr\{\max\{|X_1|, \ldots, |X_n|\}>a\} &\leq \frac{\E[X_n^2]}{a^2}.
\end{align*}
\end{cor}
\begin{proof}
The proof the above statements follow from and Kolmogorov's inequality for submartingales, and by considering the convex functions $f(x)=|x|$ and $f(x)=x^2$. 
\end{proof}

\begin{thm}[Strong Law of Large Numbers] Let $S_n$ be a random walk with iid step size $\{X_i: i \in \N\}$ with finite mean $\mu$. %, and let $S_n=\sum_{i=1}^{n}X_i$. 
Then
\begin{align*}
\Pr\left\{\lim_{n \in \N}\frac{S_n}{n} = \mu\right\} = 1.
\end{align*}
\end{thm}
\begin{proof}
We will prove the theorem under the assumption that the moment generating function $M(t)=\E[e^{tX}]$ for random variable $X$ exists. 
For a given $\epsilon>0$, we define
\begin{align*}
g(t) & \triangleq e^{t(\mu+\epsilon)}/M(t).
\end{align*}
Then, it is clear that $g(0) = 1$ and 
\begin{align*}
g'(0)& =\frac{ M(0)(\mu+\epsilon) -M'(0)  }{M^2(0)}=\epsilon >0.
\end{align*}
Hence, there exists a value $t_0>0$ such that $g(t_0)>1$. We now show that $S_n/n$ can be as large as $\mu+\epsilon$ only finitely often. 
To this end, note that
\begin{align}
\label{SnBound}
\left\{\frac{S_n}{n} \geq \mu+\epsilon \right\} &\subseteq \left\{ \frac{e^{t_0S_n}}{M(t_0)^n} \geq { g(t_0)}^n \right\}
\end{align}
However, $\frac{e^{t_0S_n}}{M^n(t_0)}$ is a product of independent non negative random variables with unit mean, and hence is a martingale. 
By martingale convergence theorem, we have 
\begin{align*}
\lim_{n \in \N } \frac{e^{t_0S_n}}{M^n(t_0)} ~\text{ exists and is finite}.
\end{align*}
Since $g(t_0) >1$, it follows from~\eqref{SnBound} that
\begin{align*}
\Pr\left\{\frac{S_n}{n} \geq \mu+\epsilon ~\text{for an infinite number of n} \right\} &= 0.
\end{align*}
Similarly, defining the function $f(t)=e^{t(\mu-\epsilon)}/M(t)$ and noting that since $f(0)=1$, $f'(0)=-\epsilon$, there exists a value $t_0<0$ such that $f(t_0)>1$, we can prove in the same manner that
\begin{align*}
\Pr\left\{\frac{S_n}{n} \leq \mu-\epsilon ~\text{for an infinite number of n} \right\} &= 0.
\end{align*}
Hence, result follows from combining both these results, and taking limit of arbitrary $\epsilon$ decreasing to zero.
%Hence
%\begin{align*}
%\Pr\left\{\mu-\epsilon < \frac{S_n}{n} < \mu+\epsilon ~\text{for all but a finite number of $n$} \right\} &=1,
%\end{align*}
%or, since the above is true for all $\epsilon>0$,
%\begin{align*}
%\Pr\left\{\lim_{n \in \N}\frac{S_n}{n} =\mu\right\} &= 1.
%\end{align*}
\end{proof}

\begin{defn}
A sequence of random variables $\{X_n : n \in \N\}$ with distribution functions $\{F_n: n \in \N \}$, 
is said to be \textbf{uniformly integrable} if for every $\epsilon>0$, there is a $y_\epsilon$ such that for each $n \in \N$
\begin{align*}
\E[|X|_n1\{|X_n| > y_{\epsilon}\}] = \int_{|x|>y_\epsilon}|x|dF_{n}(x) < \epsilon.
\end{align*}
\end{defn}
\begin{lem}
If $\{X_n : n \in \N\}$ is uniformly integrable then there exists finite $M$ such that $\E|X_n|<M$ for all $n \in \N$.
\end{lem}
\begin{proof}
Let $y_1$ be as in the definition of uniform integrability. Then
\begin{align*}
\E|X_n|&=\int_{|x|\leq y_1}|x|dF_n(x)+\int_{|x|>y_1}|x|dF_n(x) \leq y_1+1.
\end{align*}
\end{proof}

\subsection{Generalized Azuma Inequality}
\begin{lem}
For a zero mean random variable $X$ with support $[-\alpha, \beta]$ and any convex function $f$ 
\eq{
\E f(X) &\leq \frac{\beta}{\alpha + \beta}f(-\alpha) + \frac{\alpha}{\alpha+\beta}f(\beta).
}
\end{lem}
\begin{proof}
From convexity of $f$, any point $(X, Y)$ on the line joining points $(-\alpha, f(-\alpha)$ and $(\beta, f(\beta))$ is
\eq{
Y = f(-\alpha) + (X + \alpha)\frac{f(\beta)- f(-\alpha)}{\beta + \alpha} \geq f(X).
}
%It follows from convexity of $f$ that $Y \geq f(X)$ and taking expectations we obtain the result.
%\eq{
%f(X) & \leq  \frac{\beta}{\alpha + \beta}f(-\alpha) + \frac{\alpha}{\alpha+\beta}f(\beta) + X\frac{f(\beta)-f(-\alpha)}{\alpha + \beta}.
%}
Result follows from taking expectations on both sides.
\end{proof}
\begin{lem} 
For $\theta \in [0,1]$ and $\bar{\theta} = 1 - \theta$, we have
\eq{
\theta e^{\bar{\theta}x} + \bar{\theta}e^{-\theta x} \leq e^{x^2/8}.
}
\end{lem}
\begin{proof}
\end{proof}
\begin{prop}
Let $\{X_n : n \in \N \}$ be a zero-mean martingale with respect to filtration $\F$, 
such that for each $n \in \N$
\begin{align*}
-\alpha \leq X_n-X_{n-1} \leq \beta.
\end{align*}
Then, for any positive values $a$ and $b$
\begin{align*}
\label{GenAzumaLemma}
\Pr\{X_n \geq a+bn ~ \text{for some }n\} \leq \exp\left(-\frac{8ab}{({\alpha+\beta})^2}\right).
\end{align*}
\end{prop}
\begin{proof}
For $n \geq 0$, we define a random sequence $W_n \in \F_n$, such that
\begin{align*}
W_n = \exp\{c(X_n-a-bn)\} = W_{n-1}e^{-cb}\exp\{c(X_n-X_{n-1})\}.
\end{align*}
We will show that $W$ is a supermartingale with respect to the filtration $\F$. % for martingale $X$. 
%Since exponential is an invertible function, $\sigma(W_i, i \in [n]) = \sigma(X_i, i \in [n])$, and hence 
%Using the fact that knowledge of $W_1,W_2, \hdots W_{n-1}$ is equivalent to that of $X_1,X_2, \ldots, X_{n-1}$, we obtain that 
To this end, we observe 
\begin{align*}
\E[W_n|\F_{n-1}]&=W_{n-1}e^{-cb}\E[\exp\{c(X_n-X_{n-1})\}|\F_{n-1}].
\end{align*}
Using conditional Jensen's inequality for convex function $f(x) = e^x$, we obtain for $\theta = \alpha/(\alpha + \beta)$
\begin{align*}
\E[\exp\{c(X_n-X_{n-1})\}|\F_{n-1}]
&{\leq} \frac{\beta e^{-c\alpha} + \alpha e^{c\beta}}{\alpha+\beta} = \bar{\theta}e^{-c(\alpha+\beta)\theta} + \theta e^{c(\alpha+\beta)\bar{\theta}} {\leq} e^{c^2{(\alpha+\beta)}^2/8}.
\end{align*}
The second inequality follows from previous lemma with %$\theta=\alpha/(\alpha+\beta)$,
$x=c(\alpha+\beta)$. 
Fixing the value of $c$ as $c=8b/{(\alpha+\beta)}^2$ minimizes the right hand side inequality in the following, 
and we obtain 
\eq{
\E[W_n|\F_{n-1}] \leq W_{n-1} e^{-cb + \frac{c^2(\alpha+\beta)^2}{8}} = W_{n-1}. 
}
%and so $\{W_n: n \in \N_0\}$ is a supermartingale. 
For a fixed positive integer $k$, define the bounded stopping time $N$ by 
\begin{align*}
N= \min\{n: ~\text{either}~ X_n \geq a+bn ~ \text{or}~ n=k \}.
\end{align*} 
Now, using Markov inequality and optional stopping theorem, we get
\begin{align*}
\Pr\{X_N \geq a+bN\}&= \Pr\{W_N \geq 1\} \leq \E[W_N] \leq \E[W_0].
\end{align*}
But the above inequality is equivalent to
\begin{align*}
\Pr\{X_n \geq a+bn ~ \text{for some}~ n \leq k\} \leq e^{-8ab/{(\alpha+\beta)}^2}.
\end{align*}
Since, the choice of $k$ was arbitrary, result follow from letting $k \to \infty$. 
\end{proof}

\begin{thm}[Generalized Azuma inequality] 
Let $\{X_n : n \in \N_0\}$ be a zero-mean martingale, such that $-\alpha \leq X_n-X_{n-1} \leq \beta $ for all $n \in \N$. 
Then, for any positive constant $c$ and integer $m$:
\begin{align*}
\Pr\{X_n \geq nc\text{ for some }n \geq m\} &\leq \exp\left(-2mc^2/{(\alpha+\beta)}^2\right),\\
\Pr\{X_n \leq -nc\text{ for some }n \geq m\} &\leq \exp\left(-2mc^2/{(\alpha+\beta)}^2\right).
\end{align*}
\end{thm}
\begin{proof}
Observe that if there is an $n$ such that $n \geq m$ and $X_n \geq nc$ then for that $n, X_n \geq nc \geq mc/2+nc/2$. 
Using this fact and previous proposition for $a = mc/2$ and $b = c/2$, we get
\begin{align*}
\Pr\{X_n \geq nc\text{ for some }n \geq m\} &\leq \Pr\{X_n \geq mc/2+(c/2)n~ \text{for some} ~ n\}\leq \exp\left\{-\frac{8(mc/2)(c/2)}{(\alpha+\beta)^2}\right\}.
\end{align*}
%where the last inequality follows from Proposition \ref{GenAzumaLemma}. 
This proves first inequality, and second inequality follows by considering the martingale $\{-X_n: n \in \N_0\}$.
\end{proof}

%%% LECTURE IS TOO SHORT %%%  ADD MORE MATERIAL

\end{document}