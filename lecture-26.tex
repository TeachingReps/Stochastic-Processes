% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 23: Martingale Concentration Inequalities}
\author{}

\begin{document}
\maketitle
\section{Introduction}

\begin{lem}
\label{StoppingTimeBound}
If $\{X_n: n \in \N \}$ is  a submartingale and $N$ is a stopping time such that $\Pr\{N \leq n\}=1$ then
\begin{align*}
 \E X_1 \leq \E X_N \leq \E X_n.
\end{align*}
\end{lem}
\begin{proof}
It follows from optional stopping theorem that since $N$ is bounded, $\E[X_N] \geq \E[X_1]$. 
Now, since $N$ is a stopping time, we see that for $\{N = k\}$
\begin{align*}
\E[X_n|X_1, \ldots, X_N, N = k ]&= \E[X_n|X_1, \ldots, X_k, N=k] = \E[X_n|X_1, \ldots, X_k] \geq X_k = X_N.
\end{align*}
%where $(a)$ follows from the fact that $N$ is  a stopping time. 
Result follows by taking expectation on both sides.
\end{proof}

\begin{thm}[Kolmogorov's inequality for submartingales] If $\{X_n: n \in \N \}$ is a submartingale, then
\begin{align*}
\Pr\{\max\{X_1,X_2, \ldots, X_n\}>a\} &\leq \frac{\E[X_n]}{a},\text{ for } a>0.
\end{align*}
\end{thm}
\begin{proof}
We define a stopping time 
\begin{align*}
N &= \min\{i \in [n]: X_i >a\} \wedge n \leq n.
\end{align*} %, and define it to equal $n$ if $Z_i \leq a$ for %all $i=1, \hdots n$. 
It follows that, $\{\max\{X_1, \ldots, X_n\}>a\} = \{X_N > a\}$. 
Using this fact and Markov inequality, we get
\begin{align*}
\Pr\{\max\{X_1, \ldots, X_n\}>a\} &=\Pr\{X_N>a\} \leq \frac{\E[X_N]}{a}.
\end{align*}
Since $N \leq n$ is a bounded stopping time, result follows from the previous Lemma \ref{StoppingTimeBound}.
\end{proof}
\begin{cor}
\label{MartingaleBoundCor}
Let $\{X_n : n \in \N \}$ be a martingale. Then, for $a>0$ the following hold.
\begin{align*}
\Pr\{\max\{|X_1|, \ldots, |X_n|\}>a\} &\leq \frac{\E[|X_n|]}{a},\\
\Pr\{\max\{|X_1|, \ldots, |X_n|\}>a\} &\leq \frac{\E[X_n^2]}{a^2}.
\end{align*}
\end{cor}
\begin{proof}
The proof the above statements follow from and Kolmogorov's inequality for submartingales, and by considering the convex functions $f(x)=|x|$ and $f(x)=x^2$. 
\end{proof}

\begin{thm}[Strong Law of Large Numbers] Let $S_n$ be a random walk with iid step size $\{X_i: i \in \N\}$ with finite mean $\mu$. %, and let $S_n=\sum_{i=1}^{n}X_i$. 
Then
\begin{align*}
\Pr\left\{\lim_{n \in \N}\frac{S_n}{n} = \mu\right\} = 1.
\end{align*}
\end{thm}
\begin{proof}
We will prove the theorem under the assumption that the moment generating function $\psi(t)=\E[e^{tX}]$ for random variable $X$ exists. 
For a given $\epsilon>0$, we define
\begin{align*}
g(t) & \triangleq e^{t(\mu+\epsilon)}/\psi(t).
\end{align*}
Then, it is clear that $g(0) = 1$ and 
\begin{align*}
g'(0)& =\frac{ \psi(0)(\mu+\epsilon) -\psi'(0)  }{\psi^2(0)}=\epsilon >0.
\end{align*}
Hence, there exists a value $t_0>0$ such that $g(t_0)>1$. We now show that $S_n/n$ can be as large as $\mu+\epsilon$ only finitely often. 
To this end, note that
\begin{align}
\label{SnBound}
\left\{\frac{S_n}{n} \geq \mu+\epsilon \right\} &\subseteq \left\{ \frac{e^{t_0S_n}}{\psi(t_0)^n} \geq { g(t_0)}^n \right\}
\end{align}
However, $\frac{e^{t_0S_n}}{\psi^n(t_0)}$ is a product of independent non negative random variables with unit mean, and hence is a martingale. 
By martingale convergence theorem, we have 
\begin{align*}
\lim_{n \in \N } \frac{e^{t_0S_n}}{\psi^n(t_0)} ~\text{ exists and is finite}.
\end{align*}
Since $g(t_0) >1$, it follows from~\eqref{SnBound} that
\begin{align*}
\Pr\left\{\frac{S_n}{n} \geq \mu+\epsilon ~\text{for an infinite number of n} \right\} &= 0.
\end{align*}
Similarly, be defining the function $f(t)=e^{t(\mu-\epsilon)}/\psi(t)$ and noting that since $f(0)=1$, $f'(0)=-\epsilon$, there exists a value $t_0<0$ such that $f(t_0)>1$, we can prove in the same manner that
\begin{align*}
\Pr\left\{\frac{S_n}{n} \leq \mu-\epsilon ~\text{for an infinite number of n} \right\} &= 0.
\end{align*}
Hence, result follows from combining both these results, and taking limit of arbitrary $\epsilon$ decreasing to zero.
%Hence
%\begin{align*}
%\Pr\left\{\mu-\epsilon < \frac{S_n}{n} < \mu+\epsilon ~\text{for all but a finite number of $n$} \right\} &=1,
%\end{align*}
%or, since the above is true for all $\epsilon>0$,
%\begin{align*}
%\Pr\left\{\lim_{n \in \N}\frac{S_n}{n} =\mu\right\} &= 1.
%\end{align*}
\end{proof}

\begin{defn}
A sequence of random variables $\{X_n : n \in \N\}$ with distribution functions $\{F_n: n \in \N \}$, is said to be \textbf{uniformly integrable} if for every $\epsilon>0$, there is a $y_\epsilon$ such that
\begin{align*}
\int_{|x|>y_\epsilon}|x|dF_{n}(x) < \epsilon ~\forall n \in \N.
\end{align*}
\end{defn}
\begin{lem}
If $\{X_n : n \in \N\}$ is uniformly integrable then there exists finite $M$ such that $\E[|X_n|]<M$ for all $n \in \N$.
\end{lem}
\begin{proof}
Let $y_1$ be as in the definition of uniform integrability. Then
\begin{align*}
E[|X_n|]&=\int_{|x|\leq y_1}|x|dF_n(x)+\int_{|x|>y_1}|x|dF_n(x) \leq y_1+1.
\end{align*}
\end{proof}

\subsection{Generalized Azuma Inequality}
\begin{prop}
Let $\{X_n : n \in \N \}$ be a martingale with mean $X_0=0$, such that
\begin{align*}
-\alpha \leq X_n-X_{n-1} \leq \beta~ \forall ~n \in \N.
\end{align*}
Then, for any positive values $a$ and $b$
\begin{align*}
\label{GenAzumaLemma}
\Pr\{X_n \geq a+bn ~ \text{for some }n\} \leq \exp\left(-\frac{8ab}{({\alpha+\beta})^2}\right).
\end{align*}
\end{prop}
\begin{proof}
For $n \geq 0$, we define 
\begin{align*}
W_n = \exp\{c(X_n-a-bn)\} = W_{n-1}e^{-cb}\exp\{c(X_n-X_{n-1})\}.
\end{align*}
Since exponential is an invertible function, $\sigma(W_i, i \in [n]) = \sigma(X_i, i \in [n])$, and hence 
%Using the fact that knowledge of $W_1,W_2, \hdots W_{n-1}$ is equivalent to that of $X_1,X_2, \ldots, X_{n-1}$, we obtain that 
\begin{align*}
\E[W_n|W_1 \hdots W_{n-1}]&=W_{n-1}e^{-cb}\E[\exp\{c(X_n-X_{n-1})\}|X_1 \hdots X_{n-1}].
\end{align*}
Using Jensen's inequality for convex function $f(x) = e^x$, we obtain
\begin{align*}
\E[\exp\{c(X_n-X_{n-1})\}|X_1 \hdots X_{n-1}]
&{\leq} W_{n-1}e^{-cb}\frac{\beta e^{-c\alpha} + \alpha e^{c\beta}}{\alpha+\beta} {\leq} W_{n-1}e^{-cb}e^{c^2{(\alpha+\beta)}^2/8}.
\end{align*}
where second inequality follows from with $\theta=\alpha/(\alpha+\beta),~x=c(\alpha+\beta)$. 
Hence, fixing the value of $c$ as $c=8b/{(\alpha+\beta)}^2$ yields 
\begin{align}
E[W_n|W_1, \hdots W_{n-1}] \leq W_{n-1},
\end{align}
and so $\{W_n: n \in \N_0\}$ is a supermartingale. 
For a fixed positive integer $k$, define the bounded stopping time $N$ by 
\begin{align*}
N= \min\{n: ~\text{either}~ X_n \geq a+bn ~ \text{or}~ n=k \}.
\end{align*} 
Now, using Markov inequality and optional stopping theorem, we get
\begin{align*}
\Pr\{X_N \geq a+bN\}&= \Pr\{W_N \geq 1\} \leq \E[W_N] \leq \E[W_0].
\end{align*}
But the above inequality is equivalent to
\begin{align*}
\Pr\{X_n \geq a+bn ~ \text{for some}~ n \leq k\} \leq e^{-8ab/{(\alpha+\beta)}^2}.
\end{align*}
Letting $k \rightarrow \infty$ gives the result.
\end{proof}
\begin{thm}[Generalized Azuma Inequality] Let $\{X_n : n \in \N_0\}$ be a martingale with mean $X_0=0$, such that $-\alpha \leq X_n-X_{n-1} \leq \beta $ for all $n \in \N$. 
Then, for any positive constant $c$ and integer $m$:
\begin{align*}
\Pr\{X_n \geq nc\text{ for some }n \geq m\} &\leq \exp\left(-2mc^2/{(\alpha+\beta)}^2\right),\\
\Pr\{X_n \leq -nc\text{ for some }n \geq m\} &\leq \exp\left(-2mc^2/{(\alpha+\beta)}^2\right).
\end{align*}
\end{thm}
\begin{proof}
Observe that if there is an $n$ such that $n \geq m$ and $X_n \geq nc$ then for that $n, X_n \geq nc \geq mc/2+nc/2$. 
Using this fact and previous proposition for $a = mc/2$ and $b = c/2$, we get
\begin{align*}
\Pr\{X_n \geq nc\text{ for some }n \geq m\} &\leq \Pr\{X_n \geq mc/2+(c/2)n~ \text{for some} ~ n\}\leq \exp\left\{-\frac{8(mc/2)(c/2)}{(\alpha+\beta)^2}\right\}.
\end{align*}
%where the last inequality follows from Proposition \ref{GenAzumaLemma}. 
This proves first inequality, and second inequality follows by considering the martingale $\{-X_n: n \in \N_0\}$.
\end{proof}

\end{document}