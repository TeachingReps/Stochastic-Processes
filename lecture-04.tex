% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 04: Properties of Poisson Process}
\author{}

\begin{document}
\maketitle


\section{Characterizations of Poisson process}

It is clear that $t$ partitions $X_{N(t)+1}$ in two parts such that $X_{N(t)+1} = A(t) + Y(t)$ as seen in Figure~\ref{Fig:IndependentIncrements} for the case when $N(s) = n$. 
\begin{figure}[hhhh]
\center
\input{Figures/IndependentIncrements}
  \caption{Stationary and independent increment property of Poisson process.}
\label{Fig:IndependentIncrements}
\end{figure}

\begin{prop} A Poisson process $\{N(t), t\geqslant 0\}$ is simple counting process with stationary independent increments.
\end{prop}
\begin{proof}
It is clear that Poisson process is a simple counting process. 
To show that $N(t)$ has stationary and independent increments, it suffices to show that $N(t)-N(s)$ is independent of $N(s)$ and the distribution of increment $N(t) - N(s)$ is identical to that of $N(t-s)$. 
This follows from the fact that we can use induction to show stationary and independent increment property for for any finite disjoint time-intervals. 
%Let arrival time-instants $\{S_n: n \in \N_0\}$ and inter-arrival times $\{X_n: n \in \N\}$ be defined as before. 
%Given any time $s$, we can define the following variables
%\begin{xalignat*}{3}
%&A(s) = s - S_{N(s)},&&Y(s) = S_{N(s)+1} - s.
%\end{xalignat*}
%It is clear that $s$ partitions $X_{N(s)+1}$ in two parts such that $X_{N(s)+1} = A(s) + Y(s)$ as seen in Figure~\ref{Fig:IndependentIncrements} for the case when $N(s) = n$. 

We can write the distribution of $N(t)-N(s)$ given $N(s)$ in terms of the following events involving inter-arrival times and excess times as 
\begin{align*}
%\{ N(s) = n \} &\iff  \{ S_n + A(s)  = s\}, \\
P\{ N(t) - N(s) \geqslant m | N(s) = n \} &= P\{ Y(s) + S_{n+m} - S_{n+1} \leqslant t - s | S_n + A(s) = s \}.
\end{align*}
Further, we see that independent increment holds only if inter-arrival time is exponential. 
%Therefore, 
%\begin{align*}
%\{ N(s) = n \} &\iff  \{ S_n = s + A(s) \}, \\
%\{ N(t) - N(s) \geqslant m \} &\iff \{ Y(s) + \sum_{i=n+2}^{n+m} X_i \leqslant t - s \}.
%\end{align*}
Since, $\{X_i: i \geqslant n+2\}\cup\{Y(s)\}$ are independent of $\{X_i: i \leqslant n\}\cup{A(s)}$, we have $N(t)-N(s)$ independent of $N(s)$. 
Further, since $Y(s)$ has same distribution as $X_{n+1}$, we get $N(t) - N(s)$ having same distribution as $N(t-s)$. 
By induction, we can extend this result to $(N(t_{n})-N(t_{n-1}),...,N(s))$. 
\end{proof}

\begin{thm}[Characterization 1] A simple counting process with stationary and independent increment is a Poisson process with parameter $\lambda$ when
\begin{xalignat*}{3}
&\lim_{t \downarrow 0}\frac{P\{N(t) = 1\}}{t} = \lambda,&&\lim_{t \downarrow 0}\frac{P\{N(t) \geq 2\}}{t} = 0.
\end{xalignat*}
\end{thm}
\begin{proof}
It suffices to show that first inter-arrival times $X_1$ is exponentially distributed with parameter $\lambda$. 
Notice that, the probability $P_0(t)$ of no arrivals in a time duration $[0,t)$ satisfies the semi-group property. 
That is,
\begin{align*}
P_0(t+s) = P\{N(t+s) - N(t) = 0, N(t) = 0\} = P_0(t)P_0(s).
\end{align*}
Using the conditions in the theorem, the result follows.
\end{proof}



%
%%\begin{rem} 
%A Poisson process is not a stationary process. That is, the finite dimensional distributions are not shift invariant. 
%This is clear from looking at the first moment, which is a function of time. 
%%\end{rem}
%%In the following section, we show that the Poisson process is a \textit{stationary,  independent increment} process. To this end, we will use an important property of exponential distribution- namely memoryless property. Memoryless property of exponential distribution will facilitate the computation of fdd of the Poisson process via one dimensional marginal distribution. 
%\begin{lem} For any finite time $t > 0$, a Poisson process is finite almost surely.
%\end{lem}
%\begin{proof} By strong law of large numbers, we have 
%\begin{align*}
%\lim_{n \to \infty} \frac{S_{n}}{n} = E[X_{1}] = \frac{1}{\lambda}\quad\mathrm{a.s.} 
%\end{align*}
%%Therefore, we have $S_n \rightarrow \infty$, a.s. This implies $P\{N(t) < \infty\} =1$. To see this, 
%Fix $t > 0$ and let $M = \{\omega \in \Omega: N(t)(\omega) = \infty \}$ be a subset of the sample space. Let $\omega \in M$, then $S_{n}(\omega)\leqslant t$ for all $n \in \N$. This implies $\lim\sup_n\frac{S_{n}}{n} = 0$  and $\omega \not\in \{\lim_n \frac{S_{n}}{n} = \frac{1}{\lambda} \}.$ Hence, the probability measure for set $M$ is zero. 
%\end{proof}
%
%\begin{prop}[Characterization 2] 
%Let $\{I_i \subseteq \R_+: i \in [k]\}$ be a finite collection of disjoint intervals. 
%A stationary independent increment simple point process $\{N(t),~t\geqslant 0\}$, such that $N(0) = 0$ is Poisson process iff 
%%\begin{figure}[h!]
%%\center
%  %% Requires \usepackage{graphicx}
%  %\includegraphics[width=2.8in, height=0.9in]{./Figures/SPQT.png}\\
% %% \caption{}\label{}
%%\end{figure}
%\begin{align*}
%  P\bigcap_{i=1}^k \{N(I_i)= n_{i}\} = \prod_{i=1}^{k}\frac{(\lambda|I_i|)^{n_{i}}}{n_{i}!} e^{-\lambda |I_i|}.
%\end{align*}
%\end{prop}

%In the following section, we show that the Poisson process is a \textit{stationary,  independent increment} process. To this end, we will use an important property of exponential distribution- namely memoryless property. Memoryless property of exponential distribution will facilitate the computation of fdd of the Poisson process via one dimensional marginal distribution. 


\begin{prop}[Characterization 2] 
Let $\{I_i \subseteq \R_+: i \in [k]\}$ be a finite collection of disjoint intervals. 
A stationary and independent increment simple counting process $\{N(t),~t\geqslant 0\}$ with $N(0) = 0$ is Poisson process iff 
%\begin{figure}[h!]
%\center
  %% Requires \usepackage{graphicx}
  %\includegraphics[width=2.8in, height=0.9in]{./Figures/SPQT.png}\\
 %% \caption{}\label{}
%\end{figure}
\begin{align*}
  P\bigcap_{i=1}^k \{N(I_i)= n_{i}\} = \prod_{i=1}^{k}\frac{(\lambda|I_i|)^{n_{i}}}{n_{i}!} e^{-\lambda |I_i|}.
\end{align*}
\end{prop}
\begin{proof} 
It is clear that Poisson process satisfies the above conditions. 
Further, since $P\{N(t)=0\} = e^{-\lambda t}$, it follows that the counting process with stationary and independent increment is Poisson with rate $\lambda$.  
\end{proof}

\begin{prop}\label{Prop:SIIPoisson}
Let $\{N(t), t\geqslant 0\}$ be a Poisson process with $\{I_i \subseteq \R_+: i \in [n]\}$ a set of finite disjoint intervals with $I = \cup_{i \in [n]}I_i$, and $\{k_i \in \N_0: i \in [n]\}$ and $k = \sum_{i \in [n]}k_i$. 
Then, we have 
\begin{align*}
P\{N(I_i) = k_i, i\in [n] | N(I) = k\} &= k!\prod_{i \in [n]}\frac{1}{k_i!}\left(\frac{|I_i|}{|I|}\right)^{k_i}.
\end{align*}
\end{prop}
\begin{proof}
It follows from the stationary and independent increment property of Poisson processes that
\begin{align*}
P\{N(I_i) = k_i, i\in [n] | N(I) = k\} &= \frac{P\bigcap_{i \in [n]}\{N(I_i) = k_i\}}{P\{N(I) = k\}} = \frac{\prod_{i \in [n]}P\{N(I_i) = k_i\}}{P\{N(I)= k\}}.
\end{align*}
\end{proof}

\subsection{Conditional distribution of arrivals}
\begin{prop} 
For a Poisson process $\{N(t), t\geqslant 0\}$, distribution of first arrival instant $S_1$ conditioned on $\{N(t)=1\}$ is uniform between $[0,t)$.
\end{prop}
\begin{proof} If $N(t) = 1$, then we know that conditional distribution of $S_1$ is supported on $[0,t)$. By Proposition~\ref{Prop:SIIPoisson}, we see that
\begin{align*}
P\{S_1 \leq s | N(t) = 1\} &= P\{ N(s) = 1, N(t-s) = 0 | N(t) = 1\}1{\{s < t\}} + 1\{s \geq t\} = \frac{s}{t}1{\{s < t\}} + 1\{s \geq t\}.
\end{align*}
\end{proof}
%\begin{proof}[Alternative proof]
%For any $0 \leq u < t$, we can write $\{S_1 = u, N(t) = 1\}$ as intersection of two independent events, %as follows
%\begin{align*}
%\{S_1 = u, N(t) = 1\} \iff \{S_1 = u\}\cap\{X_2 > t - u\}.
%\end{align*}
%Therefore, integrating LHS with respect to $u$ in interval $[0,s]$ for $s < t$, we obtain
%\begin{align*}
%P\{S_1 \leq s, N(t) = 1\} = \int_{0}^{s}du \lambda \exp(-\lambda u)\exp(-\lambda (t-u)) = s\lambda\exp(-\lambda t).
%\end{align*}
%Since $P\{N(t) = 1\} = \lambda t \exp(-\lambda t)$, it follows that 
%\begin{align*}
%P\{S_1 \leq s| N(t) = 1\} = \begin{cases}\frac{s}{t}, & s < t\\ 0, & s \geq t.\end{cases}.
%\end{align*}
%%\noindent For the  with rate $0<\lambda< \infty$, we would like to find the density.
%%\begin{eqnarray*}
%%% \nonumber to remove numbering (before each align)
%  %P(S_{1}\mid N(t)=1), t>0 \\
%  %P_{S_1}(S \mid N(t)=1) \\
%  %P(s\leq S_{1} \leq s+h \mid N(t)=1)\\
%   %&\approx& h P_{S_1}(S \mid N(t)=1 \\
%  %P(s\leq S_{1}< s+h \mid N(t)=1)\\
%   %&=& \frac{P(s\leq S_{1}< s+h,N(t)=1)}{P(N(t)=1)}\\
%   %&=& \frac{P\{s\leq S_{1}<s+h,X_{2}>t-(s+h))}{\lambda t e^{-\lambda t}} \\
%   %&=& \frac{h \lambda e^{-\lambda h} e^{-\lambda (t-(s+h))}}{h{\lambda t} e^{-\lambda t}}.
%   %\end{eqnarray*}
%   %Take  $\lim _{h\rightarrow 0}\frac{1}{t}{e^{-\lambda h}} =\frac{1}{t}.$ Thus, the conditional density is the density of a uniform random variable distributed over the interval $[0,t].$ 
%\end{proof}
%This property of Poisson process can be generalized for any set of arrival instants.
\begin{prop}%[Conditional Distribution of Arrival Instants] 
For a Poisson process $\{N(t), t\geqslant 0\}$, joint distribution of arrival instant $\{S_1, \ldots, S_n\}$ conditioned on $\{N(t)=n\}$ is identical to joint distribution of order statistics of $n$ \textit{iid} uniformly distributed random variables between $[0,t]$.
\end{prop}
%\begin{proof} Let $\{ s_0 = 0 < s_1 < s_2 <\ldots < s_n < t \}$ be a finite sequence of non-negative increasing numbers between $0$ and $t$. Then, by Proposition~\ref{Prop:SIIPoisson}, we get
%\begin{align*}
%P\{S_i \leq s_i, i \in [n] | N(t) = n\} &= P\{N((0, s_i]) \geq i, i \in [n] | N(t) = n\}.
%%\\&=\frac{1}{t^n}\sum_{a_1 + \ldots + a_n = n, a_i \geq i} \prod_{i \in [n]}\frac{(s_i - s_{i-1})^{a_i}}{a_i!}.
%\end{align*}
%\end{proof}
\begin{proof} 
Let $\{ s_i \in (0, t) : i \in [n]\}$ be a sequence of increasing numbers. If we denote $s_{0} = 0$, then we can write 
\begin{align*}
\bigcap_{i=1}^n\{S_i = s_i\}\cap\{N(t) = n\} \iff \bigcap_{i=1}^n\{X_i = s_i - s_{i-1}\}\cap\{X_{n+1} > t - s_n\}.
\end{align*}
Note that all the events on RHS are independent. 
Hence, it is easy to compute the joint distribution of $\{S_1,\ldots, S_n\}$ as 
\begin{align*}
P\bigcap_{i=1}^n\{S_i \leq s_i\}\cap\{N(t) = n\} &= \int_{0}^{s_1}du_1\cdots\int_{0}^{s_n}du_n \prod_{i=1}^n\lambda \exp(-\lambda (u_i-u_{i-1})\exp(-\lambda (t-u_n))\\
&= \lambda^n\exp(-\lambda t)\prod_{i=1}^ns_i.
\end{align*}
Since $P\{N(t) = n\} = \exp(-\lambda t)(\lambda t)^n/n! $, it follows that 
\begin{align*}
P\{S_1 \leq s_1,\ldots, S_n\leq s_n | N(t) = n\} = 
\begin{cases}
n!\prod_{i=1}^n\frac{s_i}{t} & s < t\\ 
0 & s \geq t.
\end{cases}
\end{align*}
%\begin{eqnarray*}
%% \nonumber to remove numbering (before each align)
    %&P\{s_{1}\leq S_{1}<s_{1}+h, s_{2}\leq S_{2}<s_{2}+h, ...s_{n}\leq S_{n}\leq s_{n}+h \mid N(t)=n]  \\
   %&=   \frac{P\{s_{1}\leq S_{1}<s_{1}+h, s_{2}\leq S_{2}<s_{2}+h,...,s_{n}\leq S_{n}\leq s_{n}+h,N(t)=n]}{P\{N(t)=n]} \\
   %&=  \frac{P\{s_{1} \leq S_{1}<s_{1}+h_{1},s_{2}-s_{1}\leq X_{2}<s_{2}- (s_{1}+h),..., X_{n+1}> t-s_{n}]}{P\{N(t)=n]} \\
   %&\stackrel{(a)}{=}  \frac{P\{s_{1 \leq S_{1}<s_{1}+h}]P\{s_{2}-s_{1}\leq X_{2}<s_{2}-s_{1}+h] P\{X_{n+1}>t -s_{n}]}{P\{N(t)=n]}\\
   %&=  \frac{\lambda h e^{-s_{1}\lambda}h \lambda e^{-(s_{2}-s_{1})\lambda},..., e^{-(t-s_{n})\lambda}}{(\lambda t)^{n}\frac{e^{-\lambda t}}{n!}} 
   %\end{eqnarray*}
   %\begin{eqnarray*}
    %\lim _{h\rightarrow 0} \frac{ \lambda h e^{-s_{1}\lambda}h \lambda e^{-(s_{2}-s_{1})\lambda},... e^{-(t-s_{n})\lambda}}{h^{n}(\lambda t)^{n}\frac{e^{-\lambda t}}{n!}}=\frac{n!}{t^{n}}.
%\end{eqnarray*}
%where (a) follows as $X_i$s are independent. 
Let $U_{1},\ldots,U_{n}$ be \textit{iid} uniform random variables in $[0,t]$. Then, the order statistics  of $U_1 \ldots, U_n$ has an identical joint distribution to $n$ arrival instants conditioned on $\{N(t)=n\}$.\end{proof}

%\section{Age and excess time}
%\begin{defn} For a point process $\{N(t), t \geqslant 0\}$, we can define age process $\{A(t), t \geqslant 0\}$ and excess time process $\{Y(t), t \geqslant 0\}$ as 
%\begin{xalignat*}{3}
%&A(t) = t - S_{N(t)},&&Y(t) = S_{N(t)+1} - t.
%\end{xalignat*}
%\end{defn}
%\begin{prop}
%For a Poisson process with rate $\lambda$, the corresponding age and excess time are both exponentially distributed with rate $\lambda$ irrespective of time $t$.
%\end{prop}
%\begin{proof} Using stationary independent increment property of Poisson process, we can write complementary distribution of excess time process as 
%\begin{align*}
%P\{Y(t) > y\} &= \sum_{n \in \N_0}P\{Y(t) > y, N(t) = n\} = \sum_{n \in \N_0}P\{N(t+y) - N(t) = 0, N(t) = n\}\\
%&= P\{N(y) = 0\}\sum_{n \in \N_0}P\{N(t) = n\} = P\{N(y) = 0\}.
%\end{align*}
%Similarly, we can write complementary distribution for the age process as
%\begin{align*}
%P\{A(t) \geq x\} &= \sum_{n \in \N_0}P\{A(t) \geq x, N(t) = n\} = \sum_{n \in \N_0}P\{N(t) - N(t-x) = 0, N(t) = n\}\\
%&= \sum_{n \in \N_0}P\{N(t-x) = n\}P\{N(x) = 0\} = P\{N(x) = 0\}.
%\end{align*}
%\end{proof}
%We give some more properties of the Poisson process.
\section{Superposition and decomposition of Poisson processes}
\begin{thm}[Sum of Independent Poissons] Let $\{N_1(t), t \geqslant  0\}$ and $\{N_2(t), t \geqslant  0\}$ be two independent Poisson processes with rats $\lambda_{1}$ and $\lambda_{2}$ respectively. Then, the process $N(t)= N_{1}(t) +N_{2}(t)$ is Poisson with rate $\lambda_{1}+\lambda_{2}$.
\end{thm}
\begin{proof} We need to show that $\{N(t)\}$ has stationary independent increments, and 
\begin{align*}
	P\{N(t)=n\}=   \exp(-(\lambda_{1}+\lambda_{2})t)\frac{(\lambda_{1}+\lambda_{2})^n t^n}{n!}.
\end{align*}
For two disjoint interval $(t_{1}, t_{2})$ and $(t_3,t_4)$, we can see that for both processes $N_{1}(t)$ and $N_2(t)$,  arrivals in $(t_{1}, t_{2})$ and $(t_3,t_{4})$ are independent. Therefore, $N(t)$ has independent increment property. Similarly, we can argue about the stationary increment property of $\{N(t)\}$. Further, we can write 
\begin{align*}
	\{N(t)=n\} = \bigcup_{k=0}^n\{\{N_1(t) = k\}\cap\{N_2(t) = n-k\}\}.
\end{align*}
Since $N_1(t)$ and $N_2(t)$ are independent, we can write
\begin{align*}
	P\{N(t)=n\} &= \sum_{k=0}^n\exp(-\lambda_1t)\frac{(\lambda_1t)^k}{k!}\exp(-\lambda_2t)\frac{(\lambda_2t)^{n-k}}{(n-k)!},\\
	&= \frac{\exp(-(\lambda_1+\lambda_2)t)}{n!}\sum_{k=0}^n\binom{n}{k}(\lambda_1t)^k(\lambda_2t)^{n-k}.% = \exp(-(\lambda_1+\lambda_2)t)\frac{\{(\lambda_1+\lambda_2)t\}^n}{n!}.
\end{align*}
Result follows by recognizing that summand is just binomial expansion of $[(\lambda_1 + \lambda_2)t]^n$.
%\begin{eqnarray*}
%% \nonumber to remove numbering (before each align)
  %P\{N(t)=n] &=& P\{N_{1}(t)+ N_{2}(t)= n] \\
   %&=& \sum_{n1} P [N_{1}(t)=n_{1},N_{2}(t)= n-n_{1} ] \\
  %&=& \sum_{n_{1}}\frac{e^{-\lambda _{1}t}(\lambda_{1}t)^{n_{1}}}{n_{1}!}e^{-\lambda_{2}t}\frac{(\lambda_{2}t)^{(n-n_{1})}}{(n-n_{1})!} \\
   %&=&\sum_{n_{1} (\lambda _{2}t)} \frac{e^{-(\lambda_{1}+\lambda_{2})t} (\lambda_{1}t)^{n1}(\lambda_{2}t)^{-n_{1}}}{n_{1}! (n-n_{1})!} \\
   %&=& \frac{e^{-(\lambda_{1}+\lambda_{2})t}}{n!}\sum^{n}_{n_{1=0}}\left(\frac{\lambda_{1}}{\lambda_{2}}\right)^{n_{1}}\left(\frac{\lambda_{2}}{n_{1}!}\right)^{n}\frac{n!}{(n-n_{1})!} \\
   %&=&  \frac{e^{-(\lambda_{1}+\lambda_{2})t}}{n!}(\lambda_{1}+\lambda_{2})^{n}t^{n}
%\end{eqnarray*}
\end{proof}
\begin{rem}If independence condition is removed, the statement is not true.
\end{rem}
%\begin{figure}
%\centering
  %\includegraphics[width=5.0in]{Figures/comment.PNG}\\
 %% \caption{}\label{}
%\end{figure}
\begin{figure}[hhhh]
\center
  \input{Figures/IndependentSplitting}
 \caption{Splitting a Poisson process into two independent Poisson processes.}
\label{Fig:IndependentSplitting}
\end{figure}

\begin{thm}[Independent Spilitting] Let $\{N(t), t \geqslant 0\}$ be a Poisson arrival process. Each arrival can be randomly assigned to either arrival type 1 or 2, with probability $p$ and $(1-p)$ respectively, independent of previous assignments. Arrival processes of type 1 and 2 are denoted by $N_1(t)$ and $N_2(t)$ respectively. Then, $\{N_{1}(t), t \geqslant 0\}$,and $\{N_{2}(t), t \geqslant 0\}$ are mutually independent Poisson processes with rates $\lambda p$ and $\lambda (1-p)$ respectively.  
\end{thm}
\begin{proof} To show that ${N_{1}(t), t \geq 0}$ is a Poisson process with rate $\lambda p$, we show that it is stationary independent increment process with the distribution
\begin{align*}
 P\{N_{1}(t)= n\}  = \frac{(p \lambda t)^{n}}{n!}e^{-\lambda p t}.
\end{align*}
%\begin{figure}
%\center
  %% Requires \usepackage{graphicx}
  %\includegraphics[width=4.5in]{Figures/distr.PNG}\\
 %% \caption{}\label{}
%\end{figure}
The stationary, independent increment property of the probabilistically filtered processes $\{N_1(t), t \geqslant 0\}$ and $\{N_2(t), t \geqslant 0\}$ can be understood and argued out from the example given in the figure. Notice that 
\begin{align*}
	\{N_1(t)=k\} = \bigcup_{n=k}^\infty\{N(t) = n, N_1(t) = k\}.
\end{align*}
Further notice that conditioned on $\{N(t) = n\}$, probability of event $\{N_1(t) = k\}$ is merely probability of selecting $k$ arrivals out of $n$, each with independent probability $p$. Therefore, 
\begin{align*}
	P\{N_1(t)=k\} &= \exp(-\lambda t)\sum_{n=k}^\infty\frac{(\lambda t)^n}{n!}\binom{n}{k}p^k(1-p)^{n-k},\\
	&= \exp(-\lambda t)\frac{(\lambda p t)^k}{k!}\sum_{n=k}^\infty\frac{(\lambda(1-p)t)^{n-k}}{(n-k)!}.% = \exp(-p\lambda t)\frac{(p\lambda t)^k}{k!}.
\end{align*}
Recognizing that infinite sum in RHS adds up $\exp(\lambda(1-p)t)$, the result follows. We can find the distribution of $N_2(t)$ by similar arguments. We will show that events $\{N_1(t) = n_1\}$ and $\{N_2(t) = n_2\}$ are independent. To this end, we see that 
\begin{align*}
	\{N_1(t) = n_1, N_2(t) = n_2\} = \{N(t) = n_1 + n_2, N_1(t) = n_1\}.
\end{align*}
Using their distribution for $N_1(t), N_2(t)$, and conditional distribution of $N_1(t)$ on $N(t)$, we can show that
\begin{align*}
	P\{N_1(t) = n_1, N_2(t) = n_2\} &= \exp(-\lambda t)\frac{(\lambda t)^{n_1 + n_2}}{(n_1 + n_2)!}\binom{n_1 + n_2}{n_1}p^{n_1}(1-p)^{n_2},\\
	&= P\{N_1(t) = n_1\}P\{N_2(t) = n_2\} .
\end{align*}


  %\begin{eqnarray*}
  %% \nonumber to remove numbering (before each align)
    %P\{N_{1}(t) =n]&=& \sum^{\infty}_{m=0}P\{N(t)=m, N_{1}(t) = n]  \\
   %&=& \sum^{\infty}_{m=n}P\{N(t)=m] p^{n}(i-p)^{m-n}mC_n\\
   %&=&  \sum^{\infty}_{m=n}\frac{e^{-\lambda t(\lambda t)^{m}}}{m!}p^{n}(1-p)^{m-n}(mC_n)\\
     %&=& e^{-\lambda t}p^{n}\sum_{m=n}\frac{(\lambda t)^{m-n}}{m!}\frac{(1-p)^{m-n}m!}{n!(m-n)!} \\
     %&=& \frac{e^{-\lambda t p}(p \lambda t)^{n}}{n!} \sum^{\infty}_{m=n}e^{-\lambda (1-p)t}\frac{(\lambda (1-p)t)^{m-n}}{(m-n)!} \\
     %&=&  \frac{e^{-\lambda t p}(p \lambda t)^{n}}{n!} \sum^{\infty}_{k=0}e^{-\lambda (1-p)t}\frac{(\lambda (1-p)t)^{k}}{k!}  \\
     %&=& \frac{e^{-\lambda p t}(p \lambda t)^{n}}{n!}.
  %\end{eqnarray*}
%To show that 
%\begin{eqnarray*}
%% \nonumber to remove numbering (before each align)
   %\{N_{1}(t), t \geq 0\}&\perp&\{N_{2}(t), t \geq 0\}  \\
   %P\{N_{1}(t)&=&n_{1}, N_{2}(t) = n_{2}]\\
    %P\{N(t) = n_{1}+ n_{2}]&=& \frac{e^{-\lambda t} (\lambda t)^{n_{1}+n_{2}}}{(n_{1}+n_{2})!} \\
   %&=&  \frac{e^{-\lambda p t}e^{-\lambda (1-p)t}}{(n_{1}+n_{2})!} -(\lambda t)^{n_{1}+n_{2}}\\
   %&=&  \frac{e^{-\lambda p t}e^{-\lambda (1-p)t} (\lambda t)^{n_{1}}(\lambda t)^{n_{2}}}{n_{1}! n_{2}!}
%\end{eqnarray*}
In general, we need to show finite dimensional distributions factorize. That is, we need to show that for measurable sets $A_1, \hdots, A_n: j \in [m]\}$, we have \begin{align*}
   P\left(\bigcap_{i=1}^n\{N_{1}(t_{i})\in A_{i}\}\bigcap_{j=1}^m\{N_{2}(s_{j})\in B_{j}\}\right)
   =P\left(\bigcap_{i=1}^n\{N_{1}(t_{i})\in A_{i}\}\right)P\left(\bigcap_{j=1}^m\{N_{2}(s_{j})\in B_{j}\}\right).
\end{align*}
\end{proof}

\appendix
\section{Order statistics}
For any $n$ length sequence $a \in \R^n$, the \textbf{order statistics} is a permutation $\sigma: [n] \to [n]$ such that 
\eq{
a_{\sigma(1)} \leq a_{\sigma(2)} \leq \dots \leq a_{\sigma(n)}.
}
For, $k \in [n]$, we call $a_{\sigma(k)}$ as the \textbf{$k$th order statistic} of the sequence $a$. 
In particular, first order statistic is the minimum, and the $n$th order statistic is the maximum of a $n$ length sequence. 
\begin{lem}
Let $X = (X_1, X_2, \dots, X_n)$ be an $n$-length sequence of \textit{iid} random variables with common distribution and density functions $F$ and $f$ respectively. 
Then, the joint density of order statistics of sequence $X$ for $x \in \R^n$ is
\eq{
f_{X\circ\sigma}(x) &= n!\prod_{i=1}^nf(x_i).
} 
\end{lem}

\begin{lem}
Let $X = (X_1, X_2, \dots, X_n)$ be an $n$-length sequence of \textit{iid} random variables with common distribution and density functions $F$ and $f$ respectively.   
Then, the density function of $k$th order statistic of sequence $X$ for $x \in \R$ is
\eq{
f_{X_{\sigma(k)}}(x) &= \binom{n}{k}F(x)^{k-1}\bar{F}(x)^{n-k}f(x).
} 
\end{lem}
\end{document}