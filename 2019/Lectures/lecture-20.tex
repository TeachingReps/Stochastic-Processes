% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
%\documentclass[a4paper,10pt,english]{article}
\documentclass[all-lectures.tex]{subfiles}
%\input{header}
%\title{Lecture 01: Poisson Processes}
%\author{}
\setcounter{chapter}{4}

\begin{document}
%\maketitle

%\chapter{Martingales}
\setcounter{section}{3}
\setcounter{subsection}{1}

\section*{\centering{Lecture 20 \linebreak }}

\section{McDiarmid's Inequality:Applications} 
\begin{exmp}[Machine learning:Classification problem]
Let the training samples $(X_1, Y_1), \dots, (X_n,Y_n)$ be iid. $X_i \in \R^d$, $Y_i \in \{1, 2, \dots, N\}$.
$h$  is the classifier, $h(x) \to \{1, 2, \dots, N\}$. 
$1_{\{h(X_i) \neq Y_i\}}$ denotes the error.\\
The probability of error for a given classifier is given by
\eq{
R(h) = P[h(X) \neq Y]
}
and its estimate from the training sample is 
\eq{
\hat{R}_n(h) = \frac{1}{n} \sum_{i=1}^{n} 1_{\{h(X_i) \neq Y_i\}}.
}
Define
\eq{
f((x_1,y_1), \dots, (x_n, y_n)) = \frac{1}{n} \sum_{i=1}^{n} 1_{\{h(x_i) \neq y_i\}}.
}
Removing $i$th component and replacing with another,
\eq{
|f((x_1,y_1), \dots, (x_i,y_i), \dots, (x_n, y_n)) - f((x_1,y_1), \dots, (x'_i,y'_i), \dots, (x_n, y_n))|  \le \frac{1}{n}.
}
Then by McDiarmid's Inequality,
\eq{
P[|\hat{R}_n(h) - E[\hat{R}_n(h]| \ge \lambda] \le 2 exp{\left(\frac{-2\lambda^2}{n\frac{1}{n^2}}\right)} =  2 exp{\left(-2\lambda^2n\right)},
}
where $\E[\hat{R}_n(h)] = R(h)$.
\end{exmp}

\begin{exmp}
If $X_1, X_2, \dots, X_n \sim P$. We want an estimate of $P$.
 We estimate $P$ by 
 \eq{
\hat{P}_n(A) = \frac{1}{n} \sum_{i=1}^{n} 1_{\{X_i \in A\}},
}
Since,
\eq{
\E[\hat{P}_n(A)] = P(A),
}
it is an unbiased estimate. Define
\eq{
f(x_1, x_2, \dots, x_n) = \frac{1}{n} \sum_{i=1}^{n} 1_{\{X_i \in A\}}.
}
Since $|f(x_1, x_2, \dots, x_n) - f(x_1, \dots,y_i, x_{i+1}, \dots, x_n)| \le  \frac{1}{n}$,
by  McDiarmid's Inequality,
\eq{
P[|f(x_1, x_2, \dots, x_n) - E[f]| \ge \epsilon] &= P[|\hat{P}_n(A)-P(A)| \ge \epsilon] \\
&\le 2 exp\left(-2 \epsilon^2 n\right).
}
\end{exmp}

\section*{Hoffding's inequality}
Let $X_1, X_2, \dots, X_n$ $i.i.d.$, $a \le |X_i| \le b$ $a.s.$, and $S_0 = X_1 + X_2 + \dots + X_n$, $\mu = \E[X_1]$.\\
Define
\eq{
f(x_1, x_2, \dots, x_n) = X_1 + X_2 + \dots +X_n.
}
Since,
\eq{
|f(x_1, x_2, \dots, x_n)-  f(x_1, \dots,y_i, \dots, x_n)| = |x_i - y_i| \le |b-a|,
}
by McDiarmid's Inequality,
\eq{
P[|S_n - n \mu| \ge \epsilon] \le  2 exp\left(-2  \frac{\epsilon^2 n}{n (b-a)^2}\right)
}
\eq{
P[|S_n - n \mu| \ge \frac{\epsilon}{n}]  = P[|S_n - n \mu| \ge \delta] \le  2 exp\left(-2  \frac{\delta^2 n}{ (b-a)^2}\right).
}

\section{Martingale Convergence Theorem}

Let $\{X_n\}$ be a submartingale and $\alpha < \beta$. 
Let $U_n$ denotes the number of times $X_k$ goes from below $\alpha$ to above $\beta$ in time $n$. We will need the following equality.

\begin{lem}[Upcrossing inequality]
\eq{
E[U_n] \le \frac{\E[|X_n|]+ \alpha}{\beta - \alpha}.
}
\end{lem}

\begin{proof}
Let $Y_n = \max(0, X_n - \alpha)$. $Y_n$ is also a submartingale.\\
Let $U_n$ be the number of times $Y_n$ goes from 0 to above $\beta-\alpha$. 
This number is same as of $\{X_n\}$ upcrossing from $\alpha$ to $\beta$.
Let $T_1$ be the first time when $Y_k = 0$, $T_2$ = $\min \{k > T_1$ such that $Y_k \ge \beta - \alpha\}$. Similary, define $T_k$ as the sequence of stopping times upto time $n$, with $T_n$  the maximum possible.
\eq{
\E[Y_n] = \E[Y_{T_n}] &= \E[\sum_{k=0}^{n} (Y_{T_k}-Y_{T_{k-1}})] \\
&= \sum_{k:even} \E[Y_{T_k}-Y_{T_{k-1}}] + \sum_{k:odd} \E[Y_{T_k}-Y_{T_{k-1}}].
}
All are bounded stopping times, $Y_n$ is a submartingale, $T_k < T_{k+1} < \dots \le n$.
Also $\E[Y_{T_k}] \ge \E[Y_{T_{k-1}}]$ for $k$ odd and $\E[Y_{T_k}-Y_{T_{k-1}}] \ge \beta-\alpha$ for $k$ even.
Therefore,
\eq{
\E[Y_n] \ge \E[U_n] (\beta -\alpha).
}
Hence,
\eq{
\E[U_n] \le \frac{\E[Y_n]}{(\beta -\alpha)} &= \frac{\E[\max(0,X_n-\alpha)]}{(\beta -\alpha)}\\
&\le \frac{\E[|X_n-\alpha|]}{(\beta -\alpha)}\\
&\le \frac{\E[|X_n|]+\alpha}{(\beta -\alpha)}.
}
\end{proof}
\begin{thm}
$\{X_n\}$ submartingale and $\sup_k \E[|X_k|] \le M < \infty$.
Then $X_n \to X$ almost surely and $E[|X|] < \infty$.
\end{thm}
\begin{proof}
Take $\alpha < \beta$. 
Let $\liminf X_n(\omega) = X_*(\omega)$, $\limsup X_n(\omega) = X^*(\omega)$. If $X_*(\omega) < \alpha < \beta < X^*(\omega)$, then this sequence will not converge.\\

Let $U_n$ be the number of upcrossings of $\{X_n\}$ from below $\alpha$ to above $\beta$ in time $n$.
Thus, from the above lemma,
\eq{
\E[U_n] \le  \frac{\E[|X_n|]+\alpha}{(\beta -\alpha)} \le \frac{M-\alpha}{(\beta -\alpha)}.
}
$U_n(\omega)$ increases as $n$ increases, and it will converge to $U(\omega) < \infty$ or reach $\infty$.
Thus,
\eq{
\E[U_n] \nearrow \E[U] \le \frac{M-\alpha}{(\beta -\alpha)} < \infty.
}
Therefore, $P[U(\omega) < \infty] = 1$, and $ P[X_* < \alpha <  \beta < X^*] = 0$.
Hence, for rational $\alpha, \beta$,
\eq{
\quad \quad P[X_* < X^*] \le P[\cup_{\alpha < \beta} \{X_* < \alpha <  \beta < X^*\}] \le \sum_{\alpha <  \beta } P[X_* < \alpha <  \beta < X^*] = 0.
}
This implies
$X_n \to X$ \quad $a.s.$. Also,
\eq{
M \ge \liminf \E[|X_n|] \ge \E[|X|], \enskip \text{by Fatou's lemma}.
}
\end{proof}
If $X_n$ is a martingale, then $\{|X_n|\}$ is also a submartingale and $\E[|X_n|] = \E[\E[|X_n||\F_{n-1}]] \ge \E[|X_{n-1}|]$.
Therefore,
\eq{
\sup_k \E[|X_k|] = \lim_{n \to \infty} \E[|X_n|].
}\\
\begin{lem}
If $\{X_k\}$ is submartingale. then for $X^+_k = \max(0, X_k)$,
\end{lem}
\eq{
\sup_k \E[|X_k|] < \infty \iff \sup_k \E[X^+_k] < \infty.
}
\begin{proof}
$[\Rightarrow]$
$X^+_k \le |X_k|$, hence
\eq{
\sup_k \E[X^*_k] \le \sup_k \E[|X_k|].
}
$[\Leftarrow]$
$|X_k| = 2X^+_k - X_k$.
Thus,
\eq{
\E[|X_k|] = 2\E[X^+_k] - \E[X_k] \le 2\E[X^+_k] - \E[X_0].
}
\eq{
\sup_k \E[|X_k|] \le 2\sup_k \E[X^+_k] - \E[X_0].
}
If RHS is finite, then LHS is finite.
\end{proof}
Thus, if a submartingale is upper bounded, $X_k \le  M_1 < \infty$ $a.s.$, then
$X^+_k \le M_1  \Rightarrow \sup_k \E[X^+_k] \le M_1$ .
Therefore, if a submartingale is upperbounded then it converges $a.s.$.

If $X_k$ is a supermartingale, then $-X_k$ is submartingle.
Therefore, if $\sup_k (-X_k) \le M_1, \quad X_k \ge -M_1, \quad \forall k$.
Therefore, if $X_k$ is a supermartingale and lower bounded then it converges.

If $X_k$ is a martingale then it is a supermartingale and a submartingale. Therefore an upper or lower bounded martingle converges $a.s.$.
\end{document}