% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 15 : Continuous Time Markov Chains}
\author{}
\begin{document}
\maketitle

\section{Markov Process}
%\begin{defn}
A stochastic process $\{X(t) \in E, ~ t \geqslant 0\}$ assuming values in a state space $E$ and 
indexed by positive reals is a \textbf{Markov process} if 
the distribution of the future states conditioned on the present, is independent of the past. 
That is,
\begin{align*}
\Pr\{X(t+s) = j |X(u),~ u \in [0,s]\} &= \Pr\{X(t+s) = j |X(s)\}, \text{ for all } s, t \geqslant 0 \text{ and } i, j \in E.
\end{align*} 
A Markov process with countable state space is referred to as \textbf{continuous time Markov chain (CTMC)}. 
%and assuming values from a countable state space $E$ 
We define the \textbf{transition probability} from state $i$ at time $s$ to state $j$ at time $s+t$ as 
\begin{align*}
P_{ij}(s, s+t) = \Pr\{X(s+t) = j | X(s) = i\}.
\end{align*}
%\end{defn}
%\begin{defn}
The Markov process has \textbf{homogeneous} transitions for all $i,j \in E, s,t \geqslant 0$, if
\eq{
P_{ij}(s,s+t) = P_{ij}(0,t),
}
and we denote the \textbf{transition probability matrix} at time $t$ by $P(t) = \{P_{ij}(t) = P_{ij}(0,t): i, j \in E\}$. 
We will mainly be interested in continuous time Markov processes with homogeneous jump transition probabilities. 
%\begin{defn}
A distribution $\pi$ is an \textbf{equilibrium distribution} of a Markov process if
\begin{align*}
\pi P(t) &= \pi,~~ \forall t \geqslant 0. 
\end{align*}
%\end{defn}
\subsection{Strong Markov property}
%\end{defn}
For a process $\{X(t), t\geqslant 0\}$,  if we denote the $\sigma$-algebra generated by realization of the process till time $t$ as 
$\F_t = \sigma\left(\{X(u): u \leqslant t\}\right)$, 
then a random variable $\tau$ is a \textbf{stopping time} if for each $t \in \R_+$, 
\eq{
\{\tau \leq t\} \in \F_t.
}
That is, a random variable $\tau$ is a stopping time if the event $\{\tau \leq t\}$ can be determined completely by the collection $\{X(u): u \leqslant t\}$. 
A stochastic process $X$ has \textbf{strong Markov property} if for any almost surely finite stopping time $\tau$, 
\eq{
\Pr\{X(\tau+s) = j|X(u), u \leq \tau\} &= \Pr\{X(\tau+s) = j|X(\tau)\}. 
}
\begin{lem}
\label{Lemma:StrongMarkovProperty}
A homogeneous continuous time Markov chain $X$ has the strong Markov property. 
\end{lem}
\begin{proof}
%\eq{
%\Pr(A|B) &= \frac{\Pr(A,B)}{\Pr(B)} = \frac{\Pr(A,B,C)}{Pr(B,C)}
%}
Let $\tau$ be an almost surely finite stopping time with conditional distribution $F$ on the collection of events $\{X(u): u \leq s\}$. 
Then, 
\eq{
\Pr\{X(\tau+s) =j |X(u), u \leq \tau\} = \int_{0}^{\infty}dF(t)\Pr\{X(t+s) = j| X(u), u \leq t, \tau = t\} = \Pr\{X(\tau+s) = j|X(\tau)\}. %\Pr\{\tau = t |  X(u), u \leq \tau\}
}
\end{proof}
Since the CTMC is homogeneous and $\tau$ is almost surely finite stopping time, it is clear that 
\eq{
\Pr\{X(\tau+s) = j|X(\tau) = i\} &= P_{ij}(\tau,\tau+s) = P_{ij}(0,s). 
}
\subsection{Jump and sojourn times}
For any stochastic process on countable state space $E$ that is right continuous with left limits (rcll), 
%and has finite jumps in any finite interval, 
we wish to know following probabilities 
\eq{
\Pr\{X(s+t) = j | X(u), ~ u \in [0,s]\},~s, t \geq 0.
}
%Suppose $X(0)=i$, and  for all $u \in [0,s]$, we have $X(u)=i$.  
To this end, we define the sojourn time in any state, the jump times, and the jump transition probabilities. % from it. 
First, we define a stopping time for any stochastic process 
\eq{
\tau_t &= \inf\{s > t : X(s) \neq X(t)\}.
}
For a homogeneous CTMC $X$, the distribution of %stopping time 
$\tau_t - t$ only depends on $X(t)$ and doesn't depend on time $t$. 
Hence, we can define the following conditional distribution 
\eq{
F_i(u) &\triangleq \Pr\{\tau_s - s\leq u | X(s) = i\}.
}
\begin{lem}
For a homogeneous CTMC $X$, there exists some $\nu_i > 0$, such that 
\eq{
\Pr\{\tau_s - s > t |X(s) = i\} = e^{-t \nu_i}.
}
\end{lem}
\begin{proof}
Using Markov property and the fact that $\tau_s$ is a stopping time, we can write 
\eq{
\bar{F}_i(u+v) &= \Pr\{\tau_s - s > u+v | X(s) = i, \F_s\}= \Pr\{\tau_{s} - s > u+v| \tau_s - s > u, X(s) = i, \F_{s}\}\Pr\{\tau_s -s > u|X(s) = i, \F_s\}\\
&= \Pr\{\tau_{s+u} - (s+u)> v| X(s+u) = i, \tau_s > u + s, \F_{s+u}\}\Pr\{\tau_s - s > u|X(s) = i, \F_s\} = \bar{F}_i(v)\bar{F}_i(u).
}
The only continuous function $\bar{F}_i \in [0,1]$ that satisfies semigroup property is an exponential function with a negative exponent. 
\end{proof}
%\begin{defn}
The \textbf{jump times} of a stochastic process $\{X(t), t \geqslant 0\}$ are defined as %defined by
\begin{xalignat*}{3}
&S_0 = 0, &&S_n \triangleq \inf\{t > S_{n-1}: X(t) \neq X(S_{n-1})\}. 
\end{xalignat*}
%\end{defn}
%\begin{defn}
The \textbf{sojourn time} of %a stochastic process $\{X(t), t \geqslant 0\}$
this process staying in state $X(S_{n-1})$ is %in any state $i \in I$ are 
\begin{align*}
T_n &\triangleq (S_n - S_{n-1}). %1\{X(S_{n-1}) = i\}. 
%\tau_i \triangleq \inf\{t \geqslant 0: X(t) \neq i|X(0)=i\}, i \in I. 
\end{align*}
%\end{defn}
\begin{lem} 
Jump times $\{S_n: n \in \N\}$ are stopping times with respect to the process $\{X(t): t \geqslant 0\}$. 
\end{lem}
\begin{proof}
It is clear that $\{S_n \leq t\}$ is completely determined by the collection $\{X(u): u \leqslant t\}$. 
\end{proof}
\begin{lem}
\label{Lemma:MemorylessSojourn}
For a homogeneous CTMC, each sojourn time $T_n$ is a continuous memoryless random variable, and the sequence of sojourn times $\{T_n: n \in \N\}$ are independent. %and exponentially distributed. 
\end{lem}
\begin{proof}
%For some $i \in E$, let $X(S_{n-1}) = i \in E$ without any loss of generality. 
We observe that $S_n = \tau_{S_{n-1}}$, and hence the conditional distribution of $T_n$ given $\F_{S_{n-1}}$ is 
\eq{
\Pr\{T_n  > y| \F_{S_{n-1}}\} &= \exp(-y \nu_{X(S_{n-1})}) = \bar{F}_{X(S_{n-1})}(y),~y\geq 0.
}
For independence of sojourn times, we show that the $(n+1)$th sojourn time is independent of $n$th jump time $S_n$. 
%We observe that $T_n = \tau_{S_{n-1}}$, and hence 
We can write the joint distribution as a conditional expectation
\eq{
\Pr\{T_n > y, S_{n-1}\leq x|X(S_{n-1})\} %&= \int_{0}^{x}dF_{S_{n-1}}(u)\Pr\{T_n > y |S_{n-1}=u\} = \int_{0}^{x}dF_{S_{n-1}}(u)\Pr\{\tau_{u} > y|S_{n-1}=u\}\\
&= \E[\E[1\{\tau_{S_{n-1}}> S_{n-1}+y\}1\{S_{n-1} \leq x\}|\F_{S_{n-1}}]|X(S_{n-1})].
}
Using strong Markov property of homogeneous CTMC $X$, we can write the right hand side as 
\eq{
\E[\bar{F}_{X(S_{n-1})}1\{S_{n-1} \leq x\}|X(S_{n-1})] &= \bar{F}_{X(S_{n-1})}\Pr\{S_{n-1} \leq x\}.
%&= \Pr\{\tau_{S_{n-1}} -S_{n-1}> y, \tau_{S_{n-2}} \leq x \} = \Pr\{\tau_{X(S_{n-1})} > y\}\Pr\{S_{n-1} \leq x\}.
}

%Let $X(S_{n-1}) = i \in E$ without any loss of generality, then we observe that,
%%\begin{align*}
%%\Pr\{\tau_i \geqslant s+t | \tau_i > s\} &=\Pr\{X(v)=i,~v \in [s,s+t) | X(u)=i,  u \in [0, s]\}\\
%%&= \Pr\{X(v)=i,~v \in [0,t) | X(0)=i\} = \Pr\{\tau_i \geqslant t \}.
%%\end{align*}
%\eq{
%\Pr\{T_n > s+t |T_n > s\} &= P_{ii}(s,s+t) = P_{ii}(0,t) = \Pr\{T_{n} > t\}.
%%\Pr\{X(v)=X(S_{n-1}),~v -S_{n-1} \in [s,s+t] | X(u)=X(S_{n-1}),  u -S_{n-1}\in [0, s]\}\\
%%&= \Pr\{X(v)=X(S_{n-1}),~v -S_{n-1} \in [0,t] | X(S_{n-1})\} = \Pr\{T_n > t \}.
%}
\end{proof}
\begin{cor}
If $X(S_{n}) = i$, then the random variable $T_{n+1}$ has same distribution as the exponential random variable $\tau_{i}$ with rate $\nu_i$. 
\end{cor}
%\begin{rem} 
Inverse of mean sojourn time in state $i$ is called the \textbf{transition rate} out of state $i$ denote by $\nu_i$. and typically $\nu_i < \infty$.  
%\end{rem}
%\begin{rem} 
If $\nu_i = \infty$, we call the state to be \textbf{instantaneous}. 
%\end{rem}
%We define the conditional distribution of stopping time $\tau_s$ as 
%\eq{
%F_i(u) &\triangleq \Pr\{\tau_s \leq u | X(s) = i\}.
%}

\subsection{Jump process}
%\begin{defn} 
The \textbf{jump process} is a discrete time process $\{X^J(n) = X(S_n): n \in \N_0\}$ derived from 
the continuous time stochastic process $\{X(t): t \geqslant 0\}$ by sampling at jump times.  
%\begin{align*}
%X^J(n) = X(S_n).
%\end{align*}
%\end{defn}
%\begin{defn}
The corresponding \textbf{jump transition probabilities} %of a stochastic process $\{X(t), t \geqslant 0\}$ 
are defined by
\begin{align*}
p_{ij}(S_n) \triangleq %P_{ij}(S_{n-1},S_n) = 
\Pr\{X(S_n)  = j | X(S_{n-1}) =  i \}, ~i,j \in E.
\end{align*}
%\end{defn}
\begin{lem}
For any right continuous left limits stochastic process, the sum of jump transition probabilities $\sum_{j \neq i}p_{ij}(S_n) = 1$ for all $X(S_{n-1}) = i \in E$. 
\end{lem}
\begin{proof}
It follows from law of total probability. 
\end{proof}
Let $N(t)$ be the counting process associated with jump times sequence $\{S_n: n \in \N\}$. 
That is, 
\eq{
N(t) = \sum_{n \in \N}1\{S_n \leq t\}.
}

\begin{prop} 
For a homogeneous CTMC such that $\inf_{i \in E} \nu_i \geq \nu > 0$, then the jump times are almost surely finite stopping times. 
\end{prop}
\begin{proof}
We obsevre that the jump times are sum of independent exponential random variables. % and hence the result follows. 
Further by coupling, we can have a sequence of \textit{iid} random variables $\{\overline{T}_n: n \in \N\}$, 
such that $T_n \leq \overline{T}_n$ and $\E \overline{T}_n = 1/\nu$ for each $n \in \N$. 
Hence, we have 
\eq{
S_n = \sum_{i=1}^{n}T_i \leq \sum_{i=1}^{n}\overline{T}_i \triangleq \overline{S}_n.
}
Result follows since $\overline{S}_n$ is the $n$th arrival instant of a Poisson process with rate $\nu$. 
\end{proof}
\begin{lem}
\label{Lemma:JumpProb}
For a homogeneous CTMC, the jump probability from state $X(S_{n-1})$ to state $X(S_n)$ depend solely on $X(S_{n-1})$ and are independent of jump instants.  
\end{lem}
\begin{proof}
We can re-write the jump probability as
\eq{
\Pr\{X(S_{n}) = j | X(S_{n-1}) = i\} &= P_{ij}(S_{n-1}, S_n) = P_{ij}(0,T_n).
}
For $T_n > x$, then we can write
\eq{
%P_{ij}(0,T_n)
\Pr\{T_n > x, X(S_{n}) = j| X(S_{n-1}) = i\} &=  \Pr\{X(S_{n}) =j | T_n > x, X(S_{n-1}) = i \}\Pr\{T_n > x|X(S_{n-1}) = i\}\\
&= P_{ij}(S_{n-1}+x, S_n)\Pr\{\tau_{X(S_{n-1})} > S_{n-1} + x\} = P_{ij}(0,T_n-x)\bar{F}_i(x).
}
Similarly, for $T_n \leq x$, we can write 
\eq{
%P_{ij}(0,T_n)
\Pr\{T_n \leq x, X(S_{n}) = j|X(S_{n-1}) = i\}&=  \int_{0}^{x}\Pr\{X(S_{n}) =j | T_n = u, X(S_{n-1}) = i \}dF_i(u) = P_{ij}(0,0)F_i(x).
%\int_{0}^{x}P_{ij}(u)dF_i(u)\\
%&= P_{ij}(S_{n-1}+x, S_n)\Pr\{\tau_{X(S_{n-1})} > x\} = P_{ij}(0,T_n-x)\bar{F}_i(x)
}
%Similarly, we can write $P_{ii}(0,T_n) = 0$ for each $n \in \N$. 
%We denote conditional distribution of $T_n$ given $X(S_{n-1}) = i$ as $F_i$ to write the above probability as 
%\eq{
%P_{ij}(0,T_n) &= \Pr\{X(S_{n}) =j | T_n > x, X(S_{n-1}) = i \}\Pr\{T_n > x|X(S_{n-1}) = i\}\\
%&= P_{ij}(S_{n-1}+x, S_n)\Pr\{\tau_{X(S_{n-1})} > x\} = P_{ij}(0,T_n-x)\bar{F}_i(x).
%}
%This holds for all realizations 
Hence for any $x \in \R_+$, we can write
\eq{
P_{ij}(T_n) &= P_{ij}(T_n-x)\bar{F}_i(x) + P_{ij}(0)F_i(x).
}
Result follows, since the only solution to this equation is $P_{ij}(T_n) = P_{ij}(0)$. 
\end{proof}
\begin{cor} 
For a homogeneous CTMC, the transition probabilities $p_{ij}$ and sojourn times $\tau_i$ are independent. 
\end{cor}
\begin{cor}
The jump process is a homogeneous Markov chain with countable state space $E$. 
\end{cor}

%We could sample the homogenous Markov process at sojourn times and study the resulting DTMC. 
\subsection{Alternative construction of CTMC}
\begin{prop} A stochastic process $\{X(t) \in E, t \geqslant 0 \}$ is a CTMC iff 
\begin{enumerate}[a.]
\item sojourn times are independent and exponentially distributed with rate $\nu_i$ where $X(S_{n-1}) = i$, and 
\item jump transition probabilities $p_{ij}(S_n)$ are independent of jump times $S_n$, such that $\sum_{i \neq j}p_{ij}=1$.
\end{enumerate}
\end{prop}
\begin{proof}
Necessity of two conditions follows from Lemma~\ref{Lemma:MemorylessSojourn} and~\ref{Lemma:JumpProb}. 
For sufficiency, we assume both conditions and show that Markov property holds and the transition probability is homogeneous. 
%To this end, we observe
%\begin{align*}
%\{ T_n > s \} &= \{ X(S_{n-1}+ u) = X(S_{n-1}), u \in [0,s] \}.
%\end{align*}
Using jump time independence of jump probabilities, we can write %for small $t > 0, X(S_{n-1})=i, X(S_{n}) = j$, 
\begin{align*}
\Pr\{ X(t+s) = j | X(s) = i, \F_s \} &= \sum_{ k = 0}^{\infty}\Pr\{X(t+s) = j, N(s+t) - N(s) = k | N(s), X(s) = i, \F_s\}.
%&= \int_0^{t}\frac{dF_{T_n}(s+u)\Pr\{T_{n+1} > t-u\}}{\Pr\{T_n> s\}}p_{ij} + o(t).
%&= \frac{\Pr\{ X(t+s) = j, \tau_i \in (s, s+t) \}}{\Pr\{\tau_i > s\}}\\
%&= \int_0^{t}\frac{\Pr\{\tau_i = s+u\}\Pr\{\tau_j > t-u\}}{\Pr\{\tau_i > s\}}p_{ij} + o(t).
%\sum_{k \neq i}\frac{\Pr\{ X(t+s) = j, \tau_i \in (s,s+t), X(\tau_i) = k \}}{\Pr\{\tau_i > s\}}\\
%&= \int_0^{t}\sum_{k \neq i}\Pr\{ X(t+s) = j| X(u+s) = k\} p_{ik} e^{-\nu_iu}du.
\end{align*}
Since sojourn times are memoryless and depend only on the previous state, it follows that $N(t+s)-N(s)$ is independent of $N(s)$ and 
\eq{
\Pr\{X(t+s) = j, N(s+t) - N(s) = k | N(s), X(s) = i, \F_s\} &= \Pr\{X(t+s) = j, N(s+t) - N(s) = k | X(s) = i\}.
}
%\begin{align*}
%\Pr\{ X(S_{n-1} +t+s) = j | X(S_{n-1}+u) = i, u \in [0,s] \} 
%&= \Pr\{X(S_{n-1}+t) = j | X(S_{n-1}) = i\}, ~~\forall s, t \geqslant 0.
%\end{align*}
This shows that the $X$ is a Markov process. 

The distribution of $N(s+t) - N(s)$ conditioned on $X(s) = i$ has 
the same distribution as $N(t)$ conditioned on $X(0) = i$. 
Further, the jump probabilities are independent of jump times. 
Together, it follows that
\eq{
\Pr\{X(t+s) = j, N(s+t) - N(s) = k | X(s) = i\} &=  \Pr\{X(t) = j, N(t) - N(0) = k | X(0) = i\}.
}
Therefore, it follows that $X$ is a homogeneous Markov process. 
%Further, since the sojourn times depend only on the preceding state, it follows that 
%\begin{align*}
%\Pr\{X(S_{n-1}+t) = j | X(S_{n-1}) = i\} &= P_{ij}(S_{n-1},S_{n-1}+t) = P_{ij}(0,t).% \Pr\{X(t) = j | X(0) = i\}.
%\end{align*}
\end{proof}
%\begin{rem}
%\end{rem}
%\begin{rem}  
%In fact, a CTMC is a DTMC with exponential sojourn time in each state. 
%\end{rem}
%\begin{defn} 
A CTMC is called \textbf{regular} if for all finite $t \in \R_+$, number of jumps $N(t)$ is almost surely finite. 
That is, for all $t \in \R_+$
\begin{align*}
\Pr\{ N(t) < \infty \} = 1.
\end{align*} 
%\end{defn}
\begin{lem} 
A homogeneous CTMC is regular if $\sup_{i \in E} \nu_i < \nu < \infty$. 
\end{lem}
\begin{proof}
By coupling, we can have a sequence of \textit{iid} random variables $\{\underline{T}_n: n \in \N\}$, 
such that $\underline{T}_n \leq T_n$ and $\E X_n = \nu$ for each $n \in \N$. 
Let $\underline{m}(t)$ be the associated renewal function with the sequence $\underline{T}$, 
then we can write 
\eq{
\Pr\{N(t) < \infty\} &= \sum_{n \in \N_0}\Pr\{S_n \geq t\} = 1 + m(t) \leq 1 + \underline{m}(t).
}
\end{proof}
\begin{shaded*}
%\begin{exmp} 
Consider the following example of a non-regular CTMC, where for all $i \in \N$
\eq{
p_{i,i+1}=1, \nu_i = i^2.
} 
Clearly, $\sup_{i \in E}\nu_i = \infty$, and hence it is not regular.
%\end{exmp}
\end{shaded*}

\subsection{Properties of transition matrix}
For each $t$, we have the transition matrix $P(t)$.
\begin{lem}[continuity]
Transition matrix $P(t)$ for a homogeneous CTMC $X$ is a continuous function of time $t \in \R_+$, such that
\eq{
\lim_{t \downarrow 0}P(t) = I.
}
\end{lem}
\begin{proof}
It follows from continuity of probability functions and alternate characterization of homogeneous CTMC.
\end{proof}
\begin{lem}[semigroup property]
Transition matrix $P(t)$ satisfies the semigroup property
\eq{
P(s+t) &= P(s)P(t).
}
\end{lem}
Since each entry of $P(t)$ is a probability, this leads to characterization of $P(t)$ completely. 
\begin{proof}
From homogeneity of Markov chains, we can write the $(i,j)$th entry of $P(s+t)$ as
\eq{
P_{ij}(0,s+t) &= \sum_{k \in E}P_{ik}(0,s)P_{kj}(s,s+t) =  \sum_{k \in E}P_{ik}(0,s)P_{kj}(0,t) = [P(s)P(t)]_{ij}.
}
\end{proof}
For a homogeneous CTMC with transition matrix $P(t)$, the \textbf{generator matrix} $Q \in \R^{E \times E}$ is defined as the following limit when it exists 
\eq{
Q &\triangleq \lim_{t \downarrow 0}\frac{P(t) - I}{t}.
}
\begin{thm} 
For a homogeneous CTMC, the generator matrix exists and is defined in terms of sojourn time rates $\{\nu_i: i \in E\}$, and jump transition matrix $p = \{p_{ij}: i,j \in E\}$ as
\begin{xalignat*}{3}
&Q_{ii} = -\nu_i,&&Q_{ij} = \nu_ip_{ij}.
\end{xalignat*}
\end{thm}
\begin{proof}
We can expand the $(i,j)$th entry of transition matrix in terms of disjoint events  $\{N(t) = n\}$ as  
\eq{
P_{ij}(t) &= \Pr\{X(t)=j|X(0) = i\} = \sum_{n \in \N_0}\Pr\{X(t)=j, N(t) = n|X(0) = i\}.
}
We can write the upper and lower bound as 
\eq{
\sum_{n = 0}^{1}\Pr\{X(t)=j, N(t) = n|X(0) = i\} &\leq P_{ij}(t) \leq \sum_{n = 0}^{1}\Pr\{X(t)=j, N(t) = n|X(0) = i\} + \Pr\{N(t) \geq 2\}.
}
For $t > 0$, we can compute for $j \neq i \in E$
\begin{xalignat*}{3}
&\Pr\{X(t)= i, N(t) = 0 | X(0) = i\} = e^{-\nu_i t}, && \Pr\{X(t) = i, N(t) = 1| X(0) = i\} = 0,\\
&\Pr\{X(t)= j, N(t) = 0 | X(0) = i\} = 0, && \Pr\{X(t) = j, N(t) = 1| X(0) = i\} = p_{ij}\int_{0}^t\nu_ie^{-\nu_j(t-u)}e^{-\nu_iu}du.
\end{xalignat*}
Since $\{N(t) \geq 2\}$ is of order $o(t)$ for small $t$, we can write
\eq{
\frac{P_{ij}(t) - I_{ij}}{t} &= -\left(\frac{1-e^{-\nu_it}}{t}\right)I_{ij} + \nu_ip_{ij}\frac{(e^{-\nu_j}-e^{-\nu_it})}{(\nu_i-\nu_j)t}(1-I_{ij}) + o(t).
%\begin{cases}
%\frac{1-e^{-\nu_it}}{t} + o(t) &i = j, \\
%\frac{\nu_i(e^{-\nu_j}-e^{-\nu_it})}{(\nu_i-\nu_j)t}& i \neq j.
%\end{cases}
}
Taking limit as $t \downarrow 0$, we get the result.
\end{proof}

\begin{cor}
For each state $i \in E$, the generator matrix $Q \in \R^{E \times E}$ for a homogeneous CTMC satisfies 
\begin{xalignat*}{5}
&0 \leq -Q_{ii} < \infty, && Q_{ij} \geq 0,&&\sum_{j \in E}Q_{ij} = 0.
\end{xalignat*}
\end{cor}

\subsection{Chapman Kolmogorov equations}
\begin{thm}[backward equation] For a homogeneous CTMC with transition matrix $P(t)$ and generator matrix $Q$, we have
\begin{align*}
%P'_{ij}(t)= \sum_{k \neq i}q_{ik}P_{kj}(t)-\nu_iP_{ij}(t) ,  i,j \in I
\frac{dP(t)}{dt}=QP(t), ~~t \geqslant 0.
\end{align*}
\end{thm}
\begin{proof} 
Using semigroup property of transition probability matrix $P(t)$ for a homogeneous CTMC,  
%we have
%\begin{align*}
%P_{ij}(t+h)=\sum_{k \in I}P_{ik}(h)P_{kj}(t),~~i,j \in I.
%\end{align*}
%Re-arranging terms, 
we can write
\begin{align*}
%P_{ij}(t+h)-P_{ij}(t)=\sum_{k \neq i}P_{ik}(h)P_{kj}(t)-(1-P_{ii}(h))P_{ij}(t).
\frac{P(t+h) - P(t)}{h} &= \frac{(P(h)- I)}{h}P(t). % = P(t)\frac{(P(h) - I)}{h}.
\end{align*}
%Dividing above equation by $h$,  and 
Taking limits $h \downarrow 0$ and exchanging limits and summation, we get 
\begin{align*}
\frac{dP_{ij}(t)}{dt} = \sum_{k \neq i}Q_{ik}P_{kj}(t)-\nu_iP_{ij}(t). 
\end{align*}
Now the exchange of limit and summation has to be justified. For any finite subset $F \subset E$, we have
\begin{align*}
\liminf_{h \downarrow 0} \sum_{k \neq i}\frac{P_{ik}(h)}{h}P_{kj}(t) \geq \sum_{k \in F\setminus \{i\} }\liminf_{h \downarrow 0}\frac{P_{ik}(h)}{h}P_{kj}(t) = \sum_{k \in F\setminus \{i\}}Q_{ik}P_{kj}(t).
\end{align*}
Since, above is true for any finite set $F \subset E$, taking supremum over increasing sets $F$, we get the lower bound.
%\begin{align*}
%\liminf_{h \downarrow 0} \sum_{k \neq 1}\frac{P_{ik}(h)}{h}P_{kj}(t) \geq \sum_{k \neq i}Q_{jk}P_{kj}(t).
%\end{align*}
%%Suffices to show that $\limsup_{h \rightarrow 0} \sum_{k \neq 1}\frac{P_{ik}(h)}{h}P_{kj}(t) \leq \sum_{k \neq 1}q_{jk}P_{kj}(t)$. 
For the upper bound, we observe for any finite subset $F \subseteq E$
\begin{align*}
\limsup_{h \downarrow 0}\sum_{k \neq i}\frac{P_{ik}(h)}{h}P_{kj}(t) &\leq \limsup_{h \downarrow 0}\left(\sum_{k \in F\setminus \{i\}}\frac{P_{ik}(h)}{h}P_{kj}(t)+\sum_{k \notin F\setminus\{i\} }\frac{P_{ik}(h)}{h}\right)\\
& = \limsup_{h \downarrow 0}\left(\sum_{k \in F\setminus \{i\}}\frac{P_{ik}(h)}{h}P_{kj}(t)+\frac{1-P_{ii}(h)}{h} -\sum_{k \in F\setminus \{i\}}\frac{P_{ik}(h)}{h} \right)\\
&= \sum_{k \in F\setminus \{i\}}Q_{ik}P_{kj}(t)+\nu_i- \sum_{k \in F\setminus \{i\}}Q_{ik}. %&=\sum_{k \neq 1}q_{ik}P_{kj}(t). 
\end{align*}
\end{proof}
\begin{thm}[forward equation] 
%Under suitable regularity conditions, 
For a homogeneous CTMC with transition matrix $P(t)$ and generator matrix $Q$, we have 
\eq{
\frac{dP(t)}{dt} &=P(t)Q.
}
%$P'_{ij}(t)=\sum_{k \neq i}P_{ik}(t)q_{kj}-P_{ij}(t)\nu_i$, i.e. $\frac{dP(t)}{dt}=P(t)Q$.
\end{thm}
\begin{proof}
Using semigroup property of transition probability matrix $P(t)$ for a homogeneous CTMC,  
we can write
\begin{align*}
\frac{P(t+h) - P(t)}{h} &= P(t)\frac{(P(h) - I)}{h}.
\end{align*}
Taking limits $h \downarrow 0$, we get 
\begin{align*}
\frac{dP_{ij}(t)}{dt} = \sum_{k \neq j}P_{ik}(t)Q_{kj}-\nu_jP_{ij}(t). 
\end{align*}
By taking limiting value for increasing sequence of finite sets $F \subseteq E$, we obtain the lower bound 
\eq{
\sum_{k \neq j}P_{ik}(t)Q_{kj} &\leq \liminf_{h \downarrow 0} \sum_{k \neq j}P_{ik}(t)\frac{P_{kj}(h)}{h}.
}
To obtain the upper bound, we observe for any finite subset $F \subseteq E$, 
\eq{
\limsup_{h \downarrow 0}\sum_{k \neq i}P_{ik}(t)\frac{P_{kj}(h)}{h} &\leq 
%\limsup_{h \downarrow 0}\left(\sum_{k \in F\setminus \{i\}}P_{ik}(t)\frac{P_{kj}(h)}{h}+\sum_{k \notin F\setminus\{i\} }\frac{P_{kj}(h)}{h}\right)\\& = 
\limsup_{h \downarrow 0}\left(\sum_{k \in F\setminus \{j\}}P_{ik}(t)\frac{P_{kj}(h)}{h}+\frac{1-P_{jj}(h)}{h} -\sum_{k \in F\setminus \{j\}}\frac{P_{kj}(h)}{h} \right).
%\\&= \sum_{k \in F\setminus \{j\}}P_{ik}(t)Q_{kj}+\nu_j- \sum_{k \in F\setminus \{j\}}Q_{kj}.
} 
\end{proof}

\begin{cor}
For a homogeneous CTMC with finite state space $E$, the transition matrix $P(t)$ and generator matrix $Q$, we have
\eq{
P(t) &= e^{tQ} = I + \sum_{n \in \N}\frac{t^nQ^n}{n!},~t\geqslant 0.
}
\end{cor}
\subsection{Generator matrix}
%\begin{defn}
A \textbf{generator matrix} denoted by $Q \in \R^{E \times E}$ is defined in terms of sojourn times $\{\nu_i, i \in E\}$ and jump transition probabilities $\{p_{ij}, i \neq j \in E\}$ of a CTMC as
\begin{enumerate}[i\_]
\item $q_{ii}= -\nu_i$,
\item $q_{ij}=\nu_i p_{ij}$. 
\end{enumerate}

%\end{defn}
\begin{lem} A matrix $Q$ is a generator matrix for a CTMC iff for each $i \in I$,
 \begin{enumerate}[i\_]
\item $0 \leq -q_{ii} < \infty$, 
\item $q_{ij} \geq 0$,
\item $\sum_{j \in I}q_{ij}=0$.
\end{enumerate}
\end{lem}

From the $Q$ matrix, we can construct the whole CTMC.  In DTMC, we had the result $P^{(n)}(i,j)=(P^n)_{i,j}$. We can generalize this notion  in the case of CTMC as follows: $P=e^{Q}\triangleq \sum_{k \in \mathbb{N}_0}\frac{Q^k}{k !}$.  Observe that $e^{Q_1+Q_2}=e^{Q_1}e^{Q_2},~ e^{nQ}=(e^Q)^n=P^n$.\\
\begin{thm}
Let $Q$ be a finite sized matrix. Let $P(t)=e^{tQ}$. Then $\{P(t),~ t \geq 0\}$ has the following properties:\begin{enumerate}
\item {$P(s+t)=P(s)P(t),~ \forall s,~t$ (semi group property).}
\item {$P(t),~t \geq 0$ is the unique solution to the forward equation, $\frac{dP(t)}{dt}=P(t)Q,~P(0)=I$.}
\item {And the backward equation $\frac{dP(t)}{dt}=QP(t),~P(0)=I$.}\\
\item {For all $k \in \mathbb{N}$, $\frac{d^kP(t)}{d^k(t)}|_{t=0}=Q^k$.}
\end{enumerate}
\end{thm}  
\begin{proof}
$\frac{dM(t)e^{-tQ}}{dt}=0,$ $M(t)e^{-tQ}$ is constant. $M(t)$ is any matrix satisfying the forward equation.
\end{proof}
\begin{thm}
A finite matrix $Q$ is a generator matrix for a CTMC iff $P(t)=e^{tQ}$ is a stochastic matrix for all $t \geq 0$. 
\end{thm}
\begin{proof}
$P(t)=I+tQ+O(t^2)$ ($f(t)=O(t) \Rightarrow \frac{f(t)}{t} \leq c,$ for small $t,~c < \infty$ ). $q_{ij} \geq 0$ if and only if $P_{ij}(t) \geq 0,~ \forall i \neq j$ and $t \geq 0$ sufficiently small. $P(t)=P(\frac{t}{n})^n$. Note that if $Q$ has zero row sums, $Q^n$ also has zero row sums.\\
\begin{flalign*}
\sum_j [Q^n]_{ij}&= \sum_j \sum_k [Q^{n-1}]_{ik}Q_{kj}= \sum_j \sum_k Q_{kj}[Q^{n-1}]_{ik}=0.\\
\sum_{j}P_{ij}(t)&=1+\sum_{n \in \mathbb{N}} \frac{t^n}{n!}\sum_j [Q^n]_{ij}=1+0=1.
\end{flalign*}  
Conversely $\sum_{j}P_{ij}(t)=1,~ \forall t \geq 0$, then $\sum_jQ_{ij}= \frac{dP_{ij}(t)}{dt}=0$.
\end{proof}

\appendix 
\section{Exponential random variables}
\begin{lem} 
Let $X$ be an exponential random variable, and $S$ be any positive random variable, independent of $X$. 
Then,
\eq{
\Pr\{X > S+u| X > S\} &= \Pr\{X > u\}.
}
\end{lem}
\begin{proof}
Let the distribution of $S$ be $F$. 
Then, using memoryless property of exponential random variable, we can write
\eq{
\Pr\{X > S+u| X > S\} &= \int_{0}^{\infty}dF(v)\Pr\{X > u + v| X > v\} = \Pr\{X > u\}\int_{0}^{\infty}dF(v) = \Pr\{X > u\}.
}
\end{proof}
\end{document}

%%%%%%%%
\subsection{Kolmogorov Differential Equations}
\begin{lem} For a CTMC with transition probability matrix $P(t)$, the following properties hold.
\begin{enumerate}
\item Limiting values of the transition probabilities are related to the generator matrix $Q$, i.e.
\begin{align}
\label{eq:limP}
\lim_{t \downarrow 0} \frac{P(t)-I}{t} &= Q.
\end{align}
%\begin{xalignat*}{3}
%&\lim_{t \downarrow 0} \frac{1-P_{ii}(t)}{t} =\nu_i,&&
%\lim_{t \downarrow 0} \frac{P_{ij}(t)}{t} =q_{ij}.
%\end{xalignat*}
\item The transition probability matrix has the semi group property, i.e.
\begin{align}
P(t+s) &= P(t)P(s), ~~s,t \geq 0.
\label{eqn:chapkolmogorov}
\end{align}
\end{enumerate}
\end{lem}
\begin{proof}
Since the probability of two of more transitions in $(0,t]$ is $o(t)$ it follows that $P_{ii}(t) = e^{-\nu_i t}+o(t)$, that is, the probability of being in state $i$ at time $t$ having started in state $i$ is the sum of the probability of either no transitions in $(0,t]$ and the probability of being in $i$ after two or more transitions, the latter being $o(t)$. Thus, we have
\begin{align}
\underset{t\downarrow 0}{\mathrm{lim }} \frac{1-P_{ii}(t) }{t} = \nu_i,
\end{align}
which asserts the diagonal terms in \eqref{eq:limP}, since $\nu_i = -q_{ii}.$
Similarly, for the off-diagonal terms, let $i\not = j,$ we have 
\begin{align}
P_{ij}(t) &= P(X(t)=j|\text{one transition in }(0,t])(1-e^{-\nu_i t})+ o(t)\\
&= P_{ij} \cdot (1-e^{-\nu_it})+o(t).
\end{align}
Dividing by $t$ and taking limits now establishes \eqref{eq:limP}.

Akin to the Chapman-Kolmogorov equations for the DTMC, we have
\begin{align*}
P_{ij}(t+s) &= P(X(t+s)=j|X(0)=i)\\
&=\sum_k P(X(t+s)=j|X(0)=i,X(t)=k)P(X(t)=k|X(0)=i)\\
&=\sum_k P_{kj}(s)P_{ik}(t),
\end{align*}
establishing \eqref{eqn:chapkolmogorov}.
\end{proof}

