% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\include{header}
\title{Lecture 12 : Limit Theorems for Markov Chains}
\author{}
\begin{document}
\maketitle
%\section{Discrete Time Markov Chains Contd}

\section{Limit Theorems}
Let $N_j(t)$ denote the number of transitions into state $j \in E$ up to time $t$, 
\eq{
N_j(t) = \sum_{k=1}^t1\{X_k = j\}.
}
Let $S_0 = 0$, then we can define the arrival instants of $j$th state as
\eq{
S_n = \inf\{ k > S_{n-1}: X_k = j\}.
}
If $X_0 = j$ and $j$ is recurrent, then $N_j(t)$ is a renewal process with the \textit{iid} inter-arrival distribution, 
\eq{
P\{S_n - S_{n-1} = k\} &= f_{jj}^{(k)},~k \in \N.
}
%\{P[\text{interarrival time} =n]\}_{n\geq 1}=\{f_{jj}^{(n)}\}_{n \geq 1}.$
If $X_0 = i \neq j$, for some $i \leftrightarrow j$ and $j$ recurrent, then $N_j(t)$ is a
delayed renewal process with first inter-arrival distribution
\eq{
P\{S_1 = k\} &= f_{ij}^{(k)},~k \in \N.
}
Hence from renewal theory, we have the following results. 
\begin{prop}
\end{prop}

If $i \leftrightarrow j$ then
\begin{enumerate}
	\item 
	\begin{align*}P\left[ \lim_{t \to \infty} \frac{N_j(t)}{t}= \frac{1}{\mu_{jj}} \vline X_0 = i\right] = 1\end{align*}
	where
 \begin{align*}\mu_{jj} = \left\{\begin{array}{l l}
	\infty & j \mbox{ transient} \\
	\sum_n n f_{jj}^{(n)} & j \mbox{ recurrent}	
	\end{array}\right.\end{align*}
      Here, $\mu_{jj}$ is the expected number of transitions needed to
      return to state $j$ beginning from $j$.
    \item
      \begin{align*}\lim_{n \to \infty} \frac{\sum_{k=1}^n p_{ij}^{(k)}}{n} =
      \lim_{n \to \infty} \frac{\E[N_j(n)] }{n} =
      \frac{1}{\mu_{jj}}\end{align*}

	\item If $j$ is aperiodic (\textit{i.e.,} $d(j)=1$), then \begin{align*} \lim_{n \to \infty} p_{ij}^{(n)} = \lim_{n \to \infty} \E[\# \mbox{ renewals at $n$}]  = \frac{1}{\mu_{jj}}\end{align*}
	\item If $j$ is periodic with period $d$, then \begin{align*} \lim_{n \to \infty} p_{ij}^{(nd)}= \lim_{n \to \infty} \E[\# \mbox{ renewals at $nd$}] = \frac{d}{\mu_{jj}}.\end{align*}
\end{enumerate}

\section{Positive and Null recurrence}
A recurrent state $j$ is said to be \textbf{positive recurrent} if $\mu_{jj} < \infty$ and \textbf{null recurrent} if $\mu_{jj} = \infty$. Let
\begin{align*}\pi_j = \lim_{n \to \infty} p_{jj}^{(nd)}\end{align*}
where d is the period of state $j$. Then $\pi_j > 0$ if and only if $j$ is positive recurrent and $\pi_j = 0$ if  $j$ is null-recurrent. 

\begin{prop}
Positive recurrence and null recurrence are class properties.
\end{prop}
\begin{defn}
An state that is aperiodic and positive recurrent is called \textbf{ergodic}.
\end{defn}

\begin{defn}
  A probability distribution $\{\pi_j\}_{j \in \N_0}$ is said to
  be \textbf{stationary} for the DTMC if $\forall j \in \N_0$
\begin{align*}\pi_j = \sum_{k \in \N_0} \pi_k p_{kj}\end{align*}
More compactly, $\pi$ is stationary if $\pi = \pi P$ where $P$ is the TPM.
\end{defn}

A thing to note is that if we start the DTMC with the stationary distribution, the distribution remains the same throughout the chain. That is, if we started the chain with $X_0 \sim \pi$ where $\pi$ is the stationary distribution, we would have for every $n \geq 1$, $X_n \sim \pi$. Moreover since $X_n$ is a DTMC, we have $X_n, X_{n+1}, ... X_{n+m}$ have the same joint distribution. Hence it is a stationary process. More precisely,
\begin{align*}(X_1,\ldots,X_s)\sim(X_{t+1},\ldots,X_{t+s}) \quad \forall t \geq 0.\end{align*}

\begin{thm}[Classification of DTMCs]
  An irreducible aperiodic DTMC is of one of the following two types:
\begin{enumerate}
	\item All states are either transient or null recurrent, in which case \begin{align*}\lim_{n \to \infty} p_{ij}^{(n)} = 0 \quad \forall i,j \in \N_0\end{align*} and there exists no stationary distribution.
	\item All states are positive recurrent (the DTMC is ergodic), wherein
          \begin{align*} \pi_j := \lim_{n \to \infty} p_{ij}^{(n)} > 0 \quad
          \forall i,j \in \N_0.\end{align*}
          Moreover, $(\pi_j)_{j \in \N_0}$ is the unique stationary
          distribution for the DTMC.
\end{enumerate}
\end{thm}
\begin{proof}
  Suppose that all states are either transient or null recurrent (note
  that exactly one of these will hold since there is only one
  communicating class). This implies that $\mu_{jj} = \infty$ for each
  $j \in \N_0$, and by Limit theorem $3$, we have that
  $\lim_{n \to \infty} p_{ij}^{(n)} = 1/\mu_{jj} = 0$ for each
  $i,j \in \N_0$.  

  Suppose, in this case, that a stationary distribution existed, say
  $\pi$. We then have $\pi_j = \sum_{i \in \N_0}\pi_i p_{ij}^{(n)}$,
  with $p_{ij}^{(n)} \leq 1$ $\forall n, i$. By the Dominated
  Convergence Theorem (DCT), for all $j \in \N_0$
  \begin{align*} \pi_j =\sum_{i \in \N_0} \pi_i \lim_{n \to \infty} p_{ij}^{(n)} =
  0, \end{align*}
  which contradicts $\pi$ being a stationary distribution, proving the
  first part of the theorem.  \\

  For the second part, assume that all states are positive recurrent
  and, thus, $\pi_j := \lim_{n \to \infty} p_{ij}^{(n)} =\frac{1}{\mu_{jj}}> 0$. We will
  show that $\pi$ is a stationary distribution for the DTMC and that
  it is unique. Observe that
  $\sum_{j=0}^Mp_{ij}^{(n)} \leq \sum_{j \in \N_0}p_{ij}^{(n)}
  = 1$
  for any fixed $M$.  Taking limits as $n \to \infty$ and then
  $M \to \infty$ on both sides yields \begin{align*}\sum_{j=0}^M \lim_{n \to \infty}p_{ij}^{(n)} \leq 1\end{align*}
\begin{align*}\Rightarrow\sum_{j=0}^\infty \pi_j \leq 1.\end{align*}
Now we have, 
\begin{align*}
p_{ij}^{(n+1)} &= \sum_{k \in \N_0} p_{ik}^{(n)}p_{kj}  \\
\Rightarrow \quad \lim_{n \to \infty}p_{ij}^{(n+1)} &= \lim_{n \to \infty}\sum_{k \in \N_0} p_{ik}^{(n)}p_{kj} \\
&\geq \lim_{n \to \infty}\sum_{k =0}^M p_{ik}^{(n)}p_{kj} \quad \mbox{(for any $M$)}\\
\Rightarrow \pi_j &\geq \sum_{k=0}^M \pi_k p_{kj} \quad \mbox{(by DCT)}\\
\Rightarrow \pi_j &\geq \sum_{k \in \N_0} \pi_k p_{kj}. \quad \mbox{($M \to \infty$)}\\
\Rightarrow \pi &\geq  \pi P.
\end{align*}
To show equality, suppose that the inequality above is strict for some
state $j$ in $\N_0$. Then, observe that
\begin{align*} \sum_{j \in \N_0} \pi_j > \sum_{j \in \N_0} \sum_{k
  \in \N_0} \pi_k p_{kj} = \sum_{k \in \N_0} \pi_k
\sum_{j \in \N_0} p_{kj} = \sum_{k \in \N_0} \pi_k\end{align*}
which is a contradiction. Hence it is an equality
$\forall j \in \N_0$, showing that $\pi$ must be a stationary
distribution. 

To prove uniqueness of $\pi$ as a stationary distribution, suppose
$\lambda$ is another stationary distribution that works here. Since
$\lambda$ is stationary, suppose we start the DTMC with this
distribution. Then we get
\begin{align*}
\lambda_j &= P[X_n = j] \\
&=\sum_{i=0}^\infty P[X_n =j | X_0 =i]P[X_0=i] \\
&= \sum_{i=0}^\infty p_{ij}^{(n)} \lambda_i \\
&\geq \sum_{i=0}^M p_{ij}^{(n)} \lambda_i
\end{align*}  
Let $n$ and then $M$ approach $\infty$. This yields
\begin{align*} \lambda_j \geq \sum_{i=0}^\infty \pi_j \lambda_i = \pi_j.\end{align*}
Now consider again,
\begin{align*}
\lambda_j &=  \sum_{i=0}^\infty p_{ij}^{(n)} \lambda_i \\
&= \sum_{i=0}^M p_{ij}^{(n)} \lambda_i  + \sum_{i=M+1}^\infty p_{ij}^{(n)} \lambda_i \\
&\leq \sum_{i=0}^M p_{ij}^{(n)} \lambda_i  + \sum_{i=M+1}^\infty \lambda_i \\
\end{align*}
Once again, letting $n$ and then $M$ approach $\infty$ yields the result.
\end{proof}
\begin{thm}
An irreducible, aperiodic Markov Chain with countable state space $I$ is of one of the following types:
\begin{enumerate}[i)]
\item All the states are either transient or null recurrent. That is, $\lim_{n \in \N}P_{ij}^n = 0$  and there exists no stationary distribution.
\item All the states are positive recurrent. There exists a unique stationary distribution $\pi \in \Delta(I)$,
\begin{align*}
 \pi_j &= \lim_{n \in \N} P^n_{ij} > 0,~ j \in I.
 \end{align*}
\end{enumerate}
\end{thm}
\begin{proof} Let $\{X_n: n \in \N\}$ be an irreducible, aperiodic Markov chain with state space $I$. 
\begin{enumerate}[i)]
\item 
If the states are transient or null recurrent and $P \in \Delta(I)$ is a stationary distribution, 
then for any $n \in \N$, we have
\begin{align*}
P_j = \sum_{i \in I} \Pr\{X_n = j|X_0 = i\} \Pr\{X_0 = i\} = \sum_{i \in I} \pi_i P_{ij}^n.
\end{align*} 
Since, $\pi_j = 0$ for all $j \in I$, we have $P_j=0$ for every $j \in I$. 
This contradicts the assumption that $P_j$ is a probability distribution. 
\item 
%We will initially prove the second case. 
From renewal reward theorem, $\lim_{n \in \N}P_{ij}^{(n)} = 1/\mu_{jj}$ exists, which we take as $\pi_j$. Further, for any finite set $A \subseteq I$, we have
\begin{align*}
\sum_{j \in A} P_{ij}^{(n)}&\leq \sum_{j \in I} P_{ij}^{(n)} = 1.
\end{align*}
Taking limit $n \in \N$ on both sides, we conclude that $\sum_{j \in A}\pi_j  \leq 1$ for all $A$ finite. Taking limit with respect to $A$, we conclude,  
\begin{align*}
\sum_{j \in I} \pi_j \leq 1.
\end{align*}
Further, we can write for all $A \subseteq I$,
\begin{align*}
P_{ij}^{n+1} &= \sum_{k \in I}P_{ik}^nP_{kj} \geq \sum_{k \in A}P_{ik}^nP_{kj}.
\end{align*}
Applying limit $n \in \N$ on both sides, %$P_{ij}^{n+1} \geq \sum_{k=0}^{M}P_{ik}^nP_{kj}$, 
we get $\pi_j \geq \sum_{k \in A} \pi_kP_{kj}$ for all $A$ finite. 
Hence, taking limits with respect to $A$, we get for all $j \in I$,
\begin{align*}
\pi_j &\geq \sum_{k\in I} \pi_kP_{kj}.
\end{align*}	
Assuming that the inequality is strict for some $j \in I$, we can sum the inequalities over $j$. 
Since, summands are non-negative we can exchange summation orders to get
\begin{align*}
\sum_{j \in I} \pi_j &> \sum_{j \in I} \sum_{k \in I}\pi_kP_{kj} = \sum_{k \in I} \pi_k \sum_{j \in I} P_{kj} = \sum_{k \in I} \pi_k.
%&(summation \: can\: be\: interchanged \:since\: \pi_k\: and\: P_{kj}\: are\: non-negative.)\\
\end{align*}	
This is a contradiction. Therefore, for all $j \in I$
\begin{align*}
&\pi_j = \sum_{k \in I} \pi_k P_{kj}.
\end{align*}
Defining normalized $P_j = \frac{\pi_j}{\sum_{k \in I}\pi_k}$, 
we see that $\{P_j,j \in I\}$ is a stationary distribution and so at least one stationary distribution exists. 
Let $\{P_j,j \in I\}$ be any stationary distribution. Let $\{P_j,j \in I\}$ be the probability distribution of $X_0$, then for any finite subset $A \subseteq I$, we get 
\begin{align*}
P_j &= \Pr\{X_n = j\} = \sum_{i \in I} \Pr\{X_n = j|X_0 = i\} P\{X_0 = i\} = \sum_{i=0}^{\infty} P_{ij}^nP_i \geq \sum_{i \in A} P_{ij}^nP_i.
\end{align*}
As before, we take limit $n \in \N$, followed by limit of increasing subsets $A \uparrow I$, to obtain
\begin{align*}
P_j &\geq \sum_{i \in I} \pi_j P_i = \pi_j.
\end{align*}
To show $P_j \leq \pi_j$, we use the fact that $P_{ij}^n \leq 1$. Let $A \subseteq I$ be a finite subset, then
\begin{align*}
P_j &= \sum_{i \in I} P_{ij}^nP_i = \sum_{i \in A}P_{ij}^nP_i + \sum_{i \in A^c}P_{ij}^nP_i \leq \sum_{i \in A} P_{ij}^{n}P_i + \sum_{i \in A^c} P_i.
\end{align*}
Using our standard approach of taking limit $n \in \N$, followed by $A \subseteq I$, we obtain
\begin{align*}
%&n \in \N \Rightarrow P_j \leq \sum_{i=0}^{M}\pi_jP_i + \sum_{i=M+1}^{\infty} P_i, \forall M\\
%&\because \sum_{0}^{\infty}P_i = 1, M \to \infty \Rightarrow 
P_j &\leq \sum_{i=0}^{\infty} \pi_j P_i = \pi_j.
\end{align*}
\end{enumerate}
%Thus for the first case, no stationary distribution exists and the proof is complete.
\end{proof}

\begin{cor} An irreducible, aperiodic DTMC defined on a finite state space $I$ will be positive recurrent.
\end{cor}
\begin{proof}
Suppose that the DTMC is not positive recurrent, then 
\begin{align*}
\lim_{n \in \N}P_{ij}^{(n)} &= 0.
\end{align*}
Interchanging limit and finite summation gives
\begin{align*}
0 = \sum_{j \in I}\lim_{n \in \N}P_{ij}^{(n)}  = \lim_{n \in \N}\sum_{j \in I}P_{ij}^{(n)} = 1.
\end{align*}
This is a contradiction. 
Hence the DTMC mentioned above is positive recurrent.
\end{proof}

Let $S_0 = 0$. 
For a state $i \in E$, we can define following stopping times for a Markov chain $X$ as 
\eq{
S_n(i)  &\triangleq \inf\{ n > S_{n-1}: X_n = i\}.
}
Clearly, $S(i) = \{S_n(i): n \in \N\}$ is a delay renewal process. 
From strong Markov property it follows that $X$ is a regenerative process with renewal points $\{S_n: n \in \N\}$. 
We can define the inter-renewal duration, the number of time steps to return to the state $i$ as 
\eq{T_n(i) \triangleq S_n(i) - S_{n-1}(i).} 
\begin{cor} 
For an irreducible and aperiodic Markov chain with stationary distribution $\pi$ on countable state space $E$, we have 
\eq{
\E_i[T_1(i)] &\triangleq \E[T_1(i)| X_0 = i] = \frac{1}{\pi_i},~i \in E.
} 
%for any state $i \in I$, 
%where $\E_i[T_1((i)] = \E[T_1(i)| X_0 = i]$ denotes the expected . 
\end{cor}
Further, we can define the number of visits to state $j$ during on renewal duration as 
\eq{
N_{ij} &= \sum_{k=1}^{S_1(i)}1\{X_n = j\}.
}
\begin{prop} 
For an aperiodic and irreducible Markov chain $X$ with stationary distribution $\pi$ on countable state space $E$, 
the mean number of visits during return to state $i$ is given 
\eq{
\E_iN_{ij} &= \frac{\pi_j}{\pi_i}.
}
\end{prop}
\begin{proof} 
Let $X_0 = i \in E$, then from renewal reward theorem for renewal instants $S(j)$ and definition of $\pi$, we can write 
\eq{
\lim_{n \in \N}P_i\{X_n = j\} &= \lim_{n \in \N}\frac{\sum_{k=1}^{n}1\{X_k = j\}}{n} = \frac{1}{\E_j[T_1(j)]} = \pi_j.
}
Result follows from rewriting of the above expression for renewal instants $S(i)$ as 
\eq{
\lim_{n \in \N}P_i\{X_n = j\} &= \lim_{n \in \N}\frac{\sum_{k=1}^{n}1\{X_k = j\}}{n} = \frac{\E_i\sum_{k=1}^{S_1(i)}1\{X_k = i\}}{\E_iS_1(i)} = \frac{\E_iN_{ij}}{\E_iT_1(i)} = \pi_i \E_iN_{ij}.
}
\end{proof}

\subsection{Ergodic theorem for Markov Chains} 

\begin{prop} Let $\{X_n\}$ be an irreducible, aperiodic and positive recurrent Markov chain on countable state space $I$ with stationary distribution $\pi$. 
Let $f : I \to \R$, such that $\sum_{i \in I} \arrowvert f(i) \arrowvert \pi_i < \infty$, that is $f$ is integrable over $I$ with respect to $\pi$.  
Then for any initial distribution of $X_0$, 
\eq{
\lim_{n\in \N}\frac{1}{n} \sum_{i=1}^{n} f(X_i) = \sum_{i \in I} \pi_if(i) \text{ almost surely}.
}
\end{prop}

\begin{proof}
Fix $X_0 = i \in I$. Let $S(i)$ be sequence of successive instants at which state $i$ is visited, with $S_0(i) = 0$. 
%Notice these are renewal instants.
For all $p \geq 0$,  let $R_{p+1} = \sum_{n = S_p(i) + 1}^{S_{(p + 1)}} f(X_n)$ be the net reward earned at the end of cycle $(p+1)$. 
Each cycle forms a renewal. 
By the strong Markov property, these cycles are independent. 
At each of these stopping times, Markov chain is in state $i$. 
%Average reward earned during the first $k$ cycles is given by 
%\begin{align*}	
%\frac{1}{n} \sum_{k=1}^{n} R_k &= \frac{1}{n} \sum_{k=1}^{n} \sum_{\tau_{k-1} + 1}^{\tau_k} f(X_i)= \frac{1}{n} \sum_{t=1}^{\tau_n} f(X_t).
%\end{align*}
From renewal reward theorem and applying previous corollary, we get
\begin{align*}
\lim_{n \in \N}\frac{\sum_{k=1}^nR_k}{n} &= %\frac{\E_i[\text{reward in one renewal cycle}]}{\E_i[\text{renewal cycle length}]} = 
\pi_i\E_i[\sum_{n=1}^{S_1(i)}f(X_n)] = \pi_i\E_i\sum_{n=1}^{S_1(i)}\sum_{j \in E}f(j)1\{X_n = j\}.
\end{align*}
Assuming $f$ is non-negative, and substituting the mean number of visits to state $j$ during successive return to state $j$,  we can write 
\begin{align*}
%&E_0[cycle \: length] = \frac{1}{\pi_0}\: (from \: Corollary \: 2)\\
%&E_0[cycle \: reward] = E_0[\sum_{n=0}^{\tau_1}f(X_n)]\\
%\E_i[\text{cycle reward}] &= \E_i\sum_{n=1}^{\tau_1} \sum_{j \in I} f(j) 1_{\{X_n = j\}} = \sum_{j \in I} f(j) \E_i\sum_{n=1}^{\tau_1} 1_{\{X_n = j\}}.
\lim_{n \in \N}\frac{\sum_{k=1}^nR_k}{n} &= \pi_i\E_i\sum_{j \in E}\sum_{n=1}^{S_1(i)}f(j)1\{X_n = j\} = \pi_i\sum_{j \in \E}\E_iN_{ij}f(j)= \sum_{j \in \E}\pi_jf(j). 
\end{align*}
%To calculate the reward given above, we need to find the expected number of visits to state $j$ between successive visits to state $i$. 
%We denote the average number of visits to state $j$ by  
%$Y_j = \lim_{n \in \N}\frac{\sum_{t=1}^{n} 1_{\{X_t = j\}}}{n}$. %Let us call it $\circledR$.\\
%We can compute $Y_j$ in two ways.
%\begin{enumerate}
%	\item DTMC undergoes a renewal each time it hits state $j$. 
%	So, from elementary renewal theorem, 
%	\begin{align*}
%	Y_j &= \E_i1_{\{X_n = j\}} = 1/(1/\pi_j) = \pi_j.
%	\end{align*}
%	\item Without loss of generality, renewal occurs whenever DTMC hits state $i$. Hence, $\E_i\tau_1 = 1/\pi_i$ and by renewal reward theorem, we obtain
%	\begin{align*}
%	Y_j &= \frac{\E_i[\sum_{n=1}^{\tau_1} 1_{\{X_n = j\}}]}{1/\pi_i}.
%	\end{align*}
%\end{enumerate}
%Combining above two, we get $\E_i[\sum_{t=1}^{\tau_1} 1_{\{X_t = j\}}] = \frac{\pi_j}{\pi_i}$ and the result follows. 
%%$\frac{1}{\tau_n} \sum_{t=0}^{\tau_n} f(X_t) \stackrel {n \in \N}{\to} \frac{\sum_{j \in I} f(j) \pi_j/\pi_i}{1/\pi_i}$
\end{proof}
\end{document}
%
%% !TEX spellcheck = en_US
%% !TEX spellcheck = LaTeX
%\documentclass[a4paper,10pt,english]{article}
%\input{header}
%\title{Lecture 11 : Time Reversibility of Discrete Time Markov Chains}
%\author{}
%\begin{document}
%\maketitle
%\section{Discrete Time Markov Chains Contd.}

\subsection{Time Reversibility of Discrete Time Markov Chains}

Consider a discrete time Markov chain with transition probability matrix $P$ and stationary probability vector $\pi$.\\
\textbf{Claim:} The reversed process is a Markov chain.
\begin{proof}
\begin{align*}
&P(X_{m-1}=i|X_m=j,X_{m+1}=i_{m+1} \hdots )=\frac{P(X_{m-1}=i,X_m=j, \hdots )}{P(X_m=j, X_{m+1}=i_{m+1}\hdots )}\\
&=\frac{P(X_{m-1}=i,X_m=j)P(X_{m+1}=i_{m+1}\hdots |X_{m-1}=i,X_m=j  )}{P(X_m=j)P(X_{m+1}=i_{m+1} \hdots |X_m=j)}\\
&\stackrel{(a)}{=}\frac{P(X_{m-1}=i,X_m=j)P(X_{m+1}=i_{m+1}\hdots |X_m=j  )}{P(X_m=j)P(X_{m+1}=i_{m+1} \hdots |X_m=j)}\\
&=P(X_{m-1}=i|X_m=j).\\
\end{align*}
where $(a)$ follows from the Markov property.
\end{proof}
The sequence $\{X_n,X_{n-1} \hdots \}$ is called reverse process. Let $P^*$ denote the transition probability matrix. 
\begin{align*}
P_{ij}^*&=P(X_{n-1}=j|X_n=i)=\frac{P(X_{n-1}=j,X_n=i)}{P(X_n=i)}\\
&=\frac{P(X_{n-1}=j)P(X_n=i|X_{n-1}=j)}{P(X_{n}=i)}\\
&=\frac{P(X_{n-1}=j)}{P(X_{n}=i)}P_{ji}
\end{align*}
suppose we are considering a stationary Markov chain, $P(X_n=l)=P(X_{n-1}=l)=\pi(l),~\forall l$, $\pi(i){P^*}_{ij}=\pi(j)(P)_{ji}$. If $P^*=P^T$ then the Markov chain is called time reversible. Thus the condition for time reversibility is given by $\pi P_{ij}=\pi_jP_{ji}$. Any non-negative vector $X$ satisfying $X_iP_{ij}=X_jP_{ji},~\forall i,j$ and $\sum_{j \in \mathcal{N}_0}X_j=1$ is stationary distribution of time-reversible Markov chain. This is true because,
\begin{align*}
\sum_{i} X_i P_{ij}=\sum_i X_j P_{ji}=X_j \sum X_i = 1.
\end{align*}
Since stationary probabilities are the unique solution of the above, the claim follows.\\

\textbf{Example 4.7(A) An Ergodic Random Walk}: Any ergodic, positive recurrent random walk is time reversible. The transition probability matrix is $P_{i,i+1}+P_{i-1,i}=1$. For every two transitions from $i+1$ to $i$, there must be one transition from $i$ to $i+1$. The rate of transitions from $i+1$ to $i$ must hence be same as the number of transitions from $i$ to $i+1$. So the process is time reversible.

\textbf{Example 4.7(B) The Metropolis Algorithm: } Let $a_j,~ j=1, \hdots m$ be positive numbers and let $A=\sum_{i=1}^{m}a_i$. Suppose that $m$ is large and $A$ is difficult to compute. One way to compute it by simulating a sequence of random variables is by generating a Markov chain whose limiting probabilities are $p_j$s. The Metropolis algorithm provides a convenient approach. \\
Let $Q$ be an irreducible transition probability matrix on the integers $1, \hdots n$ such that $q_{ij}=q_{ji}$ and for all $i$ and $j$. Generate a Markov chain $\{X_n\}$ such that the transition probabilities are given by 
\begin{align*}
P_{ij} = \left\{
     \begin{array}{lr}
       q_{ij}\min(1,a_j/a_i) & : j \neq i\\
       q_{ii}+\sum_{j \neq i}q_{ij}\{1-\min(1,a_j/a_i)\} & : j = i.
     \end{array}
   \right.
\end{align*} 
It can be directly verified that the chain is irreducible and hence verify that $p_j$s are the limiting probabilities.

\textbf{Edge weighted graphs:} Consider a graph having a positive number $w_{ij}$ associated with each edge $(i,j)$, and suppose that a particle moves from vertex to vertex in the following manner: If the particle is presently at vertex  $i$ then it will next move to vertex $j$ with probability
\begin{equation*}
P_{ij}=\frac{w_{ij}}{\sum_{j}w_{ij}}
\end{equation*}
where $w_{ij}$ is 0 if $(i,j)$ is not an edge of the graph. The Markov chain describing the sequence of vertices visited by the particle is called a random walk on an edge weighted graph. 
\begin{prop}
Consider a random walk on an edge weighted graph witha finite number of vertices. If this Markov chain is irreducible then it is, in steady state, time reversible with stationary probabilities given by 
\begin{equation*}
\pi_i = \frac{\sum_{i}w_{ij}}{\sum_{j}\sum_{i}w_{ij}}.
\end{equation*}
\end{prop}
\begin{proof}
The time reversibility equation
\begin{equation*}
\pi_iP_{ij}=\pi_{j}P_{ji}
\end{equation*}
reduces to 
\begin{equation*}
 \frac{\pi_i w_{ij}}{\sum_{k}w_{ik}}=\frac{\pi_j w_{ji}}{\sum_{k}w_{jk}}
\end{equation*}
But noting that $w_{ij}=w_{ji}$ and $\sum_{i}\pi_i = 1$, we get the desired result.
 
\end{proof}
\subsubsection*{Necessary condition for time reversibility}
If we try to prove the equations necessary for time reversibility, $X_iP_{ij}=X_jP_{jk}$ for all $i,j$, for any arbitrary Markov chain, one may not end up getting any solution. This is so because, if $P_{ij}P_{jk}>0$, then $\frac{X_i}{X_k}=\frac{P_{ji}P_{kj}}{P_{jk}P_{ij}} \neq \frac{P_{kj}}{P_{jk}}$.\\
Thus we see that a necessary condition for time reversibility is $P_{ij}P_{jk}P_{ki}=P_{ik}P_{kj}P_{ji},~ \forall i,j,k$. In fact we can show the following.
\begin{thm}
A stationary Markov chain is time reversible if and only if starting in state $i$
, any path back to state $i$ has the same probability as the reversed path, for all $i$. That is, if
\begin{align*}
P_{i i_1}P_{i_1 i_2}\hdots P_{i_k i}=P_{i,i_k}P_{i_k i_{k-1}} \hdots P_{i_1,i}.
\end{align*} 
\end{thm}
\begin{proof}
The proof of necessity is as indicated above. To see the sufficiency part, fix states $i,j$
\begin{align*}
&\sum_{i_1,i_2,\hdots i_{k}}P_{ii_1}\hdots P_{i_k,j}P_{j,i}=\sum_{i_1,i_2,\hdots i_{k}}P_{i,j}P_{j,i_k}\hdots P_{i_1 i}\\
&(P^k)_{ij}P_{ji}=P_{ij}(P^k)_{ji}\\
&\frac{\sum_{k=1}^{n}(P^k)_{ij}P_{ji}}{n}= \frac{\sum_{k=1}^{n}P_{ij}(P^k)_{ji}}{n}
\end{align*}
As limit $n \in \N$, we get the desire result.
\end{proof}

\begin{thm}
Consider irreducible Markov chain with transition matrix $P$. If one can find non-negative vector $\pi$ and other transition matrix $P^*$ such that $\sum_j \pi_j =1$ and $\pi_iP_{ij}=\pi_jP^*_{ji}$ then $\pi$ is the stationary probability vector and $P^*$ is the transition matrix for the reversed chain.
\end{thm}
\begin{proof}
Summing $\pi_iP_{ij}=\pi_jP_{ji}^*$ over $i$ gives, $\sum_{i}\pi_iP_{ij}=\pi_j$. Hence $\pi_i$s are the stationary probabilities of the forward and reverse process. Since $P_{ji}^*=\frac{\pi_iP_{ij}}{\pi_j}$, $P_{ij}^*$ are the transition probabilities of the reverse chain.
\end{proof} 

\subsection{Example 4.7(E): Example 4.3(C) revisited}
Example 4.3(C) was with regard to the age of a renewal process. $X_n$ the forward process there was such that it increases in steps of 1 until it hits a value chosen by the inter arrival distribution. Hence the reverse process should be such that it decreases in steps of 1 until it hits 1 and then jumps to a state as chosen by the inter arrival distribution. Thus letting $P_i$ as the probability of inter arrival, it seems likely that  $P_{1i}*=P_i, ~ P_{i,i-1}=1,~ i > 1$. We have that $P_{i,1}=\frac{P_i}{\sum_{j \geq 1}P_j}=1-P_{i,i+1}, ~ i \geq 1$. For the reversed chain to be given as above, we would need 
\begin{align*}
&\pi_i P_{ij}=\pi_j P_{ji}^*\\
&\pi_i \frac{P_i}{\sum_j P_j}=\pi_1 P_i\\
&\pi_i=\pi_1 P(X \geq i)\\
&1=\sum_i \pi_i=\pi_1 E[X]; \pi_i=\frac{P(X \geq i)}{E[X]}, 
\end{align*}
where $X$ is the inter arrival time. We need to verify that $\pi_i P_{i,i+1}=\pi_{i+1}P^*_{i+1,i}$ or equivalently $P(X \geq i)(1-\frac{P_i}{P(X \geq i)})=P(X \geq i)$ to complete the proof that the reversed process is the excess process and the limiting distributions are as given above. But that is immediate.
\subsection{Semi Markov Processes}
Consider a stochastic process with states $0,1,2 \hdots$ such that whenever it enters state $i$,
\begin{enumerate}
\item {The next state it enters is state $j$ with probability $P_{ij}$.}\\
\item {Given the next state is going to be $j$, the time until the next transition from state $i$ to state $j$ has distribution $F_{ij}$. If we denote the state at time $t$ to be $Z(t)$, $\{Z(t), t \geq 0\}$ is called a Semi Markov process.}
\end{enumerate}

Markov chain is a semi Markov process  with 

\begin{align*} 
  F_{ij}(t) = \left\{
     \begin{array}{lr}
       0 & : t \leq 1 \\
       1 & : t > 1.
     \end{array}
   \right.
\end{align*}

Let $H_i$ the distribution of time the semi Markov process stays in state $i$ before transition. We have $H_j(t)= \sum_i P_{ij}F_{ij}(t)$, $\mu_i = \int_0 ^ \infty X dH_i(x)$. Let $X_n$ denote the $n^{\text{th}}$ state visited. Then $\{X_n\}$ is a Markov chain with transition probability $P$ called the embedded Markov chain of the semi Markov process. \\
\textbf{Definition:} If the embedded Markov chain is irreducible, then Semi Markov process is said to be irreducible. Let $T_{ii}$ denote the time between successive transitions to state $i$. Let $\mu_{ii}=E[T_{ii}]$.
\begin{thm}
If semi Markov process ius irreducible and if $T_{ii}$ has non-lattice distribution with $\mu_{ii}< \infty$ then, 
\begin{align*}
P_i=\lim_{t \in \N}P(Z(t)=i|Z(0)=j)
\end{align*}
exists and is independent of the initial state. Furthermore, $P_i=\frac{\mu_i}{\mu_{ii}}$.
\end{thm}
\textbf{Corollary 4.8.2} If the semi-Markov process is irreducible and $\mu_{ii}<\infty$, then with probability 1,
$\frac{\mu_i}{\mu_{ii}}=\frac{\lim_{t \in \N} \text{Amount of time in [0,t]}}{t}~\text{a.s}$.\\
\begin{thm}
Suppose conditions of the previous thmrem hold and the embedded Markov chain is positive recurrent. Then $P_i= \frac{\pi_i\mu_i}{\sum_{i}\pi_j \mu_j}$. 
\end{thm} 
\begin{proof}
Define the notation as follows:

$Y_i(j)=$ amount of time spent in state $i$ during $j^\text{th}$ visit to that state. $i,j \geq 0$. \\
$N_i(m)=$ number of visits to state $i$ in the first $m$ transitions of the semi-Markov process.\\

The proportion of time in $i$ during the first $m$ transitions:\\

\begin{align*}
P_{i=m}&= \frac{\sum_{j=1}^{N_i(m)}Y_i(j)}{\sum_l \sum_{j=1}^{N_i(m)}Y_i(j) }\\
&= \frac{\frac{N_i(m)}{m}\sum_{j=1}^{N_i(m)}Y_i(j)}{\sum_l \frac{N_i(m)}{m} \sum_{j=1}^{N_i(m)}Y_i(j) }\\
\end{align*}
Since $N_i(m)\in \N$ as $m \in \N$, it follows from the strong law of large numbers that $\frac{\sum_{i=2}^{N_i(m)}Y_i(j)}{N_i(m)}\rightarrow \mu_i$ and $\frac{N_i(m)}{m}\rightarrow (E[\text{number of transitions between visits to state }i])^{-1}=\pi_i$. Letting $m \in \N$, result follows.
\end{proof}
\begin{thm}
If Semi Markov process is irreducible  and non lattice, then $\lim_{t \in \N}P(Z(t)=i,Y(t)>x,S(t)=j|Z(0)=k)=\frac{P_{ij}\int_x^\infty F_{ij}^c(y)d(y)}{\mu_{ii}}$. Let $Y(t)$ denote the time from $t$ until the next transition. $S(t)$ state entered at the first transition after $t$. 
\end{thm}
\begin{proof}
The trick lies in defining the "ON" time. 
\begin{align*}
E[\text{ON time in a cycle}]=E[(X_{ij}-x)^+].
\end{align*}
\end{proof}
\textbf{Corollary:} 
\begin{align*}
\lim_{t \in \N} P(Z(t)=i, Y(t) >x|Z(0)=k)= \frac{\int_{x}^{\infty}H_i^c(y)d(y)}{\mu_{ii}}.
\end{align*}

\end{document}
