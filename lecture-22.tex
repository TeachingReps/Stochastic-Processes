% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 22 : Martingales}
\author{}
\begin{document}
\maketitle
\section{Martingales}
A \textbf{filtration} is an increasing sequence of $\sigma$-fields, with $n$th $\sigma$-field denoted by $\F_n$. 
A sequence $X = \{X_n: n \in \N\}$ of random variables is said to be \textbf{adapted} to the filtration $\{\F_n: n \in \N\}$ if $X_n \in \F_n$. 
%A martingale is a type of stochastic process whose definition formalizes the concept of a fair game.
%\begin{defn}
A discrete stochastic process $\{X_n,~n \in \N \}$ is said to be a \textbf{martingale} with respect to $\{\F_n: n \in \N\}$ if 
\begin{enumerate}[i\_]
\item $\E[|X_n|]< \infty$,
\item $X_n$ is adapted to $\F_n$,
\item $\E[X_{n+1}|\F_n]=X_n$, for each $n \in \N$.
\end{enumerate}
If the equality in third condition is replaced by $\leq$ or $\geq$, then the process is called \textbf{supermartingale} or \textbf{submartingale}, respectively.
%\end{defn}
For a discrete stochastic process $X = \{X_n: n \in \N\}$, its \textbf{natural filtration} is defined as 
\eq{
\F_n \triangleq \sigma(X_1, \dots, X_n).
}
\begin{cor} For a martingale $X$ adapted to a filtration $\F$, for each $n \in \N$
\eq{
\E X_n &= \E X_1.
}
\end{cor}
%Taking expectation on both sides of part 2 of the above definition, we get $\E[Z_{n+1}]=\E[Z_n]$, and hence $\E[Z_{n+1}]=\E[Z_1]$, for all $n$.
%\end{cor}
\begin{shaded*}
\begin{exmp*}[Simple random walk]
Let $\{X_i: i \in \N\}$ be a sequence of independent random variables with mean $\E X_i = 0$ and $\E|X_i| < \infty$ for each $i \in \N$. 
Let $Z_n=\sum_{i=1}^n X_i$ and $\F_n = \sigma(X_1, \dots, X_n)$ for each $n \in \N$. 
Then, $\{Z_n,~n \in \N \}$ is a martingale with respect to the natural filtration of $X$.  
This follows, since $\E Z_n=0$ and 
\begin{align*}
\E[Z_{n+1}|\F_n] =\E[Z_{n}+X_{n+1}|\F_n] &=Z_n.
\end{align*} 
\end{exmp*}
\end{shaded*}
\begin{shaded*}
\begin{exmp*}[Product martingale]
Let $\{X_i: i \in \N\}$ be a sequence of independent random variables with mean $\E X_i = 1$ and $\E|X_i| < \infty$ for each $i \in \N$. 
Let $Z_n=\Pi_{i=1}^n X_i$ and $\F_n = \sigma(X_1, \dots, X_n)$. 
Then, $\{Z_n,~n \in \N \}$ is a martingale with respect to the natural filtration of $X$. 
This follows, since $\E Z_n=1$ and 
\begin{align*}
\E[Z_{n+1}|\F_n] =\E[Z_{n}X_{n+1}|\F_n] &=Z_n.
\end{align*} 
\end{exmp*}
\end{shaded*}

\begin{shaded*}
\begin{exmp*}[Branching process] 
Consider a population where each individual $i$ can produce an independent random number of offsprings $Z_i$ in its lifetime, given by a common distribution $P= \{P_j : j \in \N_0\}$ and mean $\mu = \sum_{j \in \N}jP_j$.  
Let $X_n$ denote the size of the $n$th generation, which is same as number of offsprings generated by $(n-1)$th generation. 
The discrete stochastic process $\{X_n \in \N_0: n \in \N\}$ is called a branching process. 
Let $X_0=1$ and $\F_n = \sigma(X_1, \dots, X_n)$, then,
\begin{align*}
X_n = \sum_{i=1}^{X_{n-1}}Z_i.
\end{align*}
%where $Z_i$ represents the number of offspring of the individual $i$ of the $(n-1)$th generation. 
Conditioning on $X_{n-1}$ yields, $\E[X_n]= \mu^n$ where $\mu$ is the mean number of offspring per individual. Then $\{Y_n = X_n / \mu^n: n \in \N\}$ is a martingale because $\E[Y_n]= 1$ and 
\begin{align*}
\E[Y_{n+1}|\F_n] %&= \frac{1}{\mu^{n+1}}\E[X_{n+1}|Y_1, \hdots Y_n]\\
&= \frac{1}{\mu^{n+1}}\E[\sum_{i=1}^{X_{n}}Z_i|\F_n]
%&=  \frac{1}{\mu^{n+1}}X_{n}\E[Z_i]
= \frac{X_n}{\mu^n}=Y_n.
\end{align*}
\end{exmp*}
\end{shaded*}

\begin{shaded*}
\begin{exmp*} [Doob's Martingale]
Let $X$ be an arbitrary random variable such that $\E[|X|]< \infty$, and $Y = \{Y_n: n \in \N\}$ be an arbitrary random sequence. 
Let $\F$ be the natural filtration associated with the stochastic process $Y$, then
\begin{align*}
X_n = \E[X|\F_n]
\end{align*}
is a martingale. 
The integrability condition can be directly verified, and
\begin{align*}
\E[X_{n+1}|\F_n]&= \E[\E[X|\F_{n+1}]|\F_{n}] = \E[X|\F_{n}] = X_n.
\end{align*} 
%Thus the result follows. The above martingale is called the Doob type martingale.
\end{exmp*}
\end{shaded*}

\begin{shaded*}
\begin{exmp*}[Centralized Doob sequence]
For any sequence of random variables $X = \{X_n: n \in \N\}$ and its natural filtration $\F$, 
the random variables $X_i-\E[X_i|\F_{i-1}]$ have zero mean, then 
\begin{align*}
Z_n =\sum_{i=1}^n (X_i -\E[X_i|\F_{i-1}])
\end{align*}
is a martingale with respect to $\F$, provided $\E |Z_n|< \infty$.  
To verify the same, 
\begin{align*}
\E[Z_{n+1}|\F_n]&= \E[Z_n+X_n-\E[X_n|\F_{n-1}]|\F_n] = Z_n+\E[X_n-\E[X_n|\F_{n-1}]]=Z_n.\\
\end{align*}
\end{exmp*}
\end{shaded*}
\subsection{Stopping Times}
Consider a discrete filtration $\F = \{\F_n: n \in \N_0\}$.  
%\begin{defn}
A positive integer valued, possibly infinite, random variable $N$ is said to be a \textbf{random time} with respect to the 
filtration $\F$,  if the event $\{N=n\} \in \F_n$ for each $n \in \N$. 
%process $\{X_n: n \in \N\}$ if the event $\{N=n\}$ is determined by the random variables $X_1 \hdots X_n$. 
%That is, if $\F$ is the natural filtration associated with the discrete process $X$, then $\{N = n \} \in \F_n$. 
If $\Pr\{N < \infty\}=1$, then the random time $N$ is said to be a \textbf{stopping time}. 
%\end{defn}
%\begin{defn}
%Consider a discrete filtration $\F = \{\F_n: n \in \N\}$.  
A sequence $\{H_n: n\in \N\}$ is \textbf{predictable} with respect to the the filtration $\F$, if $H_n \in \F_{n-1}$ for each $n \in \N$. 
Further, we define 
%process $\{X_n\}$ is the one where $H_n$ is completely determined by $X_1,X_2,...,X_{n-1}$
\begin{align*}
(H\cdot X)_n &\triangleq \sum_{m=1}^{n}H_m(X_m-X_{m-1}).
\end{align*}
%\end{defn}
\begin{thm}
Let $\{X_n: n \in \N_0\}$ be a super martingale with respect to a filtration $\F$. 
If $H = \{H_n: n \in \N\}$ is predictable with respect to $\F$ and each $H_n$ is non-negative and bounded, 
then $(H \cdot X)_n$ is a super martingale w.r.t. $\F$. 
\end{thm}
\begin{proof}
It follows from the definition, 
\begin{align*}
\E[(H\cdot X)_{n+1}|\F_n]=&\E[H_{n+1}(X_{n+1}-X_n)+(H\cdot X)_n|\F_n] = H_{n+1}(\E[X_{n+1}|\F_n]-X_n)+(H\cdot X)_n \leq (H\cdot X)_n.
\end{align*}
\end{proof}

\subsection{Stopped process}
%\begin{defn}
Consider a discrete stochastic process $X = \{X_n: n\in \N\}$ adapted to a discrete filtration $\F$. 
Let $T$ be a random time for the filtration $\F$, 
then the \textbf{stopped process} $\{X_{T\wedge n}: n \in \N\}$ is defined as 
\begin{align*}
X_{T\wedge n} = X_n1_{\{n \leq T\}} + X_T1_{\{n > T\}}.
\end{align*}
%\end{defn}
\begin{prop}
Let  $\{X_n: n\in \N\}$ be a martingale with a discrete filtration $\F$. 
If $T$ is an integer random time for the filtration $\F$, 
then the stopped process $\{X_{T\wedge n}\}$ is a martingale. 
\end{prop}
\begin{proof}
We observe that $H = \{1\{n\leq T\}: n \in \N\}$ is a non-negative, predictable, and bounded sequence, since 
\eq{
  \{n\leq T\}=\{T>n-1\}=\{T\leq n-1\}^c = (\cup_{i=0}^{n-1}\{T = i\})^c = \cap_{i=1}^{n-1}\{T \neq i\} \in \F_{n-1}. 
}
%Since, we have 
%\begin{align*}
%X_{T \wedge n}&= X_{T\wedge n-1}+1_{\{n \leq T\}}(X_n-X_{n-1}), 
%\end{align*}
In terms of the predictable and bounded sequence $H$, we can write the stopped process as
\eq{
X_{T \wedge n} &= X_0 + \sum_{m=1}^{T\wedge n}(X_{m} - X_{m-1}) = X_0 + \sum_{m=1}^{n}1\{m \leq T\}(X_{m} - X_{m-1}) = X_0 + (H \cdot X)_n. 
}
%\begin{align*}
%    X_{T\wedge n}&= (H\cdot X)_n\hspace{2 mm} when\hspace{2 mm} H_n=1_{\{n\leq T\}}\\
%    X_{T \wedge n}&= X_{T\wedge n-1}+1_{\{n \leq T\}}(X_n-X_{n-1})\\
%\end{align*}
% $n\leq T$:\hspace{2 mm}  $X_{T\wedge n}= X_n$\\
% $n>T$: Since $n>T$ gives $n-1\geq T$ therefore $T\wedge n-1\geq T$ which implies $X_{T\wedge n}=X_{T}$ \\
% \begin{align*}
%     X_{T\wedge n}= X_0 +  \sum_{m=1}^{n} 1_{\{m\leq T\}}(X_m-X_{m-1})
% \end{align*}
%     It is suffice to show $\{1_{n\leq T}\}$ is a predictable sequence which is true since 
%  \begin{align*}
%  \{n\leq T\}=\{T>n-1\}=\{T<n-1\}^c
%  \end{align*}
Therefore from the previous theorem we have 
\begin{align*}
      \E X_{T\wedge n} =\E X_{T\wedge 1} =\E X_{1}. 
\end{align*}


%We claim that 
%\begin{align*}
%\bar{Z}_n= \bar{Z}_{n-1}+1_{\{n \leq N\}}(Z_n-Z_{n-1})
%\end{align*}
%The above equation can be directly verified by considering the two cases separately by
%\begin{align*}
%\bar{Z}_n &= 
%\begin{cases}
%Z_n & n \leq N,\\
%\bar{Z}_{n-1}=Z_N, & n > N.
%\end{cases}
%\end{align*}
%Further, since $N$ is a random time, we see that 
%\begin{align*}
%\E[\bar{Z}_{n+1}|Z_1 \hdots \bar{Z}_n] %&=\E[\bar{Z}_{n}+1_{\{n \leq N\}}(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
%&=\bar{Z}_{n}+1_{\{n \leq N\}} \E[(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n] =\bar{Z}_{n}.
%\end{align*}
\end{proof}

\begin{rem}
For any martingale $\{X_n: n \in \N\}$ w.r.t. $\F$, we have $\E X_{T\wedge n} =\E X_1$, for all $n$.  
Now assume that $T$ is a stopping time w.r.t. $\F$. 
It is immediate that stopped process converges almost surely to $X_T$, i.e. 
\begin{align*}
\Pr\left\{\lim_{n\in \N}{X}_{T\wedge n} = X_T\right\} = 1.
\end{align*}
\end{rem}
We are interested in knowing under what conditions will we have convergence in mean. %, i.e.  
%\begin{align*}
%\lim_{n \in \N}\E X_{T\wedge n} = \E X_T. 
%\end{align*}
%It so turns out that the above is true under some additional regularity constraints only. %We state the following theorem without proof.
\begin{thm}[Martingale stopping theorem]
\label{MartStopThm}
Let $X$ be a martingale and $T$ be a stopping time adapted to a discrete filtration $\F$. 
Then, the random variable $X_T$ is integrable and the stopped process $X_{T \wedge n}$ converges in mean to $X_T$, i.e.
\eq{
\lim_{n \in \N}\E X_{T\wedge n} = \E X_T =\E X_1,
}
if 
%If $T$ is a stopping time for a martingale $\{X_n: n \in \N\}$ such that 
either of the following conditions holds true. 
\begin{enumerate}[(i)]
\item $T$ is bounded, 
\item $X_{T\wedge n}$ is uniformly bounded,
\item $\E T < \infty$, and for some real positive $K$, we have $\sup_{n \in \N}\E[|X_{n+1}-X_n||\F_n] < K$,
\end{enumerate}
%then $X_T$ is integrable and $\lim_{n \in \N}\E[X_{T\wedge n}] = \E[X_T]=\E[X_1]$.
\end{thm}
\begin{proof} We show this is true for all three cases.
\begin{enumerate}[(i)] 
\item Let $K$ be the bound on $T$ then for all $n \geq K$, we have $X_{T\wedge n} = X_T$, and hence it follows that
\begin{align*}
\E X_1 = \E X_{T\wedge n} &= \E X_T, ~\forall n \geq K.
\end{align*}
\item Dominated convergence theorem implies the result. 
\item Since $T$ is integrable and  
\begin{align*} 
X_{T\wedge n} \leq |X_1| + K T,
\end{align*}
we observe that $X_{T\wedge n}$ is bounded by an integrable random variable, and hence result follows from dominated convergence theorem.
\end{enumerate}
\end{proof}

\begin{cor}[Wald's Equation] 
If $T$ is a stopping time for $\{X_i: i \in \N\}$ \textit{iid} with $\E|X|< \infty$ and $\E T < \infty$, then
\begin{align*}
\E\sum_{i=1}^{T}X_i =\E T\E X.
\end{align*}
\end{cor}
\begin{proof}
Let $\mu=\E X$. Then $\{Z_n = \sum_{i=1}^{n}(X_i-\mu): n \in \N\}$ is a martingale adapted to natural filtration for $X$,  and hence from the Martingale stopping theorem, we have $\E Z_T =\E Z_1=0$. 
But 
\begin{align*}
\E[Z_T] %&= \E[\sum_{i=1}^T (X_i-\mu)]\\
%&=\E[\sum_{i=1}^N (X_i)-N\mu)]\\
&=\E\sum_{i=1}^T X_i- \mu\E T.
\end{align*}
Observe that condition $(iii)$  for Martingale stopping theorem to hold can be directly verified. 
Hence the result follows. 
\end{proof}
%\section{ Submartingales, supermartingales and the Martingale Convergence Theorem}
%%\begin{defn}
%A stochastic process $\{Z_n,~  n \geq 1\}$ having $\E[|Z_n|]< \infty$ for all $n$ is said to be a submartingale if
%\begin{equation}
%\label{Submartingale}
%\E[Z_{n+1}|Z_1 \hdots Z_n] \geq Z_n
%\end{equation}
%and is said to be a supermartingale if
%\begin{equation}
%\label{supermartingale}
%\E[Z_{n+1}|Z_1 \hdots Z_n] \leq Z_n
%\end{equation}
%%\end{defn}
%From \ref{Submartingale}, for a submartingale
%\begin{align*}
%\E[Z_{n+1}] \geq \E[Z_n]
%\end{align*}
%where the inequality is reversed for a supermartingale. 
%\begin{thm}
%\label{Stoppingtime_theorem}
%If $N$ is a stopping time for $\{Z_n,~ n\geq 1\}$ such that any one of the following sufficient conditions is satisfied:
%\begin{enumerate}
%\item $\bar{Z}_n$ is uniformly bounded, or;
%\item $N$ is bounded, or;
%\item $\E[N]< \infty$, and there is an $M < \infty$ such that
%\begin{align*}
%\E[|Z_{n+1}-Z_n| |Z_1, \hdots Z_n]<M,
%\end{align*}
%then,
%\begin{align*}
%\E[Z_N] \geq \E[Z_1] ~ \text {for a submartingale}\\
%\E[Z_N] \leq \E[Z_1] ~ \text {for a supermartingale}.
%\end{align*}
%\end{enumerate}
%\end{thm}
%\begin{proof}
%We claim that 
%\begin{align*}
%\bar{Z}_n= \bar{Z}_{n-1}+1_{N \geq n}(Z_n-Z_{n-1})
%\end{align*}
%The above equation can be directly verified by considering the two cases separately viz. 
%\begin{enumerate}
%\item $N \geq n$: $\bar{Z}_n=Z_n$.
%\item $N < n:$ $\bar{Z}_{n-1}=\bar{Z}_{n}=Z_N$
%\end{enumerate}
%\begin{align*}
%\E[\bar{Z}_{n+1}|Z_1 \hdots \bar{Z}_n]&=\E[\bar{Z}_{n}+1_{n \leq N}(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
%&\stackrel{(a)}{=}\bar{Z}_{n}+1_{n \leq N} \E[(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
%& \geq \bar{Z}_{n},
%\end{align*}
%where in $(a)$ we have used the fact that $N$ is a random time. Also, we have $\E[\bar{Z}_{n}]=\E[Z_1]$, for all $n$.  Now assume that $N$ is a stopping time. It is immediate that
%\begin{align*}
% \bar{Z}_n \rightarrow Z_N ~ \text{w.p}~ 1.
%\end{align*}
%But  is it true that
%\begin{align*}
% \E[\bar{Z}_n] \rightarrow \E[Z_N] ~ \text{as n}~ \rightarrow \infty.
%\end{align*}
%which gives that 
%\begin{align*}
%\E[Z_N ] \geq \E[Z_1].
%\end{align*}
%\end{proof}

Before we state and prove martingale convergence theorem, we state some results which will be used in the proof of the theorem.
\begin{lem}
\label{StoppingTimeBound}
If $\{X_i:  i \in \N \}$ is  a submartingale and $T$ is a stopping time such that $\Pr\{T \leq n\}=1$ then
\begin{align*}
 \E X_1 \leq \E X_T \leq \E X_n.
\end{align*}
\end{lem}
\begin{proof}
It follows from Theorem \ref{MartStopThm} that since $T$ is bounded, $\E X_T  \geq \E X_1$. 
Now, since $T$ is a stopping time, we see that for $\{T = k\}$
\begin{align*}
\E[X_n1\{T = k\}|\F_T,T=k]&= %\E[Z_n1\{T = k\}|\F_k,T=k] = 
\E[X_n1\{T = k\}|\F_k] \geq X_k1\{T = k \} = X_T1\{T = k\}.
\end{align*}
%where $(a)$ follows from the fact that $N$ is  a stopping time. 
Result follows by taking expectation on both sides and summing over $k$. 
\end{proof}

\begin{lem}
\label{ConvexFuncSubmart}
If $X = \{X_n: n \in \N\}$ is a martingale and $f$ is a convex function, then $\{f(X_n),n \in \N\}$ is a submartigale.
\end{lem}
\begin{proof}
The result is a direct consequence of Jensen's inequality.
\begin{align*}
\E[f(X_{n+1})|\F_n] &\geq f(\E[X_{n+1}|\F_n])=f(X_n).
\end{align*}
\end{proof}

\begin{con}
Let $X = \{X_n : n\in \N_0\}$ be a sub martingale. 
Let $a< b$ and $N_0  = -1$.
\begin{xalignat*}{3}
&N_{2k-1}=\inf\{m>N_{2k-2}:X_m\leq a \}, && N_{2k}=\inf\{m>N_{2k-1}:X_m\geq b \}.
\end{xalignat*}
The above quantities $N_{2k-1}$,$N_{2k}$ are stopping times and the set containing values of $m$ in the transition from $a$ to $b$ can be defined as
\begin{align*}
H_m &\triangleq \{N_{2k-1}<m\leq N_{2k} \}= %\{N_{2k-1} \leq m-1\} \cap \{m>N_{2k}\}^{c} = 
\{m-1\geq N_{2k-1}\}\cap\{m-1\geq N_{2k}\}^{c} \in \F_{m-1}.		
\end{align*}
Clearly, the event of $X$ being in an up crossing at time $m$ is  predictable. 
The number of up crossings completed in time $n$ is
\eq{
U_n &= \sum_{k=1}^nH_k = \sup\{k: n \geq N_{2k}\}.
}
%Since the above set depends on $\{m-1\}$ values instead of $\{m\}$ values, So
%\begin{align*}
%H_m=&1_{\{N_{2k-1}<m\leq  N_{2k}\}}\\
%U_n=&\sup\{k:N_{2k}\leq n\}
%\end{align*}
%$H_m$ defines a predictable sequence and $U_n$ is the number of up crossings completed in time $n$.
\end{con}
\begin{lem}[Upcrossing inequality]
If $X$ is a sub martingale, 
then for $Y_n \triangleq a+ (X_n-a)^{+}$, we have 
\begin{align*}
(b-a)\E U_n\leq& \E Y_n-\E Y_0.
%\text{where }Y_n&:=a+ (X_n-a)^{+}
\end{align*}
\end{lem}
\begin{proof}
Since $X$ is a submartingale so is $Y$, as $Y_n$ is a convex function of $X_n$. 
Since each up crossing has a gain slightly more than $b-a$, the following inequality exists, 
\begin{align*}
(b-a)U_n\leq &(H\cdot Y)_n = \sum_{m=1}^{n}1_{\{N_{2k-1}<m\leq N_{2k}\}}(Y_{m+1}-Y_{m}) = \sum_{k=1}^{U_n}(Y_{N_{2k+1}}-Y_{N_{2k+1}}).
\end{align*}
Now let $K_m=1-H_m$, then $K$ is a predictable sequence, and
\eq{
Y_n-Y_0 &= (H\cdot Y)_n+(K\cdot Y)_n.
}
From the submartingale property of Y, it follows
\eq{
\E[(K\cdot Y)_n] &\geq \E[(K\cdot Y)_0]=0. 
} 
Therefore, it follows that 
\eq{
\E(Y_n - Y_0) &= \E(H\cdot Y)_n+\E(K\cdot Y)_n \geq \E(H\cdot Y)_n  \geq  (b-a)\E U_n.
}
\end{proof}

\begin{thm}[Martingale convergence theorem] 
\label{MartingaleConvergenceTheorem} 
If $X$ is a submartingale with ${\sup}_{n\in \N} \E X_n^{+} < \infty$ then ${\lim}_{n\in \N} X_n=X$ a.s with $\E[X]<\infty$.
\end{thm}
\begin{proof} 
Since $(X-a)^{+}\leq X^{+}+|a|$, it follows from upcrossing inequality that   		
\begin{align*}
\E U_n \leq& \frac{\E X_n^{+} +|a|}{b-a}.
\end{align*}
The number of upcrossings $U_n$ increases with $n$, however the mean $\E U_n$ is bounded above for each $n \in \N$. 
Hence, $\lim_{n \in \N}\E U_n$ exists and is finite. 
    			$\lim_{n \in \N} U_n=U$ since, $\E [X_n^{+}] < \infty$ gives $U<\infty$ a.s. This conclusion leads to  
    			\begin{align*}
    			\Pr\{_{a,b \in \mathbb{Q}}{\cup}\{{\lim \inf}_{n\in \N}  X_n<a<b<{\lim \sup}_{n\in \N}  X_n \}\} = 0.
    			\end{align*}
    		  From the above probability we have a. s.
    		  \begin{align*}
    		  	{\lim \sup}_{n\in \N}  X_n &={\lim \inf}_{n\in \N}  X_n.
    		  	\end{align*}
    		  	Now the Fatou's lemma in measure theory guarantees
    		  	\begin {align*}
    		  	\E[X^{+}]\leq {\lim \inf}_{n\in \N}\E[X_n^{+}]<\infty,
    		  \end{align*}
    		  	which implies $X<\infty$  almost sure. To see $X>-\infty$, we observe that
    		  	\begin{align*}
    		  		\E[X_n^{-}]&=\E[X_n^{+}]-\E[X_n]\\
    		  		&\leq \E[X_n^{+}]-\E[X_0].
    		  		\end{align*} 
    		  		The above inequality comes from the submartingale property of $X_n$. Now from another application of Fatou's lemma gives,
    		  		\begin{align*}
    		  		\E[X^{-}]\leq {\lim \inf} _{n\in \N}\E[X_n^{-}] \leq {\sup}_{n\in \N}\E[X_n^{+}]-\E[X_0]<\infty.
    		  		\end{align*}
    		\end{proof}

%\begin{thm}[Martingale Convergence Theorem]
%\label{MartingaleConvergenceTheorem}
%If $\{Z_n,~n \geq 1\}$ is a martingale such that for some $M< \infty$
%\begin{align*}
%\E[|Z_n|] \leq M, ~ \text{for all}~ n
%\end{align*}
%then, with probability 1, $\lim_{n \rightarrow \infty}Z_n$ exists and is finite.
%\end{thm}
%\begin{proof}
%Assume $\E[Z_n^2]< \infty$ which is stronger than $\E[|Z_n|]< \infty$ (as a consequence of %Jensen's inequality). Observe that $\{Z_n^2\}$ is a submartingale (from Lemma %\ref{ConvexFuncSubmart}). Thus $\E[Z_n^2]<\infty$ and is non-decreasing in $n$. Thus, as $n %\rightarrow \infty$, $\E[Z_n^2]$ converges and let $\mu<\infty$ be given by $\mu=\lim_{n %\rightarrow \infty}\E[Z_n^2]$.
%\begin{equation}
%\label{KolmoBound}
%\Pr(\cup_{k \leq n} \{|Z_{m+k}-Z_m|> \epsilon\} )
%\end{equation}  
%\begin{align*}
%&\stackrel{(a)}{\leq }\E[(Z_{m+n}-Z_m)^2]/\epsilon^2
%&=\E[Z_{m+n}^2-2Z_mZ_{m+n}+Z_m^2]/\epsilon^2.
%\end{align*}
%Note that 
%\begin{align*}
%\E[Z_{m+n}Z_m]&=\E[\E[Z_mZ_{m+n}|Z_m]]\\
%&=\E[Z_m\E[Z_{m+n}|Z_m]]\\
%&=\E[Z_m^2].
%\end{align*}
%From \ref{KolmoBound}, 
%\begin{align*}
%\Pr(\cup_{k \leq n} \{|Z_{m+k}-Z_m|> \epsilon\}) \leq \frac{\E[Z_{m+n}^2]-\E[Z_m^2]}{\epsilon^2}.
%\end{align*}
%Letting $n \rightarrow \infty$
%\begin{align*}
%\Pr(\cup_{k \leq 1} \{|Z_{m+k}-Z_m|> \epsilon\}) \leq \frac{\mu-\E[Z_m^2]}{\epsilon^2}.
%\end{align*}
%Hence,
%\begin{align*}
%\Pr(\cup_{k \leq n} \{|Z_{m+k}-Z_m|> \epsilon\}) \rightarrow 0 ~\text{as}~ m \rightarrow \infty.
%\end{align*}
%Thus with probability 1, $\{Z_n\}$ will be  a Cauchy sequence, and thus $\lim_{n \rightarrow %\infty}Z_n$ will exist and be finite.`
%\end{proof}
%\begin{cor}
%If $\{Z_n,~m \geq 0\}$ is a non-negative martingale, then, with probability 1, $\lim_{n %\rightarrow \infty}Z_n$ exists and is finite.
%\end{cor}
%\begin{proof}
%Since $Z_n$ is non-negative,
%\begin{align*}
%\E[|Z_n|]=\E[Z_n]=\E[Z_1].
%\end{align*}
%\end{proof}

\end{document}