% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 18: Reversibility}
\author{}
\begin{document}
\maketitle

\section{Introduction}
%\begin{defn}
A stochastic process $\{X(t) \in I: t \in T\}$ is \textbf{reversible} if $(X(t_i): i \in [n])$ has the same distribution as $(X(\tau-t_i): i \in [n])$ for all $t_i, \tau \in T, i \in [n]$.
%\end{defn}
\begin{lem} 
A reversible process is stationary.
\end{lem}
\begin{proof}
Since $X(t)$ is reversible, both $(X(t_i): i\in [n])$ and $(X(\tau+t_i): i \in [n])$ have the same distribution as $(X(-t_i): i \in [n])$.
\end{proof}
\begin{thm} 
\label{thm:characterization1}
A stationary Markov chain with state space $I$ and probability transition matrix $P$ is reversible iff there exists a probability distribution $\pi$, that satisfy the detailed balanced conditions
\begin{align}
\label{eqn:DetBalCdnMC}
\pi_iP_{ij} &= \pi_jP_{ji},~~~ \forall i,j \in I.
\end{align}
When such a distribution $\pi$ exists, it is the equilibrium distribution of the process.
\end{thm}
\begin{proof} We assume that $X(t)$ is reversible, and hence stationary. We denote the stationary distribution by $\pi$, and by reversibility of $X(t)$ we have
\begin{align*}
\Pr\{X(t) = i , X(t+1) = j\} &= \Pr\{X(t) = j, X(t+1) = i\},
\end{align*}
and hence we obtain the detailed balanced conditions~\eqref{eqn:DetBalCdnMC}. 

Conversely, let $\pi$ be the distribution that satisfies the detailed balanced conditions, then
summing up both sides over $j \in I$, we see that this distribution is the equilibrium distribution.
Let $j_i \in I$ for $i \in [m]$, and we write
\begin{align*}
\Pr\{X(t+i-1) = j_{i-1}, i \in [m]\} &= \pi(j_0)\prod_{i=1}^mP(j_{i-1},j_{i}),\\
\Pr\{X(t'+i-1) = j_{m-i+1}, i \in [m]\} &= \pi(i_m)\prod_{j=m}^{1}P(j_i,j_{i-1}).
\end{align*}
From detailed balanced equations~\eqref{eqn:DetBalCdnMC} it follows that RHS of above two equations are identical. Taking $\tau = t+t'+m$, we deduce that $X(t)$ is reversible.
\end{proof}
\begin{thm} A stationary Markov process with state space $I$ and generator matrix $Q$ is reversible iff there exists a probability distribution $\pi$, that satisfy the detailed balanced conditions
\begin{align}
\label{eqn:DetBalCdnMP}
\pi_iQ_{ij} &= \pi_jQ_{ji},~~~ \forall i,j \in I.
\end{align}
When such a distribution $\pi$ exists, it is the equilibrium distribution of the process.
\end{thm}
\begin{proof} We assume that $X(t)$ is reversible, and hence stationary. We denote the stationary distribution by $\pi$, and by reversibility of $X(t)$ we have
\begin{align*}
\Pr\{X(t) = i , X(t+\tau) = j\} &= \Pr\{X(t) = j, X(t+\tau) = i\},
\end{align*}
and hence we obtain the detailed balanced conditions~\eqref{eqn:DetBalCdnMP} by taking limit $\tau \to 0$. 

Conversely, let $\pi$ be the distribution that satisfies the detailed balanced conditions, then
summing up both sides over $j \in I$, we see that this distribution is the equilibrium distribution. 
Consider now the behavior of stationary process $X(t)$ in $[-T,T]$. 
Process may start at time $-T$ in state $j_1$ and sees $m$ states by time $T$. 
For $i \in [m-1]$, we can define 
\begin{xalignat*}{5}
&S_1 = -T,&&S_{i+1} = \inf\{t > S_{i}: X(t) \neq X(S_{i})\},&&S_{m+1} = T. 
\end{xalignat*}
That is, the process spends period $S_{i+1} - S_{i}$ in state $j_{i}$ for $i \in [m]$, and transitions to state $j_{i+1}$ at instant $S_{i+1}$ for $i \in [m-1]$. 
Probability of this event is
\begin{align*}
\Pr\{X(t) = j_{i}, ~~t \in [S_{i}, S_{i+1}), i \in [m] \} &= \pi(j_1)\prod_{i=1}^{m-1}Q(j_{i},j_{i+1})\prod_{i=1}^me^{-\nu(j_i)(S_{i+1}-S_i)}. 
\end{align*}
Consider the stationary process that start in state $j_m$ at time $\tau-T$ %and sees $m$ states 
such that, for $i \in [m]$
\begin{align*}
X(t) = j_{i}, &~~t \in [\tau-S_{i+1}, \tau-S_{i}).
\end{align*}
Probability of this event is
\begin{align*}
\Pr\{X(t) = j_{i}, ~~t \in [\tau-S_{i+1}, \tau-S_{i}), i \in [m]\} &= \pi(j_m)\prod_{i=2}^mQ(j_i,j_{i-1})\prod_{i=1}^me^{-\nu(j_i)(S_{i+1}-S_i)}.
\end{align*}
From detailed balance equation~\eqref{eqn:DetBalCdnMP} it follows that
\begin{align*}
\pi(j_1)\prod_{i=1}^{m-1}Q(j_{i},j_{i+1})&= \pi(j_m)\prod_{i=2}^mQ(j_i,j_{i-1}).
\end{align*}
Hence, it follows that $X(t)$ is reversible.
%stays in this state for $h_m$ duration before jumps to state $-T+h_m$
%Equality in above equation follows from detailed balance equations and the RHS is probability of event that process starts at time $-T$ in state $j_m$ and transitions $m-1$ times, 
%where it stays in state $j_i$ for time $h_i$ and transitions to state $j_{i-1}$ afterwards. 
%From detailed balanced equations it follows that RHS of above two equations are identical. Taking $\tau = t+t'+m$, we deduce that $X(t)$ is reversible.
\end{proof}

%\begin{defn} 
The \textbf{probability flux} from state $i$ to state $j$ is defined as $\pi_iQ_{ij}$.
%\end{defn}

\begin{lem} 
\label{lem:fluxbalance}
For a stationary Markov process, probability flux balances across a cut $A \subseteq I$, that is
\begin{align*}
\sum_{i \in A}\sum_{j \notin A} \pi_iQ_{ij} &= \sum_{i \in A}\sum_{j \notin A} \pi_jQ_{ji}.
\end{align*}
\end{lem}
\begin{proof} From full balance condition $\pi Q = 0$, we get
\begin{align*}
\sum_{j \in A}\sum_{i \in I}\pi_iQ_{ij} = \sum_{j \in A}\sum_{i \in I}\pi_j Q_{ji} = 0.
\end{align*}
Further, we have the following identity
\begin{align*}
\sum_{j \in A}\sum_{i \in A}\pi_iQ_{ij} = \sum_{j \in A}\sum_{i \in A}\pi_j Q_{ji}.
\end{align*}
Subtracting the second identity from the first, we get the result.
\end{proof}
\begin{cor} 
For $A= \{i\}$, the above equation reduces to the full balance equation for state $i$, i.e.,
\begin{align*}
\sum_{i \neq j}\pi_iQ_{ij} &= \sum_{j \neq i}\pi_j Q_{ji} \\
\Rightarrow \; \sum_{j \in I}\pi_i Q_{ij} - \pi_iQ_{ii} &= \sum_{j \in I}\pi_j Q_{ji} - \pi_i Q_{ii} \\
\Rightarrow \; 0 &=  \sum_{j \in I}\pi_j Q_{ji}.
\end{align*}
\end{cor}

%\subsection{Necessary condition for time reversibility}
{\bf Is every Markov chain reversible?} If we try to prove the equations necessary for time reversibility, $x_iP_{ij}=x_jP_{ji}$ for all $i,j \in I$, for any arbitrary Markov chain, one may not end up getting any solution. 
This is so because, if $P_{ij}P_{jk}>0$, then 
\eq{
\frac{x_i}{x_k}=\frac{P_{kj}P_{ji}}{P_{ij}P_{jk}} \neq \frac{P_{ki}}{P_{ik}}.
} 
Thus, we see that a necessary condition for time reversibility is 
\eq{
P_{ij}P_{jk}P_{ki}=P_{ik}P_{kj}P_{ji} \text{  for all  }i,j,k \in I.
} 
%In fact we can show the following.
\begin{thm}[Kolmogorov's criterion for reversibility of Markov chains]
A stationary Markov chain is time reversible if and only if starting in state $i$, 
any path back to state $i$ has the same probability as the reversed path, for all initial states $i \in I$. 
That is, if
\begin{align*}
P_{i i_1}P_{i_1 i_2}\hdots P_{i_k i}=P_{ii_k}P_{i_k i_{k-1}} \hdots P_{i_1i}.
\end{align*} 
\end{thm}

\begin{proof}
The proof of necessity is as indicated above. To see the sufficiency part, fix states $i,j$. For any positive integer $k$, we compute
\begin{align*}
(P^k)_{ij}P_{ji}& = \sum_{i_1,i_2,\hdots i_{k}}P_{ii_1}\hdots P_{i_kj}P_{ji}=\sum_{i_1,i_2,\hdots i_{k}}P_{ij}P_{ji_k}\hdots P_{i_1 i}=P_{ij}(P^k)_{ji}. 
\end{align*}
Taking $k \to \infty$ and noticing that $(P^k)_{ij} \stackrel{k \to \infty}{\to} \pi_j$ $\forall i,j \in I$, we get the desired result by appealing to Theorem \ref{thm:characterization1}. 
%From this we conclude that
%\eq{
%\frac{1}{n}\sum_{k=1}^{n}(P^k)_{ij}P_{ji}&= \frac{1}{n}\sum_{k=1}^{n}P_{ij}(P^k)_{ji}.
%}
%As limit $n \rightarrow \infty$, we get the desired result.
\end{proof}


\end{document}
