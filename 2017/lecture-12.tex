% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\include{header}
\title{Lecture 12 : Limit Theorems for Markov Chains}
\author{}
\begin{document}
\maketitle
%\section{Discrete Time Markov Chains Contd}

\section{Limit Theorems}
Let $N_j(t)$ denote the number of transitions into state $j \in E$ up to time $t$. 
That is,
\eq{
N_j(t) = \sum_{k=1}^t1\{X_k = j\}.
}
Let $S_0 = 0$, then we can define the $n$th arrival instants of state $j$ as a stopping time 
\eq{
S_n(j) = \inf\{ k > S_{n-1}(j): X_k = j\}.
}
%Clearly, $S(i) = \{S_n(i) : n \in \N\}$ is a delayed renewal process. 
From strong Markov property it follows that $X$ is a regenerative process with regenerative sequence  
$S(j) = \{S_n(j) : n \in \N\}$. 
We can define the inter-renewal duration, the number of time steps to return to the state $j$ as
%We can define the $n$th inter-arrival time for state $j$ as 
\eq{
T_n(j) &= S_n(j) - S_{n-1}(j).
}
If $X_0 = j$ and $j$ is recurrent, then $S(j)$ is a renewal process with the \textit{iid} inter-arrival distribution, 
\eq{
P_j\{T_1(j) = k\} &= f_{jj}^{(k)},~k \in \N.
}
%\{P[\text{interarrival time} =n]\}_{n\geq 1}=\{f_{jj}^{(n)}\}_{n \geq 1}.$ 
Let $\mu_{jj} = \E_jT_1(j)$ be the mean inter-arrival time for the renewal process. %expected number of transitions needed to return to state $j$ beginning from state $j$. 
Then, 
 \begin{align*}
 \mu_{jj} &=\begin{cases}
	\infty & j \text{ transient}, \\
	\sum_{k \in \N} k f_{jj}^{(k)} & j \text{ recurrent}.\end{cases}
\end{align*}
If $X_0 = i \neq j$, for some $i \leftrightarrow j$ and $j$ recurrent, then $S(j)$ is a
delayed renewal process with first inter-arrival distribution 
\eq{
P_i\{T_1(j) = k\} &= f_{ij}^{(k)},~k \in \N.
}
The associated counting process $N_j(t)$ has the inverse relationship with the renewal process $S(j)$. 
From the renewal theory, we have the following results. 

\begin{prop}[basic renewal theorem] 
If $i \leftrightarrow j$, then 
$P_i\left\{ \lim_{n \in \N} \frac{N_j(n)}{n}= \frac{1}{\mu_{jj}}\right\} = 1$.
\end{prop}

%If $i \leftrightarrow j$ then
%\begin{enumerate}
%	\item 
%	\begin{align*}P\left[ \lim_{t \to \infty} \frac{N_j(t)}{t}= \frac{1}{\mu_{jj}} \vline X_0 = i\right] = 1\end{align*}
%	where
\begin{prop}[elementary renewal theorem] 
If $i \leftrightarrow j$, then 
\begin{align*}
\lim_{n \in \N} \frac{\sum_{k=1}^n p_{ij}^{(k)}}{n} = \lim_{n \in \N} \frac{\E_i[N_j(n)] }{n} = \frac{1}{\mu_{jj}}.\end{align*}
\end{prop}

\begin{prop}[Blackwell's theorem]
 If $j$ is aperiodic (\textit{i.e.,} $d(j)=1$), then 
 \begin{align*} \lim_{n \in \N} p_{ij}^{(n)} &= \lim_{n \in \N} \E_i[\# \text{ renewals at $n$}]  = \frac{1}{\mu_{jj}}
 \end{align*}
	\item If $j$ is periodic with period $d$, then 
\begin{align*} 
\lim_{n \in \N} p_{ij}^{(nd)} &= \lim_{n \in \N} \E_i[\# \text{ renewals at $nd$}] = \frac{d}{\mu_{jj}}.
\end{align*}
\end{prop}

\section{Positive and Null recurrence}
A recurrent state $j$ is said to be \textbf{positive recurrent} if $\mu_{jj} < \infty$ and \textbf{null recurrent} if $\mu_{jj} = \infty$. 
Let
\begin{align*}
\pi_j &\triangleq \lim_{n \in \N} p_{jj}^{(nd)},
\end{align*}
where d is the period of state $j$. Then $\pi_j > 0$ if and only if $j$ is positive recurrent and $\pi_j = 0$ if  $j$ is null-recurrent. 

\begin{prop}
Positive recurrence and null recurrence are class properties.
\end{prop}
%\begin{defn}
An state that is aperiodic and positive recurrent is called \textbf{ergodic}.
%\end{defn}
%\begin{defn}
For a homogeneous Markov chain on state space $E$ with transition probability matrix $E$, a probability distribution $\{\pi_j: j \in E\}$ is said to be \textbf{stationary} if for all states $j \in E$
\begin{align*}
\pi_j = \sum_{k \in E} \pi_k P_{kj}.
\end{align*}
More compactly, $\pi$ is stationary if $\pi = \pi P$. % where $P$ is the transition probability matrix.
%\end{defn}

Observe that for a Markov chain starting with its stationary distribution,  
then the distribution remains invariant for all times. % throughout the chain. 
That is, if $\pi$ is the stationary distribution, and the Markov chain has initial distribution $\nu(0) = \pi$ at time $0$, 
then at any time $n \in \N$, the Markov chain has distribution $\nu(n) = \pi$. 
%That is, if we started the chain with distribution $\pi$ for $X_0$, we would have for every $n \geq 1$, $X_n \sim \pi$. 
Moreover since $X_n$ has discrete states in $E$, the finite collection $(X_n, X_{n+1}, ... X_{n+m})$ have the same joint distribution. 
Hence it is a stationary process, and for all $k, m \in \N, i \in E^k$
\begin{align*}
P\{X_1 = i_1,\ldots,X_k = i_k\} = P\{X_{m+1} = i_1,\ldots,X_{m+k} = i_k\}.
\end{align*}

%\begin{thm}[classification of Markov chains]
%An irreducible aperiodic Markov chain is of one of the following two types. 
%\begin{enumerate}[i)]
%\item All states are either transient or null recurrent, in which case for all states $i,j \in E$
%\begin{align*}
%\lim_{n \in \N} P_{ij}^{(n)} = 0,
%\end{align*} 
%and there exists no stationary distribution. 
%\item All states are positive recurrent, and hence the chain is ergodic. 
%In this case, for all states $i,j \in E$
%\begin{align*} 
%\pi_j \triangleq \lim_{n \in \N} P_{ij}^{(n)} > 0.
%\end{align*}
%Moreover, $\{\pi_j : j \in E\}$ is the unique stationary distribution for the Markov chain.
%\end{enumerate}
%\end{thm}
%\begin{proof}
%Suppose that all states are either transient or null recurrent. 
%Note that exactly one of these will hold since there is only one communicating class. 
%This implies that $\mu_{jj} = \infty$ for each state $j \in E$, and it follows from Blackwell's theorem applied to renewals for Markov chains that for any states $i, j \in E$
%\begin{align*}
%\lim_{n \in \N} P_{ij}^{(n)} = \frac{1}{\mu_{jj}} = 0.
%\end{align*}  
%If there existed a stationary distribution $\pi$ in this case. 
%For any step size $n \in \N$ and states $i,j \in E$, we would then have
%\begin{xalignat*}{3}
%&\pi_j = \sum_{i \in E}\pi_i P_{ij}^{(n)},&&P_{ij}^{(n)} \leq 1.
%\end{xalignat*}
%We can change limits and summation using dominated convergence theorem, to get for any state $j \in E$
%\begin{align*} 
%\pi_j &=\sum_{i \in E} \pi_i \lim_{n \in \N} P_{ij}^{(n)} = 0.
%\end{align*}
%This contradicts $\pi$ being a stationary distribution, proving the first part of the theorem. 
%
%For the second part, assume that all states are positive recurrent.
%Therefore, $\pi_j = \lim_{n \in \N} P_{ij}^{(n)} = {1}/{\mu_{jj}}> 0$. 
%We will show that $\pi$ is a stationary distribution for the Markov chain and that it is unique. 
%Observe that for any finite set $M \subset E$,
%\eq{
%\sum_{j \in M}P_{ij}^{(n)} \leq \sum_{j \in E} P_{ij}^{(n)}= 1.
%}
%Taking increasing limits as $n \to \infty$ and then $M \uparrow E$ on both sides yields 
%\eq{
%\lim_{M \uparrow E}\sum_{j \in M} \lim_{n \in \N} P_{ij}^{(n)}  = \sum_{j \in E} \pi_j \leq 1.
%}
%Further, we can write for any finite $M \subset E$
%\eq{
%P_{ij}^{(n+1)} &= \sum_{k \in E} P_{ik}^{(n)} P_{kj}  \geq \sum_{k \in M} P_{ik}^{(n)}P_{kj}.
%}
%Taking limits first with increasing $n$ and then with increasing sets $M \uparrow E$, we get
%\eq{
%\pi &\geq  \pi P.
%}
%%\begin{align*}
%%\Rightarrow \quad \lim_{n \to \infty} P_{ij}^{(n+1)} &= \lim_{n \to \infty}\sum_{k \in \N_0} P_{ik}^{(n)} P_{kj} \\
%%&\geq \lim_{n \to \infty}\sum_{k =0}^M p_{ik}^{(n)}p_{kj} \quad \text{(for any $M$)}\\
%%\Rightarrow \pi_j &\geq \sum_{k=0}^M \pi_k p_{kj} \quad \text{(by DCT)}\\
%%\Rightarrow \pi_j &\geq \sum_{k \in \N_0} \pi_k p_{kj}. \quad \text{($M \to \infty$)}\\
%%\Rightarrow \pi &\geq  \pi P.
%%\end{align*}
%To show equality, suppose that the inequality above is strict for some
%state $j$ in $\N_0$. Then, observe that
%\begin{align*} \sum_{j \in \N_0} \pi_j > \sum_{j \in \N_0} \sum_{k
%  \in \N_0} \pi_k p_{kj} = \sum_{k \in \N_0} \pi_k
%\sum_{j \in \N_0} p_{kj} = \sum_{k \in \N_0} \pi_k\end{align*}
%which is a contradiction. Hence it is an equality
%$\forall j \in \N_0$, showing that $\pi$ must be a stationary
%distribution. 
%
%To prove uniqueness of $\pi$ as a stationary distribution, suppose
%$\lambda$ is another stationary distribution that works here. Since
%$\lambda$ is stationary, suppose we start the DTMC with this
%distribution. Then we get
%\begin{align*}
%\lambda_j &= P[X_n = j] \\
%&=\sum_{i=0}^\infty P[X_n =j | X_0 =i]P[X_0=i] \\
%&= \sum_{i=0}^\infty p_{ij}^{(n)} \lambda_i \\
%&\geq \sum_{i=0}^M p_{ij}^{(n)} \lambda_i
%\end{align*}  
%Let $n$ and then $M$ approach $\infty$. This yields
%\begin{align*} \lambda_j \geq \sum_{i=0}^\infty \pi_j \lambda_i = \pi_j.\end{align*}
%Now consider again,
%\begin{align*}
%\lambda_j &=  \sum_{i=0}^\infty p_{ij}^{(n)} \lambda_i \\
%&= \sum_{i=0}^M p_{ij}^{(n)} \lambda_i  + \sum_{i=M+1}^\infty p_{ij}^{(n)} \lambda_i \\
%&\leq \sum_{i=0}^M p_{ij}^{(n)} \lambda_i  + \sum_{i=M+1}^\infty \lambda_i \\
%\end{align*}
%Once again, letting $n$ and then $M$ approach $\infty$ yields the result.
%\end{proof}
\begin{thm}
An irreducible, aperiodic Markov Chain with countable state space $E$ is of one of the following types. 
\begin{enumerate}[i)]
\item All the states are either transient or null recurrent. 
For all states $i, j \in E$,
\eq{
\lim_{n \in \N}P_{ij}^n = 0,
}
and there exists no stationary distribution.
\item All the states are positive recurrent, and hence the chain is ergodic. 
There exists a unique stationary distribution $\pi \in \Delta(E)$, defined for all $i,j \in E$
\begin{align*}
 \pi_j &\triangleq \lim_{n \in \N} P^n_{ij} > 0.
 \end{align*}
\end{enumerate}
\end{thm}
\begin{proof} Let $\{X_n: n \in \N\}$ be an irreducible, aperiodic Markov chain with countable state space $E$. 
\begin{enumerate}[i)]
\item 
Suppose that all states are either transient or null recurrent. 
Note that exactly one of these will hold since there is only one communicating class. 
This implies that $\mu_{jj} = \infty$ for each state $j \in E$, and it follows from Blackwell's theorem applied to renewals for Markov chains that for any states $i, j \in E$
\begin{align*}
\lim_{n \in \N} P_{ij}^{(n)} = \frac{1}{\mu_{jj}} = 0.
\end{align*}  
If there existed a stationary distribution $\pi \in \Delta(E)$ in this case. 
For any step size $n \in \N$ and states $i,j \in E$, we would then have
\begin{xalignat*}{3}
&\pi_j = \sum_{i \in E}\pi_i P_{ij}^{(n)},&&P_{ij}^{(n)} \leq 1.
\end{xalignat*}
We can change limits and summation using dominated convergence theorem, to get for any $j \in E$
\begin{align*} 
\pi_j &=\sum_{i \in E} \pi_i \lim_{n \in \N} P_{ij}^{(n)} = 0.
\end{align*}
This contradicts $\pi$ being a stationary distribution, proving the first part of the theorem. 

%If the states are transient or null recurrent and $w \in \Delta(E)$ is a stationary distribution, 
%then for any $n \in \N$, we have
%\begin{align*}
%w_j = \sum_{i \in E} \Pr\{X_n = j|X_0 = i\} \Pr\{X_0 = i\} = \sum_{i \in E} \pi_i P_{ij}^n.
%\end{align*} 
%Since, $\pi_j = 0$ for all $j \in E$, we have $w_j=0$ for every $j \in E$. 
%This contradicts the assumption that $w_j$ is a probability distribution. 
\item 
We assume that all states are positive recurrent.
%We will initially prove the second case. 
From the theorem hypothesis, elementary renewal theorem, and positive recurrence, we get
\eq{
\pi_j &= \lim_{n \in \N}P_{ij}^{(n)} = 1/\mu_{jj} > 0.
} 
%exists, which we take as $\pi_j$. 
Further, for any finite set $A \subseteq E$, we have
\begin{align*}
\sum_{j \in A} P_{ij}^{(n)}&\leq \sum_{j \in E} P_{ij}^{(n)} = 1.
\end{align*}
Taking limit $n \in \N$ on both sides, we conclude that $\sum_{j \in A}\pi_j  \leq 1$ for all $A$ finite. 
Taking limit with respect to increasing sets $A \uparrow E$, we conclude,  
\begin{align*}
\sum_{j \in E} \pi_j \leq 1.
\end{align*}
Further, we can write for all $A \subseteq E$,
\begin{align*}
P_{ij}^{n+1} &= \sum_{k \in E}P_{ik}^nP_{kj} \geq \sum_{k \in A}P_{ik}^nP_{kj}.
\end{align*}
Applying limit $n \in \N$ on both sides, %$P_{ij}^{n+1} \geq \sum_{k=0}^{M}P_{ik}^nP_{kj}$, 
we get $\pi_j \geq \sum_{k \in A} \pi_kP_{kj}$ for all $A$ finite. 
Hence, taking limits with respect to increasing sets $A \uparrow E$, we get for all state $j \in E$,
\begin{align*}
\pi_j &\geq \sum_{k\in E} \pi_kP_{kj}.
\end{align*}	
Assuming that the inequality is strict for some state $j \in E$, we can sum the inequalities over all states $j \in E$. 
Since, summands are non-negative we can exchange summation orders to get
\begin{align*}
\sum_{j \in E} \pi_j &> \sum_{j \in E} \sum_{k \in E}\pi_kP_{kj} = \sum_{k \in E} \pi_k \sum_{j \in E} P_{kj} = \sum_{k \in E} \pi_k.
%&(summation \: can\: be\: interchanged \:since\: \pi_k\: and\: P_{kj}\: are\: non-negative.)\\
\end{align*}	
This is a contradiction. 
Therefore, for any state $j \in E$
\begin{align*}
\pi_j &= \sum_{k \in E} \pi_k P_{kj}.
\end{align*}
Defining normalized $w_j = \frac{\pi_j}{\sum_{k \in I}\pi_k}$, 
we see that $\{w_j : j \in E\}$ is a stationary distribution and so at least one stationary distribution exists. 
%Let $\{\lambda_j,j \in I\}$ be initial stationary distribution for this positive recurrent Markov chain. 
If the initial distribution of this positive recurrent Markov chain is a stationary distribution $\{\lambda_j : j \in E\}$, 
%Let $\{P_j,j \in I\}$ be the probability distribution of $X_0$, 
then for any finite subset $A \subseteq E$, we get 
\begin{align*}
\lambda_j &= \Pr\{X_n = j\} %= \sum_{i \in I} \Pr\{X_n = j|X_0 = i\} P\{X_0 = i\} 
= \sum_{i \in E} P_{ij}^n\lambda_i \geq \sum_{i \in A} P_{ij}^n\lambda_i.
\end{align*}
As before, we take limit $n \in \N$, followed by limit of increasing subsets $A \uparrow E$, to obtain
\begin{align*}
\lambda_j &\geq \sum_{i \in E} \pi_j \lambda_i = \pi_j.
\end{align*}
To show $\lambda_j \leq \pi_j$, we use the fact that $P_{ij}^n \leq 1$. 
Let $A \subseteq E$ be a finite subset, then
\begin{align*}
\lambda_j &= \sum_{i \in I} P_{ij}^n\lambda_i = \sum_{i \in A}P_{ij}^n\lambda_i + \sum_{i \notin A}P_{ij}^n\lambda_i \leq \sum_{i \in A} P_{ij}^{n}\lambda_i + \sum_{i \notin A} \lambda_i.
\end{align*}
Using our standard approach of taking limit $n \in \N$, followed by $A \uparrow E$, we obtain
\begin{align*}
%&n \in \N \Rightarrow P_j \leq \sum_{i=0}^{M}\pi_jP_i + \sum_{i=M+1}^{\infty} P_i, \forall M\\
%&\because \sum_{0}^{\infty}P_i = 1, M \to \infty \Rightarrow 
\lambda_j &\leq \sum_{i \in E} \pi_j \lambda_i = \pi_j.
\end{align*}
\end{enumerate}
%Thus for the first case, no stationary distribution exists and the proof is complete.
\end{proof}

\begin{cor} 
An irreducible, aperiodic Markov chain defined on a finite state space $E$ will be positive recurrent.
\end{cor}
\begin{proof}
Suppose that the Markov chain is not positive recurrent, then 
\begin{align*}
\lim_{n \in \N}P_{ij}^{(n)} &= 0.
\end{align*}
Interchanging limit and finite summation gives
\begin{align*}
0 = \sum_{j \in E}\lim_{n \in \N}P_{ij}^{(n)}  = \lim_{n \in \N}\sum_{j \in E}P_{ij}^{(n)} = 1.
\end{align*}
This is a contradiction. 
Hence the above mentioned chain is positive recurrent.
\end{proof}

%Let $S_0 = 0$. 
%For a state $i \in E$, we can define following stopping times for a Markov chain $X$ as 
%\eq{
%S_n(i)  &\triangleq \inf\{ n > S_{n-1}: X_n = i\}.
%}
%Clearly, $S(i) = \{S_n(i): n \in \N\}$ is a delayed renewal process. 
%From strong Markov property it follows that $X$ is a regenerative process with renewal points $\{S_n: n \in \N\}$. 
%We can define the inter-renewal duration, the number of time steps to return to the state $i$ as 
%\eq{T_n(i) \triangleq S_n(i) - S_{n-1}(i).} 
\begin{cor} 
For an irreducible and aperiodic Markov chain with stationary distribution $\pi$ on countable state space $E$, we have 
\eq{
\E_j[T_1(j)] &\triangleq \E[T_1(j)| X_0 = j] = \frac{1}{\pi_j},~j \in E.
} 
%for any state $i \in I$, 
%where $\E_i[T_1((i)] = \E[T_1(i)| X_0 = i]$ denotes the expected . 
\end{cor}
Further, we can define the number of visits to state $i$ during one renewal duration $S_1(j)$ as 
\eq{
%N_{ji} &= 
N_i(S_1(j)) = \sum_{k=1}^{S_1(j)}1\{X_n = i\}.
}
\begin{prop} 
For an aperiodic and irreducible Markov chain $X$ with stationary distribution $\pi$ on countable state space $E$, 
the mean number of visits to state $i$ in one return to state $j$ is given 
\eq{
\E_jN_{i}(S_1(j)) &= \frac{\pi_i}{\pi_j}.
}
\end{prop}
\begin{proof} 
Let $X_0 = j \in E$, then from renewal reward theorem for renewal sequence $S(i)$ and definition of $\pi$, %we can write 
\eq{
\lim_{n \in \N}P_j\{X_n = i\} &= \lim_{n \in \N}\frac{\sum_{k=1}^{n}1\{X_k = i\}}{n} = \frac{1}{\E_i[T_1(i)]} = \pi_i.
}
Result follows from rewriting of the above expression for renewal sequence $S(j)$ as 
\eq{
\lim_{n \in \N}P_j\{X_n = i\} &= \lim_{n \in \N}\frac{\sum_{k=1}^{n}1\{X_k = i\}}{n} = \frac{\E_j\sum_{k=1}^{S_1(j)}1\{X_k = i\}}{\E_jS_1(j)} = \frac{\E_jN_{i}(S_1(j))}{\E_jT_1(j)} = \pi_j \E_jN_{i}(S_1(j)).
}
\end{proof}

\subsection{Ergodic theorem for Markov Chains} 

\begin{prop} Let $\{X_n: n \in \N_0\}$ be an irreducible, aperiodic, and positive recurrent Markov chain on countable state space $E$ with stationary distribution $\pi$. 
Let $f : E \to \R$, such that $\sum_{i \in E} \arrowvert f(i) \arrowvert \pi_i < \infty$, that is $f$ is integrable over $E$ with respect to $\pi$.  
Then for any initial distribution of $X_0$, 
\eq{
\lim_{n\in \N}\frac{1}{n} \sum_{i=1}^{n} f(X_i) = \sum_{i \in E} \pi_if(i) \text{ almost surely}.
}
\end{prop}

\begin{proof}
Fix $X_0 = i \in E$. 
Let $S(i)$ be sequence of successive instants at which state $i$ is visited, with $S_0(i) = 0$. 
%Notice these are renewal instants.
For all $p \geq 0$,  let $R_{p+1} = \sum_{n = S_p(i) + 1}^{S_{(p + 1)}} f(X_n)$ be the net reward earned at the end of cycle $(p+1)$. 
Each cycle forms a renewal. 
By the strong Markov property, these cycles are independent. 
At each of these stopping times, Markov chain is in state $i \in E$. 
%Average reward earned during the first $k$ cycles is given by 
%\begin{align*}	
%\frac{1}{n} \sum_{k=1}^{n} R_k &= \frac{1}{n} \sum_{k=1}^{n} \sum_{\tau_{k-1} + 1}^{\tau_k} f(X_i)= \frac{1}{n} \sum_{t=1}^{\tau_n} f(X_t).
%\end{align*}
Since $\E_iS_1(i) = 1/\pi_i$, we get from renewal reward theorem %and applying previous corollary, we get
\begin{align*}
\lim_{n \in \N}\frac{\sum_{k=1}^nR_k}{n} &= %\frac{\E_i[\text{reward in one renewal cycle}]}{\E_i[\text{renewal cycle length}]} = 
\pi_i\E_i[\sum_{n=1}^{S_1(i)}f(X_n)] = \pi_i\E_i\sum_{n=1}^{S_1(i)}\sum_{j \in E}f(j)1\{X_n = j\}.
\end{align*}
%Assuming $f$ is non-negative, and 
Using dominated convergence theorem, and substituting the mean number of visits to state $j$ during successive return to state $j$,  we can write 
\begin{align*}
%&E_0[cycle \: length] = \frac{1}{\pi_0}\: (from \: Corollary \: 2)\\
%&E_0[cycle \: reward] = E_0[\sum_{n=0}^{\tau_1}f(X_n)]\\
%\E_i[\text{cycle reward}] &= \E_i\sum_{n=1}^{\tau_1} \sum_{j \in I} f(j) 1_{\{X_n = j\}} = \sum_{j \in I} f(j) \E_i\sum_{n=1}^{\tau_1} 1_{\{X_n = j\}}.
\lim_{n \in \N}\frac{\sum_{k=1}^nR_k}{n} &= \pi_i\E_i\sum_{j \in E}f(j)\sum_{n=1}^{S_1(i)}1\{X_n = j\} = \pi_i\sum_{j \in \E}f(j)\E_iN_{j}(S_1(i))= \sum_{j \in \E}\pi_jf(j). 
\end{align*}
%To calculate the reward given above, we need to find the expected number of visits to state $j$ between successive visits to state $i$. 
%We denote the average number of visits to state $j$ by  
%$Y_j = \lim_{n \in \N}\frac{\sum_{t=1}^{n} 1_{\{X_t = j\}}}{n}$. %Let us call it $\circledR$.\\
%We can compute $Y_j$ in two ways.
%\begin{enumerate}
%	\item DTMC undergoes a renewal each time it hits state $j$. 
%	So, from elementary renewal theorem, 
%	\begin{align*}
%	Y_j &= \E_i1_{\{X_n = j\}} = 1/(1/\pi_j) = \pi_j.
%	\end{align*}
%	\item Without loss of generality, renewal occurs whenever DTMC hits state $i$. Hence, $\E_i\tau_1 = 1/\pi_i$ and by renewal reward theorem, we obtain
%	\begin{align*}
%	Y_j &= \frac{\E_i[\sum_{n=1}^{\tau_1} 1_{\{X_n = j\}}]}{1/\pi_i}.
%	\end{align*}
%\end{enumerate}
%Combining above two, we get $\E_i[\sum_{t=1}^{\tau_1} 1_{\{X_t = j\}}] = \frac{\pi_j}{\pi_i}$ and the result follows. 
%%$\frac{1}{\tau_n} \sum_{t=0}^{\tau_n} f(X_t) \stackrel {n \in \N}{\to} \frac{\sum_{j \in I} f(j) \pi_j/\pi_i}{1/\pi_i}$
\end{proof}
\end{document}
%
%% !TEX spellcheck = en_US
%% !TEX spellcheck = LaTeX
%\documentclass[a4paper,10pt,english]{article}
%\input{header}
%\title{Lecture 11 : Time Reversibility of Discrete Time Markov Chains}
%\author{}
%\begin{document}
%\maketitle
%\section{Discrete Time Markov Chains Contd.}

\subsection{Time Reversibility of Discrete Time Markov Chains}

Consider a discrete time Markov chain with transition probability matrix $P$ and stationary probability vector $\pi$.\\
\textbf{Claim:} The reversed process is a Markov chain.
\begin{proof}
\begin{align*}
&P(X_{m-1}=i|X_m=j,X_{m+1}=i_{m+1} \hdots )=\frac{P(X_{m-1}=i,X_m=j, \hdots )}{P(X_m=j, X_{m+1}=i_{m+1}\hdots )}\\
&=\frac{P(X_{m-1}=i,X_m=j)P(X_{m+1}=i_{m+1}\hdots |X_{m-1}=i,X_m=j  )}{P(X_m=j)P(X_{m+1}=i_{m+1} \hdots |X_m=j)}\\
&\stackrel{(a)}{=}\frac{P(X_{m-1}=i,X_m=j)P(X_{m+1}=i_{m+1}\hdots |X_m=j  )}{P(X_m=j)P(X_{m+1}=i_{m+1} \hdots |X_m=j)}\\
&=P(X_{m-1}=i|X_m=j).\\
\end{align*}
where $(a)$ follows from the Markov property.
\end{proof}
The sequence $\{X_n,X_{n-1} \hdots \}$ is called reverse process. Let $P^*$ denote the transition probability matrix. 
\begin{align*}
P_{ij}^*&=P(X_{n-1}=j|X_n=i)=\frac{P(X_{n-1}=j,X_n=i)}{P(X_n=i)}\\
&=\frac{P(X_{n-1}=j)P(X_n=i|X_{n-1}=j)}{P(X_{n}=i)}\\
&=\frac{P(X_{n-1}=j)}{P(X_{n}=i)}P_{ji}
\end{align*}
suppose we are considering a stationary Markov chain, $P(X_n=l)=P(X_{n-1}=l)=\pi(l),~\forall l$, $\pi(i){P^*}_{ij}=\pi(j)(P)_{ji}$. If $P^*=P^T$ then the Markov chain is called time reversible. Thus the condition for time reversibility is given by $\pi P_{ij}=\pi_jP_{ji}$. Any non-negative vector $X$ satisfying $X_iP_{ij}=X_jP_{ji},~\forall i,j$ and $\sum_{j \in \mathcal{N}_0}X_j=1$ is stationary distribution of time-reversible Markov chain. This is true because,
\begin{align*}
\sum_{i} X_i P_{ij}=\sum_i X_j P_{ji}=X_j \sum X_i = 1.
\end{align*}
Since stationary probabilities are the unique solution of the above, the claim follows.\\

\textbf{Example 4.7(A) An Ergodic Random Walk}: Any ergodic, positive recurrent random walk is time reversible. The transition probability matrix is $P_{i,i+1}+P_{i-1,i}=1$. For every two transitions from $i+1$ to $i$, there must be one transition from $i$ to $i+1$. The rate of transitions from $i+1$ to $i$ must hence be same as the number of transitions from $i$ to $i+1$. So the process is time reversible.

\textbf{Example 4.7(B) The Metropolis Algorithm: } Let $a_j,~ j=1, \hdots m$ be positive numbers and let $A=\sum_{i=1}^{m}a_i$. Suppose that $m$ is large and $A$ is difficult to compute. One way to compute it by simulating a sequence of random variables is by generating a Markov chain whose limiting probabilities are $p_j$s. The Metropolis algorithm provides a convenient approach. \\
Let $Q$ be an irreducible transition probability matrix on the integers $1, \hdots n$ such that $q_{ij}=q_{ji}$ and for all $i$ and $j$. Generate a Markov chain $\{X_n\}$ such that the transition probabilities are given by 
\begin{align*}
P_{ij} = \left\{
     \begin{array}{lr}
       q_{ij}\min(1,a_j/a_i) & : j \neq i\\
       q_{ii}+\sum_{j \neq i}q_{ij}\{1-\min(1,a_j/a_i)\} & : j = i.
     \end{array}
   \right.
\end{align*} 
It can be directly verified that the chain is irreducible and hence verify that $p_j$s are the limiting probabilities.

\textbf{Edge weighted graphs:} Consider a graph having a positive number $w_{ij}$ associated with each edge $(i,j)$, and suppose that a particle moves from vertex to vertex in the following manner: If the particle is presently at vertex  $i$ then it will next move to vertex $j$ with probability
\begin{equation*}
P_{ij}=\frac{w_{ij}}{\sum_{j}w_{ij}}
\end{equation*}
where $w_{ij}$ is 0 if $(i,j)$ is not an edge of the graph. The Markov chain describing the sequence of vertices visited by the particle is called a random walk on an edge weighted graph. 
\begin{prop}
Consider a random walk on an edge weighted graph witha finite number of vertices. If this Markov chain is irreducible then it is, in steady state, time reversible with stationary probabilities given by 
\begin{equation*}
\pi_i = \frac{\sum_{i}w_{ij}}{\sum_{j}\sum_{i}w_{ij}}.
\end{equation*}
\end{prop}
\begin{proof}
The time reversibility equation
\begin{equation*}
\pi_iP_{ij}=\pi_{j}P_{ji}
\end{equation*}
reduces to 
\begin{equation*}
 \frac{\pi_i w_{ij}}{\sum_{k}w_{ik}}=\frac{\pi_j w_{ji}}{\sum_{k}w_{jk}}
\end{equation*}
But noting that $w_{ij}=w_{ji}$ and $\sum_{i}\pi_i = 1$, we get the desired result.
 
\end{proof}
\subsubsection*{Necessary condition for time reversibility}
If we try to prove the equations necessary for time reversibility, $X_iP_{ij}=X_jP_{jk}$ for all $i,j$, for any arbitrary Markov chain, one may not end up getting any solution. This is so because, if $P_{ij}P_{jk}>0$, then $\frac{X_i}{X_k}=\frac{P_{ji}P_{kj}}{P_{jk}P_{ij}} \neq \frac{P_{kj}}{P_{jk}}$.\\
Thus we see that a necessary condition for time reversibility is $P_{ij}P_{jk}P_{ki}=P_{ik}P_{kj}P_{ji},~ \forall i,j,k$. In fact we can show the following.
\begin{thm}
A stationary Markov chain is time reversible if and only if starting in state $i$
, any path back to state $i$ has the same probability as the reversed path, for all $i$. That is, if
\begin{align*}
P_{i i_1}P_{i_1 i_2}\hdots P_{i_k i}=P_{i,i_k}P_{i_k i_{k-1}} \hdots P_{i_1,i}.
\end{align*} 
\end{thm}
\begin{proof}
The proof of necessity is as indicated above. To see the sufficiency part, fix states $i,j$
\begin{align*}
&\sum_{i_1,i_2,\hdots i_{k}}P_{ii_1}\hdots P_{i_k,j}P_{j,i}=\sum_{i_1,i_2,\hdots i_{k}}P_{i,j}P_{j,i_k}\hdots P_{i_1 i}\\
&(P^k)_{ij}P_{ji}=P_{ij}(P^k)_{ji}\\
&\frac{\sum_{k=1}^{n}(P^k)_{ij}P_{ji}}{n}= \frac{\sum_{k=1}^{n}P_{ij}(P^k)_{ji}}{n}
\end{align*}
As limit $n \in \N$, we get the desire result.
\end{proof}

\begin{thm}
Consider irreducible Markov chain with transition matrix $P$. If one can find non-negative vector $\pi$ and other transition matrix $P^*$ such that $\sum_j \pi_j =1$ and $\pi_iP_{ij}=\pi_jP^*_{ji}$ then $\pi$ is the stationary probability vector and $P^*$ is the transition matrix for the reversed chain.
\end{thm}
\begin{proof}
Summing $\pi_iP_{ij}=\pi_jP_{ji}^*$ over $i$ gives, $\sum_{i}\pi_iP_{ij}=\pi_j$. Hence $\pi_i$s are the stationary probabilities of the forward and reverse process. Since $P_{ji}^*=\frac{\pi_iP_{ij}}{\pi_j}$, $P_{ij}^*$ are the transition probabilities of the reverse chain.
\end{proof} 

\subsection{Example 4.7(E): Example 4.3(C) revisited}
Example 4.3(C) was with regard to the age of a renewal process. $X_n$ the forward process there was such that it increases in steps of 1 until it hits a value chosen by the inter arrival distribution. Hence the reverse process should be such that it decreases in steps of 1 until it hits 1 and then jumps to a state as chosen by the inter arrival distribution. Thus letting $P_i$ as the probability of inter arrival, it seems likely that  $P_{1i}*=P_i, ~ P_{i,i-1}=1,~ i > 1$. We have that $P_{i,1}=\frac{P_i}{\sum_{j \geq 1}P_j}=1-P_{i,i+1}, ~ i \geq 1$. For the reversed chain to be given as above, we would need 
\begin{align*}
&\pi_i P_{ij}=\pi_j P_{ji}^*\\
&\pi_i \frac{P_i}{\sum_j P_j}=\pi_1 P_i\\
&\pi_i=\pi_1 P(X \geq i)\\
&1=\sum_i \pi_i=\pi_1 E[X]; \pi_i=\frac{P(X \geq i)}{E[X]}, 
\end{align*}
where $X$ is the inter arrival time. We need to verify that $\pi_i P_{i,i+1}=\pi_{i+1}P^*_{i+1,i}$ or equivalently $P(X \geq i)(1-\frac{P_i}{P(X \geq i)})=P(X \geq i)$ to complete the proof that the reversed process is the excess process and the limiting distributions are as given above. But that is immediate.
\subsection{Semi Markov Processes}
Consider a stochastic process with states $0,1,2 \hdots$ such that whenever it enters state $i$,
\begin{enumerate}
\item {The next state it enters is state $j$ with probability $P_{ij}$.}\\
\item {Given the next state is going to be $j$, the time until the next transition from state $i$ to state $j$ has distribution $F_{ij}$. If we denote the state at time $t$ to be $Z(t)$, $\{Z(t), t \geq 0\}$ is called a Semi Markov process.}
\end{enumerate}

Markov chain is a semi Markov process  with 

\begin{align*} 
  F_{ij}(t) = \left\{
     \begin{array}{lr}
       0 & : t \leq 1 \\
       1 & : t > 1.
     \end{array}
   \right.
\end{align*}

Let $H_i$ the distribution of time the semi Markov process stays in state $i$ before transition. We have $H_j(t)= \sum_i P_{ij}F_{ij}(t)$, $\mu_i = \int_0 ^ \infty X dH_i(x)$. Let $X_n$ denote the $n^{\text{th}}$ state visited. Then $\{X_n\}$ is a Markov chain with transition probability $P$ called the embedded Markov chain of the semi Markov process. \\
\textbf{Definition:} If the embedded Markov chain is irreducible, then Semi Markov process is said to be irreducible. Let $T_{ii}$ denote the time between successive transitions to state $i$. Let $\mu_{ii}=E[T_{ii}]$.
\begin{thm}
If semi Markov process ius irreducible and if $T_{ii}$ has non-lattice distribution with $\mu_{ii}< \infty$ then, 
\begin{align*}
P_i=\lim_{t \in \N}P(Z(t)=i|Z(0)=j)
\end{align*}
exists and is independent of the initial state. Furthermore, $P_i=\frac{\mu_i}{\mu_{ii}}$.
\end{thm}
\textbf{Corollary 4.8.2} If the semi-Markov process is irreducible and $\mu_{ii}<\infty$, then with probability 1,
$\frac{\mu_i}{\mu_{ii}}=\frac{\lim_{t \in \N} \text{Amount of time in [0,t]}}{t}~\text{a.s}$.\\
\begin{thm}
Suppose conditions of the previous thmrem hold and the embedded Markov chain is positive recurrent. Then $P_i= \frac{\pi_i\mu_i}{\sum_{i}\pi_j \mu_j}$. 
\end{thm} 
\begin{proof}
Define the notation as follows:

$Y_i(j)=$ amount of time spent in state $i$ during $j^\text{th}$ visit to that state. $i,j \geq 0$. \\
$N_i(m)=$ number of visits to state $i$ in the first $m$ transitions of the semi-Markov process.\\

The proportion of time in $i$ during the first $m$ transitions:\\

\begin{align*}
P_{i=m}&= \frac{\sum_{j=1}^{N_i(m)}Y_i(j)}{\sum_l \sum_{j=1}^{N_i(m)}Y_i(j) }\\
&= \frac{\frac{N_i(m)}{m}\sum_{j=1}^{N_i(m)}Y_i(j)}{\sum_l \frac{N_i(m)}{m} \sum_{j=1}^{N_i(m)}Y_i(j) }\\
\end{align*}
Since $N_i(m)\in \N$ as $m \in \N$, it follows from the strong law of large numbers that $\frac{\sum_{i=2}^{N_i(m)}Y_i(j)}{N_i(m)}\rightarrow \mu_i$ and $\frac{N_i(m)}{m}\rightarrow (E[\text{number of transitions between visits to state }i])^{-1}=\pi_i$. Letting $m \in \N$, result follows.
\end{proof}
\begin{thm}
If Semi Markov process is irreducible  and non lattice, then $\lim_{t \in \N}P(Z(t)=i,Y(t)>x,S(t)=j|Z(0)=k)=\frac{P_{ij}\int_x^\infty F_{ij}^c(y)d(y)}{\mu_{ii}}$. Let $Y(t)$ denote the time from $t$ until the next transition. $S(t)$ state entered at the first transition after $t$. 
\end{thm}
\begin{proof}
The trick lies in defining the "ON" time. 
\begin{align*}
E[\text{ON time in a cycle}]=E[(X_{ij}-x)^+].
\end{align*}
\end{proof}
\textbf{Corollary:} 
\begin{align*}
\lim_{t \in \N} P(Z(t)=i, Y(t) >x|Z(0)=k)= \frac{\int_{x}^{\infty}H_i^c(y)d(y)}{\mu_{ii}}.
\end{align*}

\end{document}
