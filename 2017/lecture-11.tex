\documentclass[a4paper,10pt,english]{article}
\input{header}

\title{ Lecture 11: Discrete Time Markov Chains}
\author{}

\begin{document}
\maketitle
\section{Introduction}

We have seen that \textit{iid} sequences are easiest discrete time processes. 
However, they don't capture correlation well. 
Hence, we look at the discrete time stochastic processes of the form 
\begin{align*}
X_{n+1} &= f(X_n, Z_{n+1}),
\end{align*}
where $\{Z_n: n \in \N\}$ is an \text{iid} sequence, independent of initial state $X_0$. 
If $X_n \in E$ for all $n \in \N_0$, then $E$ is called the \textbf{state space} of process $X$. 
We consider a countable state space, and if $X_n = i \in \E$, then we say that the process $X$ is in state $i$ at time $n$.   
%\begin{defn}[DTMC]
For a countable set $E$, a stochastic process $\{X_n \in E, n \in \N_0\}$ is called a \textbf{discrete time Markov chain (DTMC)} 
if for all positive integers $n \in \N_0$ and all states $i_0, i_1, \dots, i_{n-1},i,j \in E$, 
the process $X$ satisfies the Markov property 
\begin{align*}
P\{X_{n+1} = j| X_n = i, X_{n-1} =i_{n-1}, \cdots, X_0 = i_0\} = P\{X_{n+1} = j|X_n = i\}. 
\end{align*}
%\end{defn}
\subsection{Homogeneous Markov chain}
If for each $n \in \N_0$, we have $p_{ij}(n) \triangleq  P\{X_{n+1} = j|X_n = i\} = p_{ij}$. 
That is, when the transition probability does not depend on $n$, the DTMC is \textbf{homogeneous} and the matrix $P = \{p_{ij}: i,j \in \E\}$ is called the \textbf{transition matrix}. 
%Henceforth we shall only work with homogeneous DTMCs. 

%\begin{defn}[Transition Probability Matrix]
%  $\{p_{ij}\}$ are called transition probabilities as they define the
%  probability of transiting from state $i$ to state $j$ in one step.
%  We can create a matrix $P$ out of the transition probabilities where
%  the $i^{th}$ row and $j^{th}$ column equal $p_{ij}$. Such a matrix
%  is called a \textbf{Transition Probability Matrix} or \textbf{TPM}.
%\end{defn}
For all states $i,j \in E$, if a non-negative matrix $A \in \R_+^{E \times E}$ has the following property 
\begin{xalignat*}{3}
&a_{ij} \geq 0, &&\sum_{j \in E} a_{ij} \leq 1,
\end{xalignat*}
then it is called a \textbf{sub-stochastic} matrix. 
If the second property holds with equality, then it is called a \textbf{stochastic} matrix. 
If in addition, $A^T$ is stochastic, then $A$ is called \textbf{doubly stochastic}. 
Clearly, the transition matrix $P$ is stochastic matrix. 

\subsection{Transition graph}
A transition matrix $P$ is sometimes represented by a directed graph $G = (E, \{[i,j) \in E \times E: p_{ij} > 0\})$. 
In addition, this graph has a weight $p_{ij}$ on each edge $e = [i,j)$. 

% \subsection{Example: M/G/1 Queue}
% In this queue, arrivals occurs as per a Poisson Process but service times are distributed \emph{iid} $G$. And there is only one server. Let $X(t)$ count the number of customers in the system at time t. Consider $\{X(t), t\geq 0\}$. Let $X_n = X(D_n^+)$ be the number of customers left behind by nth departure.

% \begin{prop}
% $X_n$ is a DTMC.
% \end{prop}
% To see this, let $Y_n$ denote the number of customers arriving during service time of  $(n+1)$th customer. Assume service time $\sim G$. Then
% \begin{align*}X_{n+1} = X(D_{n+1}^+) = (X_n - 1)^+ + Y_n\end{align*}

% Hence writing the TPM (here take $i\geq 1$)
% \begin{align*}
% &P[X_{n+1} = j| X_n = i, X_{n-1} = i_{n-1}, \cdots, X_0 =i_0] \\
% & =  P[i +Y_n -1 = j| X_n = i, X_{n-1} = i_{n-1}, \cdots, X_0 =i_0] \\
% & = P[Y_n = j+1-i] := p_{ij}
% \end{align*}
%  For $i=0$, $p_{0j} = P[Y_n =j]$. As the arrival process is Poisson, we can compute the TPM hence.

% \subsection{Example: G/M/1 Queue}
% In this queue, arrivals occur as per a renewal process with interarrival times $\sim G$. The service times are exponentially distributed. Our strategy, therefore is the analogous one, where we sample the queue after the nth arrival. With this in mind, let $X_n = X(A_n^-) $ which is the number of customers seen by the nth arrival. We get
% \begin{align*}X_{n+1} = (X_n-Y_n+1)^+\end{align*}
% From this, the TPM may be calculated.

\section{Chapman Kolmogorov equations}
We can define $n$-step transition probabilities for $i,j \in E$ and $m,n \in \N$
\begin{align*}
p_{ij}^{(n)} &\triangleq P\{X_{n+m} = j | X_m = i\}. 
\end{align*}
It follows from the Markov property and law of total probability that 
\begin{align*} 
p_{ij}^{(m+n)} &= \sum_{k \in E} p_{ik}^{(m)}p_{kj}^{(n)}.
\end{align*}
We can write this result compactly in terms of transition probability matrix $P$ as $P^{(n)} = P^n$. 
Let $\nu \in \R_+^E$ is a probability vector such that 
\begin{align*}
\nu_n(i) &= P\{X_n = i\}.
\end{align*}
Then, we can write this vector $\nu_n$ in terms of initial probability vector $\nu_0$ and the transition matrix $P$ as 
\begin{align*}
\nu_n &= \nu_0P^n.
\end{align*}

%$\sum_{i \in E}\nu(i) = 1$
 
%\begin{proof}
%\begin{align*}
%p_{ij}^{(n+m)} =  & P[X_{n+m} = j| X_0 = i]\\
%=& \sum_{k \in E} P[X_{n+m} = j,X_m=k| X_0 = i]\\
%=& \sum_{k \in E}  P[X_{n+m} = j|X_m=k, X_0 = i].P[X_m=k|X_0=i]\\
%=& \sum_{k \in E}  P[X_{n+m} = j|X_m=k].P[X_m=k|X_0=i]\\
%=& \sum_{k \in E}  P[X_{n} = j|X_0=k].P[X_m=k|X_0=i]\\
%=& \sum_{k \in E}  p_{ik}^{(m)}p_{kj}^{(n)}.\\
%\end{align*}
%The third equality is just using the property of conditional expectation. The fourth equality is by using the Markov property and the fifth equality is by the time homogeneity of the DTMC.
%\end{proof}
%If $P$ is the TPM, this result may be compactly written as $P^{(n)} = P^n$. As a result, $p_{ij}^{(n)} = P^n(i,j)$.

\subsection{Strong Markov property (SMP)}
Let $T$ be an integer valued stopping time with respect to the stochastic process $X$ such that $P\{T< \infty\}=1$. 
Then for all $i_0, \dots, i_{n-1},\dots, i,j \in E$, the process $X$ satisfies the \textbf{strong Markov property} if 
\begin{align*}
P\{X_{T+1} = j|X_{T} = i, \dots,X_0 =i_0\} &= P\{X_{T+1} = j|X_{T} = i\}.
\end{align*}
\begin{lem}
Markov chains satisfy the strong Markov property. 
\end{lem}
\begin{proof}
Let $X$ be a Markov chain and $A = \{X_{T} = i, \dots, X_0 =i_0\}$. 
Then, we have 
\begin{align*}
P\{X_{T+1} = j|A\} &=\frac{\sum_{n \in \N_0}P\{X_{T+1}=j, A, T = n\}}{P\{A\}} %= \sum_{n \in \N_0}  P\{X_{n+1} = j |X_{n} = i, \cdots, X_0 =i_0, T = n\}\frac{P(A, T =n)}{P(A)} \\
%&= \sum_{n \in \N_0} P[X_{n+1} = j|X_{n} = i, \cdots, X_0 =i_0, T=n\} P\{T=n|X_{T} = i, \cdots, X_0 =i_0\}  \\
%&= \sum_{n \in \N_0} p_{ij} P[T=n|X_{T} = i, \cdots, X_0 =i_0] = p_{ij}.
= \sum_{n \in \N_0} p_{ij}\frac{P(A, T =n)}{P(A)} = p_{ij}.
\end{align*}
This equality follows from the fact that $\{T = n\}$ is completely determined by $\{X_0, \dots, X_n\}$
\end{proof}
As an exercise, if we try to use the Markov property on arbitrary random variable $T$, the SMP may not hold. 
For example, define a non-stopping time $T$ for $j \in E$
\begin{align*}
T=\inf \{n \in \N_0: X_{n+1}=j\}.
\end{align*}
In this case, we have 
\begin{align*}
P\{X_{T+1}=j|X_T=i,\dots, X_0 = i_0\}&= 1\{p_{ij} > 0\} \neq  P \{X_1=j |X_0=i\} = p_{ij}.
\end{align*}
A useful application of the strong Markov property is as follows. 
Let $i_0 \in E$ be a fixed state and $\tau_0 = 0$
Let $\tau_n$ denote the stopping times at which the Markov chain visits $i_0$ for the $n$th time. 
That is,
\begin{align*} 
\tau_n &= \inf\{n > \tau_{n-1}: X_n = i_0\}. 
\end{align*}
Then $\{X_{\tau_n + m}: m \in \N_0\}$ is a stochastic replica of $\{X_m: m \in \N_0\}$ with $X_0 = i_0$ and can be studied as a regenerative process. 

\section{Communicating classes}
%\begin{defn}
State $j \in E$ is said to be \textbf{accessible} from state $i \in E$ if $p_{ij}^{(n)} >0$ for some $n \in \N$, and denoted by $i \to j$. 
If two states $i,j \in E$ are accessible to each other, they are said to \textbf{communicate} with each other, denoted by $i \leftrightarrow j$. 
A set of states that communicate are called a \textbf{communicating class}. 
%\end{defn}

\begin{prop}
Communication is an equivalence relation. 
\end{prop}
\begin{proof}
Reflexivity and Symmetry are obvious. For transitivity, suppose $i \leftrightarrow j$ and $j \leftrightarrow k$. Suppose $p_{ij}^{(m)} >0$ and $p_{jk}^{(n)} >0$. Then by Chapman Kolmogorov, we have
\begin{align*}p_{ik}^{(m+n)} = \sum_{l \in \N_0} p_{il}^{(m)}p_{lk}^{(n)} \geq p_{ij}^{(m)}p_{jk}^{(n)} >0  \end{align*}
Hence transitivity is assured.
\end{proof}

\subsection{Irreducibility and periodicity}
A consequence of the previous result is that communicating classes are disjoint or identical. 
A Markov chain with a single class is called an \textbf{irreducible} Markov chain.
%\begin{defn}
The \textbf{period} of state $i$ is defined as
\begin{align*}d(i) = \gcd\{n \in \N_0 : p_{ii}^{(n)} > 0\}\end{align*}
If the period is $1$, we say the state is \textbf{aperiodic}.
%\end{defn}

\begin{prop}
If $i \leftrightarrow j$, then $d(i) = d(j)$. Basically, periodicity is a class property.
\end{prop}
\begin{proof}
Let $m$ and $n$ be such that $p_{ij}^{(m)}p_{ji}^{(n)} > 0$. Suppose $p_{ii}^{(s)} > 0$. Then
\begin{align*} p_{jj}^{(n+m)} \geq p_{ji}^{(n)}p_{ij}^{(m)} > 0 \end{align*}
\begin{align*} p_{jj}^{(n+s+m)} \geq p_{ji}^{(n)}p_{ii}^{(s)}p_{ij}^{(m)} > 0 \end{align*}
Hence $d(j) | n+m$ and $d(j) | n+s+m$ which implies $d(j) | s$. Hence $d(j) | d(i)$. By symmetrical arguments, we get $d(i) | d(j)$. Hence $d(i) = d(j)$.
%Source of proof is from \emph{Sheldon Ross: Stochastic Processes}.
\end{proof}

\subsection{Transient and recurrent states}
%\begin{defn}
Let $f_{ij}^{(n)}$ denote the probability that starting from state $i$, the first transition into state $j$ happens at time $n$. Then let
\begin{align*}f_{ij} = \sum_{n=1}^\infty f_{ij}^{(n)}\end{align*}
Here $f_{ij}$ would therefore denote the probability of ever entering state $j$ given that we start at state $i$. 
State $j$ is said to be \textbf{transient} if $f_{jj} < 1$ and \textbf{recurrent} if $f_{jj}=1$. 
%\end{defn}
\begin{prop} 
The total number of visits to a state $j \in E$ is denoted by 
%\eq{
$N_j = \sum_{n \in \N_0}1\{X_n = j\}$.
%}
Then, for each $m \in \N$ and $i \neq j$, we have 
\eq{
P_j\{N_j = m\} &= f_{jj}^{m-1}(1-f_{jj}), ~m \in \N\\
P_i\{N_j = m\} &=\begin{cases}
1 - f_{ij} & m = 0,\\
f_{ij}f_{jj}^{m-1}(1-f_{jj}) & m \in \N.
\end{cases}
}
\end{prop}
\begin{proof}
For each $k \in \N$, the time of the $k$th visit to the state $j$ is a stopping time. 
From strong Markov property, the next return to state $j$ is independent of the past. 
Hence, each return to state $j$ is an \textit{iid} Bernoulli random variable with probability $f_{jj}$. 
It follows that the number of visits $j$ is the time for first failure to return. 
Conditioned on $X_0 = j$, the distribution of $N_j$ is geometric random variable with success probability $1-f_{jj}$. 
%Hence the probabilities will multiply. 
%Hence starting from state $j$ we visit the state $j$ with probability $f_{jj}$. 
%Moreover, we visit the state $j$ exactly $m$ number of times (and no more) with probability $f_{jj}^m(1-f_{jj}).$\\

Conditioned on $X_0 = i$, the stopping time of first visit to $j$ is a Bernoulli random variable with probability $f_{ij}$. 
Hence, the second result follows. 
\end{proof}

\begin{cor} 
For a Markov chain $X$, 
%\eq{
$P_j\{N_j < \infty\} = 1\{f_{jj} < 1\}$. 
%}
\end{cor}
\begin{proof}
We can write the event $\{N_j < \infty\}$ as disjoint union of events $\{N_j = n\}$, to get 
\eq{
P_j\{N_j \in \N\} &= \sum_{n \in\N}P_j\{N_j = n\} = 1\{f_{jj} < 1\}.
}
\end{proof}
In particular, this corollary implies the following
\begin{enumerate}
	\item A transient state is visited a finite amount of times almost surely.
	\item A recurrent state is visited infinitely often almost surely.
	\item In a finite state Markov chain, not all states may be transient. 
\end{enumerate} 
\begin{prop}
A state $j$ is recurrent iff
%\begin{align*}
$\sum_{k\in\N} p_{jj}^{(k)} = \infty$.
%\end{align*}
\end{prop}
\begin{proof}
For any state $j \in E$, we can write
\begin{align*}
p_{ii}^{(k)} &= \mathbb{P}_i\{X_k=i\} = \E_i 1\{X_k=i\}.
\end{align*}
Using monotone convergence theorem to exchange expectation and summation, we obtain  
%Then, assuming that the infinite summation can be pushed inside the expectation 
\begin{align*}
\sum_{k\in \N}p_{ii}^{(k)}=\E_i \sum_{k\in\N}1\{X_k=i\} = \E_i N_i.
\end{align*}
Thus, $\sum_{k\in \N} p_{ii}^{(k)}$ represents the expected number of returns $\E_i N_i$ to a state $i$ starting from state $i$, 
which we know to be finite if the state is transient and infinite if the state is recurrent. 
%$[\Leftarrow]$ For the reverse argument we will show that if $\sum_{k=1}^{\infty}p_{ii}^{(k)}<\infty$ then the state $i$ will be transient, which should suffice to prove our claim. First we observe that for a transient state $j$, if $M_j$ is the number of visits to the state $j$, then
%\begin{align*}P[M_j=m|X_0=j]=f_{jj}^m(1-f_{jj}).\end{align*} 
%
%Hence now again if we calculate the expected number of times the state $j$ is visited starting from state $j$ can be calculated by the following elementary exercise.
%\begin{align*}
%\E_j[M_j=m|X_0=j]= & \sum_{m=1}^{\infty}mf_{jj}^m(1-f_{jj})=\frac{f_{jj}}{(1-f_{jj})}=
%\begin{cases}
%\infty \quad if \quad f_{jj}=1 \Leftrightarrow j \quad  recurrent,\\
%C<\infty \quad if \quad f_{jj} \neq 1 \Leftrightarrow j \quad transient.
%\end{cases}
%\end{align*}
\end{proof}

\begin{prop}
Transience and recurrence are class properties.
\end{prop}
\begin{proof}
Let us start with proving recurrence is a class property. Let $i$ be a recurrent state and let $i \leftrightarrow j$. Hence there exist some $m,n >0$, such that $p_{ij}^{(m)} > 0$ and $p_{ji}^{(n)}>0$. As a consequence of the recurrence, $\sum_{s \in \N_0} p_{ii}^{(s)} = \infty$. 
It follows that $j$ is recurrent by observing 
\begin{align*}
\sum_{s \in \N_0} p_{jj}^{(m+n+s)} &\geq \sum_{s \in \N_0} p_{ji}^{(n)} p_{ii}^{(s)} p_{ij}^{(m)} = \infty.
\end{align*}
%Hence $j$ is recurrent. 
Now, if $i$ were transient instead, we conclude that $j$ is also transient by the following observation
\begin{align*}
\sum_{s \in \N_0}  p_{jj}^{(s)} &\leq \frac{\sum_{s \in \N_0} p_{ii}^{(m+n+s)} }{ p_{ji}^{(n)} p_{ij}^{(m)}}< \infty.
\end{align*}
%Hence $j$ is transient.
\end{proof}



\begin{cor}
If $j$ is recurrent, then for any state $i$ such that $i\leftrightarrow j$, $f_{ij} = 1$.
\end{cor}
\end{document}
