% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 02: Bernoulli Processes}
\author{}

\begin{document}
\maketitle

\section{Construction of Probability Space}
Consider an experiment, where an infinite sequence of trials is conducted. 
Each trial has two possible outcomes, success or failure, denoted by $S$ and $F$ respectively.
Any outcome of the experiment is an infinite sequence of successes and failures, e.g.
\begin{align*}
\omega = (S, F, F, S, F, S, \dots).
\end{align*}
The collection of all possible outcomes of this experiment will be our sample space $\Omega = \{S,F\}^{\N}$. 
The $i$th projection of an outcome sequence $\omega \in \Omega$ is denoted by $\omega_i \in \{S,F\}$. 
We consider a $\sigma$-algebra $\F$ on this space generated by all finite subsets of the sample space $\Omega$. 
That is, 
\begin{align*}
\F &= \sigma(\{\omega \in \Omega:  \omega_i \in \{S,F\}, \forall i \in I \subset \N \text{ for finite } I \}).
\end{align*}
We further assume that each trial is independent and identically distributed, with common distribution of a single trial 
\begin{xalignat*}{3}
&P\{\omega_i = S\} = p, &&P\{\omega_i = F\} = q \triangleq 1-p.
\end{xalignat*}
This assumption completely characterizes the probability measure over all elements of the $\sigma$-algebra $\F$. 
For $a \in \F$ and the number of successes $n = |\{i \in I: a_i = S\}|$ in $I$, 
\begin{align*}
P(a) &= \prod_{i \in I}\E 1\{\omega_i = a_i\} = \prod_{i \in I: \omega_i = S}\E 1\{\omega_i = S\} \prod_{i \in I: \omega_i = F}\E 1\{\omega_i = F\} = p^nq^{|I|-n}.
\end{align*}
Hence, we have completely characterized the probability space $(\Omega, \F, P)$. 
Further, we define a discrete random process $X : \Omega \to \{0,1\}^\N$ such that 
\begin{align*}
X_n(\omega) = 1\{\omega_n = S\}.
\end{align*}
Since, each trial of the experiment is \emph{iid}, so is each $X_n$. 
\section{Bernoulli Processes}
For a probability space $(\Omega, \F, P)$, a discrete process $X = \{X_n(\omega): n \in \N\}$ taking value in $\{0,1\}^{\N}$ is a \textbf{Bernoulli Process} 
with success probability $p = \E X_n$ if $\{X_n: n \in \N\}$ are \emph{iid} with common distribution $P\{X_n = 1\} = p$ and $P\{X_n = 0\} = q$. 
\begin{shaded*}
\begin{enumerate}[i\_]
\item For products manufactured in an assembly line, $X_n$ indicates the event of $n$th product being defective. 
\item At a fork on the road, $X_n$ indicates the event of $n$th vehicle electing to go left on the fork. 
\end{enumerate}
\end{shaded*} 
For $n = |\{i \in S: 0 \leq x_i < 1\}|$, the finite dimensional distribution of $X(\omega)$ is given by
\begin{align*}
F_S(x) &= \prod_{i \in S}P\{X_i \leq x_i\} = q^n.
\end{align*}
The mean, correlation, and covariance functions are given by
\begin{xalignat*}{5}
&m_X = \E X_n = p,&&R_X = \E X_n X_m = p^2, &&C_X = \E(X_n - p)(X_m-p) = 0.
\end{xalignat*}

\section{Number of Successes}
For the above experiment, let $N_n$ denote the number of successes in first $n$ trials. 
Then, we have 
\begin{align*}
N_n(\omega) &= \sum_{i=1}^{n}1\{\omega_i = S\} = \sum_{i=1}^{n}X_i(\omega). 
\end{align*}
The discrete process $\{N_n (\omega) : n\in \N\}$ is a stochastic process that takes discrete values in $\N_0$. 
In particular, $N_n \in \{0, \dots, n\}$, i.e. the set of all outcomes is index dependent. 
\begin{shaded*}
\begin{enumerate}[i\_]
\item For products manufactured in an assembly line, $N_n$ indicates the number of defective products in the first $n$ manufactured. 
\item At a fork on the road, $N_n$ indicates the number of vehicles that turned left for first $n$ vehicles that arrived at the fork.  
\end{enumerate}
\end{shaded*} 
We can characterize the moments of this stochastic process
\begin{xalignat*}{3}
&m_N(n) = \E X_n = np,&&\Var{N_n} = \sum_{i=1}^{n}\Var{X_i} = npq. %\E (N_n- np)^2 = \E \sum_{i \neq j}X_i(\omega)X_j(\omega) + \E\sum_{i=1}^{n}X_i^2(\omega) - n^2p^2 = n(n-1)p^2 + np = npq. %, &&C_X = \E(X_n - p)(X_m-p) = 0.
\end{xalignat*}
Clearly, this process is not stationary since the first moment is index dependent. 
In the next lemma, we try to characterize the distribution of random variable $N_n$. 
\begin{lem} 
We can write the following recursion for $P_n(k) \triangleq P\{N_n(\omega) = k\}$, for all $n,k \in \N$
\begin{align*}
P_{n+1}(k) &= pP_{n}(k-1) + qP_{n}(k). 
\end{align*}
\end{lem}
\begin{proof}
We can write using the disjoint union of probability events,  
\begin{align*}
P\{N_{n+1} = k\} &= P\{N_{n+1} = k, N_{n} = k\} +  P\{N_{n+1} = k, N_{n} = k-1\}.
\end{align*}
Since $\{N_{n+1} = k, N_{n} = j\} = \{X_{n+1} = k-j, N_{n} = j\}$, from independence of $X_n$s we have
\begin{align*}
P\{N_{n+1} = k\} &= P\{X_{n+1} = 0\}P\{N_n = k\} + P\{X_{n+1} = 1\}P\{N_n = k-1\}.
\end{align*}
Result follows from definition of $P_n(k)$. 
\end{proof}
\begin{thm} The distribution of number of successes $N_n$ in first $n$ trials of a Bernoulli process is given by a Binomial $(n,p)$ distribution 
\begin{align*}
P_n(k) &= \binom{n}{k}p^kq^{(n-k)}.
\end{align*}
\end{thm}
\begin{proof} 
It follows from induction. 
\end{proof}
\begin{cor} The stochastic process $\{N_n: n \in \N\}$ has stationary and independent increments.
\end{cor}
\begin{proof}
We can look at one increment 
\begin{align*}
N_{m+n}-N_m &= \sum_{i = 1}^{n}X_{i+m}.
\end{align*}
This increment is a function of $(X_{m+1}, \dots, X_{m+n})$ and hence independent of $(X_1, \dots, X_m)$. 
The random variable $N_m$ depends solely on $(X_1, \dots, X_m)$ and hence the independence follows. 
Stationarity follows from the fact that $X_i$s are iid and $N_{m+n}-N_m$ is sum of $n$ iid Bernoulli random variables, 
and hence has a Binomial $(n,p)$ distribution identical to that of $N_n$. 
\end{proof}

\section{Times of Successes}
For the above experiment, let $T_k$ denote the trial number corresponding to $k$th success. 
Clearly, $T_k \geq k$. We can define it inductively as 
\begin{xalignat*}{3}
&T_1 = \inf\{ n \in \N: X_n(\omega) = 1\}, &&T_{k+1}(\omega) = \inf\{ n > T_{k}: X_n(\omega)  = 1\}. 
\end{xalignat*}
For example, if $X = (0, 1, 0, 1, 1, \dots)$, then $T_1 = 2, T_2 = 4, T_3 =5$ and so on.
The discrete process $\{T_k (\omega) : k \in \N\}$ is a stochastic process that takes discrete values in $\{k, k+1, \dots\}$. 
\begin{shaded*}
\begin{enumerate}[i\_]
\item For products manufactured in an assembly line, $T_k$ indicates the number of products inspected for $k$th defective product to be detected.   
\item At a fork on the road, $T_k$ indicates the number of vehicles that have arrived at fork for $k$th left turning vehicle.  
\end{enumerate}
\end{shaded*} 
We first observe the following inverse relationship between number of $n$th successful trial $T_n$, and number of successes in $n$ trials. 
\begin{lem} The following relationships hold between time of success and number of success 
\begin{xalignat*}{3}
&\{T_k \leq n\} = \{N_n \geq k\},&&\{T_k = n\} = \{N_{n-1} = k-1, X_n = 1\}.
\end{xalignat*}
\end{lem}
\begin{proof}
To see the first equality, we observe that $\{T_k \leq n\}$ is the set of outcomes, 
where $X_{T_1} = X_{T_2} = \dots = X_{T_k} = 1$, and $\sum_{i=1}^{T_k}X_i = k$. 
%\begin{xalignat*}{3}
%&X_{T_1} = X_{T_2} = \dots = X_{T_k} = 1, && \sum_{i=1}^{T_k}X_i = k.
%\end{xalignat*}
Hence, we can write the number of successes in first $n$ trials as 
\begin{align*}
N_n = \sum_{i=1}^nX_i = \sum_{i > T_k}X_i + \sum_{i=1}^{T_k}X_i \geq k.
\end{align*}
%Let $T_k \leq n$, that is $k$th success was seen in first $n$ trials. 
%This is, number of successes in $n$ trials is $N_n \geq k$. That is , 
%\begin{align*}
%\{T_k \leq n\} \subseteq \{N_n \geq k\}.
%\end{align*}
Conversely, we notice that we can re-write the number of trials for $i$th success as  
\begin{align*}
T_i &= \inf\{m \in \N: N_m = i\}.
\end{align*}
Since $N_n$ is non-decreasing in $n$, it follows that for the set of outcome such that $\{N_n \geq k\}$, there exists $m \leq n$ such that $T_k = m \leq n$. 
%the set of outcomes $\{N_n \geq k\}$ is such that there exists an ordered set $S \subseteq [n]$ of size $|S| \geq k$, 
%such that $X_i = 1$ for all $i \in S$ and zero elsewhere. 
%We can denote first $k$ elements of $S$ by  $\{T_1, T_2, \dots, T_k\}$, and hence $T_k \leq n$. 
%Using similar arguments, we observe that if $N_n \geq k$, then it follows that $T_k \leq n$. Hence, 
%\begin{align*}
%\{T_k \leq n\} \supseteq \{N_n \geq k\}.
%\end{align*}
For the second inequality, we observe that
\begin{align*}
\{T_k = n\} %&= \{T_k \leq n, T_k > n-1\} 
&= \{T_k \leq n\}\cap\{T_k \geq n-1\}^c = \{N_n \geq k\}\cap\{N_{n-1} \geq k\}^c = \{N_{n-1} = k-1, N_n = k\}. %= \{N_n \geq k, N_{n-1} < k\} = \{N_{n-1} = k-1, N_n = k\}.
\end{align*}
\end{proof}

We can write the marginal distribution of process $\{T_k: k \in \N\}$ in terms of the marginal of the process $\{N_n: n \in \N\}$ as
\begin{xalignat*}{3}
&P\{T_k \leq n\} = \sum_{j \geq k}P_n(j) = \sum_{j=k}^{\infty}\binom{n}{j}p^jq^{(n-j)},&&P\{T_k = n\} = p\dot P_{n-1}(k-1) = \binom{n-1}{k-1}p^kq^{n-k}. 
\end{xalignat*}
Clearly, this process is not stationary since the first moment is index dependent. 
It is not straightforward to characterize moments of $T_k$ from its marginal distribution. 
\begin{lem} The time of success is a Markov chain.
\end{lem}
\begin{proof} By definition, $T_k$ depends only on $T_{k-1}$ and $X_i$s for $i > T_k$. 
From independence of $X_i$s, it follows that 
\begin{align*}
P\{T_k = T_{k-1} + m | T_{k-1}, T_{k-2},\dots, T_1\} &= P\{T_k - T_{k-1} = m |T_{k-1}\} = q^{m-1}p1\{m > 0\}.
\end{align*}
\end{proof}
\begin{cor} The time of success process $\{T_k: k \in \N\}$ has stationary and independent increments. 
\end{cor}
\begin{proof} 
From the previous lemma and the law of total probability, we see that 
\begin{align*}
\sum_{n \in \N_0}P\{T_k - T_{k-1} = m, T_k = n\} &= pq^{m-1}1\{m > 0\}.
\end{align*}
\end{proof}
Since, the increment in time of success follows a geometric distribution with success probability $p$, 
we have mean of the increment $\E(T_{k+1} - T_k) = 1/p$, and the variance $\Var{T_{k+1}-T_k} = {q}/{p^2}$. 
%\begin{xalignat*}{3}
%&\E(T_{k+1} - T_k) = \frac{1}{p}, && \E(T_{k+1}-T_k)^2 = \frac{q}{p^2}.
%\end{xalignat*}
This implies that we can write the moments of $T_k$ as
\begin{xalignat*}{3}
&\E T_k = \E \sum_{i=1}^{k}(T_i - T_{i-1}) = \frac{k}{p}, && \Var{T_k} =  \Var{\sum_{i=1}^{k}(T_i - T_{i-1})} = \frac{kq}{p^2}.
\end{xalignat*}
This shows that stationary and independent increments is a powerful property of a process and makes the process characterization much simpler. 
Next, we show one additional property of this process. 
\begin{lem}
The increments of time of success process $\{T_k: k \in \N\}$ are memoryless. 
\end{lem}
\begin{proof} 
It follows from the property of a geometric distribution, that for positive integers $m,n$
\begin{align*}
P\{T_{k+1} - T_k > m + n| T_{k+1} - T_k > m \} &= \frac{P\{T_{k+1}-T_k > m+n\}}{P\{T_{k+1}-T_k > m\}} = q^{n} = P\{T_{k+1}-T_k > n\}.
\end{align*}
\end{proof}

\end{document}