% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 03: Poisson Process}
\author{}

\begin{document}
\maketitle

\section{Simple point processes}
%\begin{defn} 
A \textbf{simple point process} is a collection of distinct points 
\begin{align*}
\Phi = \{S_n \in \R^d: n \in \N\},
\end{align*}
such that $|S_n| \to \infty$ as $n \to \infty$.  
In $\R_+$, one can order these points $\{S_n: n \in \N\}$. 
Let $N(\emptyset) =0$ and denote the number of points in a set $A \subseteq \R^d$ by
\begin{align*}
N(A)= \sum_{n \in \N}1\{S_n \in A\}. 
\end{align*} 
Then $\{N(A): A \in \F\}$ is called a \textbf{counting process} for the simple point process $\Phi$. 
%\end{defn} 
%\begin{defn} 
A counting process is \textbf{simple} if the jump size is unity.
%\end{defn}
\begin{shaded*}
\begin{enumerate}
\item Point processes can model arrivals at classrooms, banks, hospital, supermarket, traffic intersections, airports etc.
\item Point processes can model location of nodes in a network, such as cellular networks, sensor networks, etc.
\end{enumerate}
\end{shaded*}
We can simplify this definition for $d=1$. 
A stochastic process $\{N(t), t\ge 0\}$ is a \textbf{counting process} if
\begin{enumerate}
  \item $N(0) = 0$, and 
  \item for each $\omega \in \Omega$, the map $t\mapsto N(t)$ is non-decreasing, integer valued, and right continuous.% and at points of discontinuity $(N(t)- N(t^{-}))\le 1,~~\forall \omega \in \Omega$. 
\end{enumerate}

\begin{lem} 
A counting process has finitely many jumps in a finite interval $[0,t)$.
\end{lem}

%\begin{defn} 
The points of discontinuity correspond to the arrival instants of the point process $N(t)$. 
The \textbf{$n$th arrival instant} is a random variable denoted $S_n$, such that  
\begin{xalignat*}{3}
&S_0 = 0, &&S_n = \inf\{t \geq 0: N(t) \ge n\},~n \in \N.
\end{xalignat*}
%\end{defn}
%\begin{defn} 
The \textbf{inter arrival time} between $(n-1)${th} and $n${th} arrival is denoted by $X_n$ and written as
\begin{align*}
X_n = S_n - S_{n-1}.
\end{align*}
%\end{defn}
%\begin{rem} 
For a simple point process, we have
\begin{align*}
P\{X_{n} = 0\} &= P\{X_n\le 0\} = 0.
\end{align*}
%This is equivalent to a point process not having an atom at any point on the line. 
General point processes in higher dimension don't have any inter-arrival time interpretation. 
%\end{rem}
\begin{figure}[hhhh]
\center
\input{Figures/Poisson}
\caption{Sample path of a simple counting process.}
\label{Fig:SimpleCounting}
\end{figure}

\begin{lem} Simple counting process $\{N(t), t \ge 0\}$ and arrival process $\{S_n: n \in \N\}$ are inverse processes. That is,
\begin{align*}
\{S_n \le t\} = \{N(t) \ge n\}.
\end{align*}
\end{lem}
\begin{proof} Let $\omega \in \{S_n \le t\}$, then $N(S_n) = n$ by definition. 
%from right continuity of the counting process. 
Since $N$ is a non-decreasing process, we have $N(t) \geq N(S_n) = n$. 
%It follows that $\{S_n \le t\} \subseteq \{N(t) \ge n\}$.
Conversely, let $\omega \in \{N(t) \ge n\}$, then it follows from definition that $S_n \leq t$.
\end{proof}

\begin{cor} The following identity is true.
\begin{align*}
\{S_n \le t, S_{n+1} > t\} = \{N(t) = n\}.
\end{align*}
\end{cor}
\begin{proof}
It is easy to see that 
\begin{align*}
\{S_{n+1} > t \} &= \{S_{n+1} \le t\}^c = \{N(t) \ge n+1\}^c = \{N(t) < n+1\}.
\end{align*}
Hence, the result follows by writing 
\begin{align*}
\{N(t) = n\} &= \{N(t) \ge n, N(t) < n+1\} = \{S_n \le t, S_{n+1} > t\}.
\end{align*}
\end{proof}

\begin{lem}
Let $F_n(x)$ be the distribution function for $S_n$, then 
\begin{align*}
P_n(t) \triangleq P\{N(t) = n\} = F_{n}(t)-F_{n+1}(t).
\end{align*}
\end{lem}
\begin{proof} It suffices to observe that following is a union of disjoint events,
\begin{align*}
\{S_n \le t \} &= \{S_n \le t, S_{n+1} > t\} \cup \{S_{n} \le t, S_{n+1} \le t\}.
\end{align*}
\end{proof}

\subsection{Stationary and independent increments}
%\begin{defn}
For an interval $I = (s,t]$, the number of arrivals in the interval $I$ is defined as $N(I) = N(t) - N(s)$. 
%\end{defn}
%\begin{defn} 
Consider an arbitrary collection of mutually exclusive intervals $\{I_j: j \in [n]\}$, time index $t \ge 0$, and set of positive integers $\{k_j \in \N_0 : j \in [n]\}$. 
A counting process $\{N(t), t\ge 0\}$ has \textbf{stationary increments} if
\begin{align*}
P\{N(I_j) = k_j, j \in [n]\} &= P\{N(t + I_j) = k_j, j \in [n]\}.
\end{align*}
%, the joint distribution of $(N(t_{n})-N(t_{n-1}),N(t_{n-1})-N(t_{n-2}),...,N(s))$ is identical to the joint distribution of $(N(t_{n}+t)-N(t_{n-1}+t),...,N(s+t)), ~ \forall t \ge 0$.
%\end{defn}
%\begin{defn} 
A counting process $\{N(t), t\ge 0\}$ has \textbf{independent increments} if %for any $k_j\in \N_0, j \in [n]$
%if it has stationary increments and the increments are independent random variables.
\begin{align*}
P\{N(I_j) = k_j, j \in [n]\} &= \prod_{j \in [n]}P\{N(I_j) = k_j\}.
\end{align*}
%\end{defn}
%%\begin{rem}
%%For any collection of points  $t_0 = 0 < t_1 <t_{2} < \ldots < t_{n}$, we can define mutually exclusive intervals $I_j = (t_{j-1}, t_j]$ for $j \in [n]$. 
%A stationary and independent increment counting process is the one that has the joint probability distribution, a product of marginals and each marginal depends solely on the interval length $|I_j|$. 
%That is, 
%\begin{align*}
%P\{N(I_j) = n_j,{j \in [n]}\} &= \prod_{j \in [n]}P\{N(|I_j|) = n_j\}.
%\end{align*} 
%%\end{rem}

\begin{lem} 
An arrival process $\{S_n, n \in \N_0\}$ has stationary and independent increments iff 
the sequence of inter-arrival times $\{X_n: n \in \N\}$ are \textit{iid} random variables.
\end{lem}
\begin{proof} 
We first suppose that $\{X_n: n \in \N\}$ is a sequence of \textit{iid} random variables. 
Then $S_{n+m} - S_m$ has the same distribution as $S_n$ and is independent of $(X_1, \ldots, X_m)$. 
Conversely, we suppose that $\{S_n: n \in \N_0\}$ has stationary and independent increments. 
Then, $\{X_n: n \in \N\}$ is a sequence of \textit{iid} random variables by looking at $X_n = S_n - S_{n-1}$.   
\end{proof}

\begin{lem} If a simple counting process $\{N(t), t \ge 0\}$ has stationary and independent increments then 
the sequence of inter-arrival times $\{X_n: n \in \N\}$ are \textit{iid} random variables.
\end{lem}
\begin{proof} 
%Suppose first that $(X_1,X_2,\ldots)$ is a sequence of \textit{iid} random variables, 
%and then $S_{n+m} - S_m$ has same distribution as $S_n$ and is independent of $(X_1, \ldots, X_m)$. 
%Then, for the ordered disjoint intervals $\{I_j = (a_j ,b_j]: j \in [n]\}$ such that $b_{j-1} \leq a_j$ for each $j$, and $K_j \triangleq \sum_{i = 1}^{j}k_i$, we have
%\begin{align*}
%\{N(I_j) = k_j\} &= \{N(b_j) - N(a_j) = k_j\} = \{S_{N(a_j)+k_j+1} - S_{N(a_j)} > |I_j|, S_{N(a_j)+k_j} - S_{N(a_j)} \le |I_j|\}
%\end{align*}
%Then, for the ordered disjoint intervals $\{I_j = (t_{j-1} ,t_j]: j \in [n]\}$ and $K_j \triangleq \sum_{i = 1}^{j}k_i$, we have 
%\begin{align*}
%P\cap_{j \in [n]}\{N(I_j) = k_j\} &= P\cap_{j \in [n]}\{ N(t_j) =  K_j\}\\
%&= P\cap_{j \in [n]}\{X_{K_j+1}+ S_{K_j} - S_{K_{j-1}} > |I_j|, S_{K_j} - S_{K_{j-1}} \le |I_j|\}. 
%\end{align*}
First, we notice that from inverse relationship, we have 
\begin{align*}
\{X_{n} > y\} = \{ N(S_{n-1}) \le N(S_{n-1} + y) < N(S_{n})\} = \{N(S_{n-1}+y)-N(S_{n-1}) = 0\}. 
\end{align*}
To show that each inter-arrival time is identically distributed, we utilize the stationarity of the increments of the counting process $N(t)$, to observe 
\begin{align*}
%\{S_n - S_{n-1} > x\} &= \{N(S_{n-1}+x) - N(S_{n-1}) = 0\}\\
P\{S_n - S_{n-1} > y\} &%= P\{N(S_{n-1}+x) - N(S_{n-1}) = 0\}
= \int_{0}^{\infty}P\{N(y) = 0\}dF_{n-1}(t) = P\{N(y) = 0\} = P\{X_1 > y\}.
\end{align*}
%If $\{X_n\}$ are \textit{iid}, then $S_n$ are independent. 
%Hence, for disjoint intervals $I_j = (a_j, b_j]$, we have 
%\begin{align*}
%P\cap_{j=1}^{n}\{N(I_j) = n_j\} &= P\cap_{j=1}^{n}\{ S_{n_j} - S_{}\leq a_j, S_{n_j+1} > b_{j}\} = \int_{}.
%\end{align*}
To show that inter-arrival times are independent, it suffices to show that $X_{n}$ is independent of $S_{n-1}$. 
% to show that all inter-arrival times are independent. 
Since the increments of the counting process $N(t)$ are independent, we see that 
%\begin{align*}
%\{S_n \le x, S_{n+1} - S_n > y\} &= \{S_n \le x, N(y+S_n) - N(S_n) = 0\},\\
%&=\{n = N(S_n) \le N(x), N(y+S_n)-N(S_n)\}. 
%\end{align*}
%Further, we observe that
\begin{align*}
P\{S_{n-1} \le x, X_{n} > y\} &%= P\{N(S_{n-1} + y) - N(S_{n-1}) = 0, S_{n-1} \le x\} 
= \int_{0}^{x}P\{N(y+t) - N(t) = 0|S_{n-1} = t\}dF_{n-1}(t)\\
&= \int_{0}^{x}P\{N(y+t) - N(t) = 0|N(t) = n-1, N(s) < n-1, s < t \}dF_{n-1}(t) \\
&= P\{X_n > y\}F_{n-1}(x).
\end{align*}

\end{proof}

\section{Poisson process}
%\begin{defn} 
A simple counting process $\{N(t),~ t\ge 0\} $ is called a \textbf{Poisson process} with a finite positive rate $\lambda$, 
if the inter-arrival times $\{X_{n}: n \in \N\}$ are \textit{iid} random variables with an exponential distribution of rate $\lambda$. 
That is, it has a distribution function $F$, such that 
 \begin{align*}
 F(x) = P\{X_{1}\le x\} = 
	\begin{cases}
		1-e^{-\lambda x}, & x\ge 0   \\
		0,  & \text{ else}.
	\end{cases}
\end{align*}
%\end{defn}

For many proofs regarding Poisson processes, 
we partition the sample space with the disjoint events $\{N(t) = n\}$ for $n \in \N_0$. 
We need the following lemma that enables us to do that. 
\begin{lem} For any finite time $t > 0$, a Poisson process is finite almost surely.
\end{lem}
\begin{proof} By strong law of large numbers, we have 
\begin{align*}
\lim_{n \to \infty} \frac{S_{n}}{n} = E[X_{1}] = \frac{1}{\lambda}\quad\mathrm{a.s.} 
\end{align*}
%Therefore, we have $S_n \rightarrow \infty$, a.s. This implies $P\{N(t) < \infty\} =1$. To see this, 
Fix $t > 0$ and we define a sample space subset $M = \{\omega \in \Omega: N(t)(\omega) = \infty \}$. 
For any $\omega \in M$, we have $S_{n}(\omega)\le t$ for all $n \in \N$. 
This implies $\lim\sup_n\frac{S_{n}}{n} = 0$  and $\omega \not\in \{\lim_n \frac{S_{n}}{n} = \frac{1}{\lambda} \}.$ Hence, the probability measure for set $M$ is zero. 
\end{proof}

\subsection{Memoryless distribution}
%\begin{defn} 
A random variable $X$ with continuous support on $\R_+$, is called \textbf{memoryless} if %for all $t,s \in \R_+$, we have
\begin{align*}
  P\{X>s\} &= P\{ X > t+s| X>t\}~\forall t,s \in \R_+.% = \frac{ P\{ X>t+s, X>t\}}{P\{X>t\}}.
\end{align*}
%\end{defn}
\begin{prop} The unique memoryless distribution function with continuous support on $\R_+$ is the exponential distribution.
\end{prop}
\begin{proof}
Let $X$ be a random variable with a memoryless distribution function $F: \R_+ \to [0,1]$.  
It follows that $\bar{F}(t) \triangleq 1 - F(t)$ satisfies the semi-group property 
\begin{align*}
 \bar{F}(t+s) = \bar{F}(t)\bar{F}(s).
 %P\{X>s\} &= P\{ X > t+s| X>t\} \\&= \frac{ P\{ X>t+s, X>t\}}{P\{X>t\}}.
\end{align*}
%Since $\{X > t + s\} =\{ X>t+s, X>t\}$, we have $g(t+s) = g(t)g(s)$ and hence $g(0) = g^2(0)$. Therefore, $g(0)$ is either unity or zero. Note, that $g$ is a right continuous function and is non-negative. 
%
%We will show that $g$ is an exponential function. That is, $g(t) = e^{\theta t}$ for some $\theta \ge 0$. We will prove this in stages. First, we show this is true for $t \in \mathbb{Z}^+$. Notice that we can obtain via induction
%\begin{eqnarray*}
%	g(2) &=& g(1) g(1) = g^{2}(1), \mathrm{ and }\\
%	g(m) &=& [g(1)]^{m}.
%\end{eqnarray*}
%Since $g(1)$ is non negative, there exists a $\beta$ such that $g(1)=e^{\beta}$ and $g(m)= e^{m \beta}, m \in \mathbb{Z}_{+}$. Next we show that for any $n \in \mathbb{Z}_{+}$,        
%\begin{align*}
%	g(1) =  g\left(\frac{1}{n}+..., +\frac{1}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{n}.
%\end{align*}
%Therefore, for same $\beta$ we used for $g(1)$, we have $g\left(\frac{1}{n}\right) = e^{\frac{\beta}{n}}$. Now, we show that $g$ is exponential for any $t \in \mathbb{Q}^+$. To this end, we see that for any $m, n \in \mathbb{Z}_{+}$, we have 
%\begin{align*}
%	g\left(\frac{m}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{m}= e^{\frac{m \beta}{n}}.
%\end{align*}
%Now, we can show that $g$ is exponential for any real positive $t$ by taking a sequence of rational numbers $\{t_n\}$ decreasing to t. From right continuity of $g$, we obtain 
%\begin{align*}
%	g(t) \stackrel{(a)}{=} \lim_{n\rightarrow \infty} g(t_n) =   \lim_{n\rightarrow \infty} e^{\beta t_{n}}= e^{\beta t}.
%\end{align*}
Since $\bar{F}(x) = P\{X > x\}$  is non-increasing in $x \in \R_+$, we have $\bar{F}(x) = e^{\theta x}$, for some $\theta < 0$ from Lemma~\ref{lem:semigroup}.
\end{proof}

\subsection{Distribution functions}
\begin{lem} Moment generating function of arrival times $S_n$ is 
 \begin{align*}
\E[e^{\theta S_n} ] &= \begin{cases}
\frac{\lambda^n}{(\lambda-\theta)^n}, & \theta < \lambda \\ \infty, & \theta \ge \lambda.
\end{cases} 
 \end{align*} 
\end{lem}
\begin{lem} Distribution function of $S_n$ is given by 
 \begin{align*}
 F_n(t) &\triangleq P\{S_n \leq t\} = 1 - e^{-\lambda t}\sum_{k=0}^{n-1}\frac{(\lambda t)^k}{k!}.
 \end{align*}
\end{lem}
% UNCOMMENT THIS %
%\begin{proof} 
%Since $S_n = \sum_{k=1}^nX_k$, where $X_k$ are \textit{iid}, the moment generating function $\mathbb{E} [ e^{\theta S_{n}} ]$ of $S_n$ is 
% \begin{align*}
%  \mathbb{E} [ e^{\theta S_{n}} ] = \left(\mathbb{E}[e^{\theta X_{1}}]\right)^{n}. 
% \end{align*} 
%Since each $X_k$ is \textit{iid} exponential with rate $\lambda$, it is easy to see that moment generating function of inter-arrival time $X_1$ is 
% \begin{align*}
%  \mathbb{E} [ e^{\theta X_1} ] = 
%		\begin{cases}
%		\frac{\lambda}{(\lambda-\theta)}, & \theta < \lambda \\
%		\infty, & \theta \ge \lambda.
%		\end{cases} 
% \end{align*} 
%\end{proof}

\begin{thm} Density function of $S_n$ is Gamma distributed with parameters $n$ and $\lambda$. That is,
\begin{align*}
f_{n}(s) =\frac{\lambda (\lambda s)^{n-1}} {(n-1)!} e^{-\lambda s}.
\end{align*}
\end{thm}
% UNCOMMENT THIS %
%\begin{proof} Notice that $X_i$'s are \textit{iid} and $S_1 = X_1$. In addition, we know that $S_n = X_n + S_{n-1}$. Since, $X_n$ is independent of $S_{n-1}$, we know that distribution of $S_n$ would be convolution of distribution of $S_{n-1}$ and $X_1$. Since $X_n$ and $S_1$ have identical distribution, we have $f_{n}=f_{n-1}*f_{1}$. The result follows from straightforward induction.
%\end{proof}

%Process $N(t)$ is of real interest, and we can compute the distribution of $N(t)$ for each $t$ from the distribution of $S_n$ in the following.
%\begin{figure}[hhhh]
%\center
	%\include{./Figures/Poisson}
 %% \caption{}\label{}
%\end{figure}
\begin{thm} For each $t >0$, the distribution of Poisson process $N(t)$ with parameter $\lambda$ is given by
	\begin{align*}
	P\{N(t)=n)\}= e^{-\lambda t}\frac{(\lambda t)^{n}}{n!}.
	\end{align*}
Further, $\E[N(t)] = \lambda t$, explaining the rate parameter $\lambda$ for Poisson process.
\end{thm}
\begin{proof}
Result follows from density of $S_n$ and recognizing that 
\begin{align*}
P_n(t) = F_n(t) - F_{n+1}(t).
\end{align*}
%\begin{eqnarray*}
%   P\{N(t) =n\}&=&  P\{S_{n}\le t, S_{n+1} >t)\\
%   &=&  \int^{t}_{0} P\left\{ {S_{n+1}>t}|{S_{n}=s}\right\}f_{n}(s)  ds\\
%   &\stackrel{(a)}{=}& \int^{t}_{0} P\{X_{n+1}>t-s\} f_{n}(s) ds\\
%   &=&  \int^{t}_{0}e^{-\lambda(t- s)} \frac{\lambda^{n}s^{n-1}}{(n-1)!}e^{-\lambda s}  ds\\
%   &=&\frac{e^{-\lambda t} (\lambda t)^{n}}{n !}.
%\end{eqnarray*}
% where (a) follows from the memoryless property of exponential distribution. %($P\{X_{n+1}>s+t|X_{k+1}>t)=P\{X_{k+1}>s)$).\\
\end{proof}

\begin{cor} Distribution of arrival times $S_n$ is 
\begin{xalignat*}{3}
&F_n(t)= \sum_{j \geq n}P_j(t),&&\sum_{n \in \N}F_n(t) = \E N(t).%e^{-\lambda t}\frac{(\lambda t)^{j}}{j!}.
\end{xalignat*}
%Further, $\sum_{n \in \N_0}F_n(t) = 1 + \lambda t$.
\end{cor}
\begin{proof}
First result follows from the telescopic sum and the second from the following observation.
\begin{align*}
\sum_{n \in \N}F_n(t) &= \E\sum_{n \in \N}1\{N(t) \ge n\} = \sum_{n \in \N}P\{N(t) \ge n\} = \E N(t).
\end{align*}
\end{proof}
%\begin{proof}[Omit]
%Result follows from distribution of $P_n(t)$ and recognizing that $F_n(t) =  \sum_{j \geq n}P_j(t)$.
%Further, we notice that
%\begin{align*}
%\sum_{n \in \N_0}F_n(t) &=  \sum_{n \in \N_0}\sum_{j \geq n}P_j(t) = \sum_{i \in \N_0}\sum_{n = 0}^{j}P_j(t) = \sum_{i \in \N_0}(j+1)P_j(t)\\
%&= 1 + E[N(t)] = 1 + \lambda t.
%\end{align*}
%\end{proof}

%\begin{rem} 
A Poisson process is not a stationary process. 
That is, the finite dimensional distributions are not shift invariant. 
This is clear from looking at the first moment $\E N(t) = \lambda t$, which is linearly increasing in time. 
%\end{rem}

\subsection{Age and excess time}
At any time $t$, the instant of last and next arrivals are $S_{N(t)}$ and $S_{N(t)+1}$ respectively. 
\textbf{Age} of a counting process defined as age from the last arrival, and the \textbf{excess} is defined as remaining time till next arrival, 
\begin{xalignat*}{3}
&A(t) = t - S_{N(t)} && Y(t) = S_{N(t)+1} - t.
\end{xalignat*}
\begin{lem} 
Age and residual processes for a Poisson process are independent and the corresponding residual process has distribution same as inter-arrival distribution
\end{lem}
\begin{proof} 
We first find the distribution of age $A(s)$ and excess time $Y(s)$ individually. 
Using stationary increment property of the counting process $N(t)$, we can write 
\eq{
P\{A(s) > x\} &= \sum_{n \in \N_0}P\{N(s) - N(s-x) = 0, N(s) = n\} = \sum_{n \in \N_0}P\{N(x) = 0, N(s-x) = n\} = P_0(x),\\
P\{Y(s) > y\} &= \sum_{n \in \N_0}P\{N(s+y) - N(s) = 0, N(s) = n\} = \sum_{n \in \N_0}P\{N(y) = 0, N(s) = n\} = P_0(y).
}
%Since the $(n+1)$st inter-arrival time $X_{n+1}$ is distributed exponentially with rate $\lambda$ and is independent of $S_{n}$, it follows that for $s > x$ 
%\eq{
%P\{A(s) > x\} &= e^{-\lambda s} + \sum_{n \in \N}\int_{0}^{s-x}dF_n(u)P\{X_{n+1} > s-u\} = e^{-\lambda s} + \int_{0}^{s-x}e^{-\lambda(s-u)}
%\sum_{n \in \N_0}dF_n(u)\\
%&= e^{-\lambda s}(1 + \int_{0}^{s-x}e^{\lambda u} \lambda du) = 1 - F_1(x).
%}
%Similarly, we can compute the distribution of excess time for $s > 0$ as 
%\eq{
%P\{Y(s) > y\} &= e^{-\lambda (s+y)} + \sum_{n \in \N}\int_{0}^{s}dF_n(u)P\{X_{n+1} > s+y-u\} = e^{-\lambda (s+y)} + \int_{0}^{s}e^{-\lambda(s+y-u)}
%\lambda du\\
%&= e^{-\lambda (s+y)}(1 + \int_{0}^{s}e^{\lambda u} \lambda du) = 1 - F_1(y).
%}
Since the counting process $N(t)$ has stationary and independent increments, 
we can write the joint probability as %of the joint event $\{A(s) > x, Y(s) > y\}$ as 
\eq{
P\{A(s)> x, Y(s) > y\} &= \sum_{n \in \N_0}P\{N(s+y)-N(s-x) =0, N(s) = n\} = \sum_{n \in \N_0}P\{N(y+x) =0, N(s-x) = n\}\\
&= P\{N(y+x) = 0\} = P\{N(y+x) - N(y) = 0\}P\{N(y) = 0\} = P_0(x)P_0(y).
}
%\begin{align*}
%\{A(s) > x, Y(s)> y\} %&= \bigcup_{n \in \N_0}\{X' \leq x, X'' > y, N(s) = n\},\\
%&= \bigcup_{n \in \N_0}\{ S_n <  s -x , S_{n+1} > s + y, N(s) = n\}\\
%&= \bigcup_{n \in \N_0}\{ S_n <  s -x , S_{n+1} > s + y\}.
%\end{align*}
%Since the inter-arrival times are \textit{iid} exponentially distributed with rate $\lambda$, we conclude for $s > x$
%\begin{align*}
%P\{A(s)> x, Y(s) > y\} &= \bar{F}_1(s+y) + \sum_{n \in \N}\int_{0}^{s-x}P\{X_{n+1} > s + y - u \}dF_n(u)
%%&= \int_{u=0}^{s-x}(1 - F_1(s+y+u))\sum_{n \in \N_0}dF_n(u) 
%= e^{-\lambda (s+y)}(1 + \int_{0}^{s-x}e^{\lambda u}\lambda du),\\
%&= \bar{F}_1(x)\bar{F}_1(y) = P\{A(s) > x\}P\{Y(s) > y\}
%\end{align*}
Therefore,  $Y(s)$ is independent of $A(s)$ and they both have the same exponential distribution as $X_{n+1}$. 
The memoryless property of exponential distribution is crucially used. 
\end{proof}


\appendix 
\section{Functions with semigroup property}
\begin{lem} 
\label{lem:semigroup}
A unique non-negative right continuous function $f: \R \to \R$ satisfying equation
\begin{align*}
 f(t+s) = f(t)f(s), \text{ for all } t,s \in \R
 \end{align*}
 is $f(t) = e^{\theta t}$, where $\theta = \log f(1)$.
\end{lem}
\begin{proof}
Clearly, we have $f(0) = f^2(0)$. Since $f$ is non-negative, it means $f(0) = 1$. 
By definition of $\theta$ and induction for $m,n \in \Z^+$, we see that 
\begin{xalignat*}{3}
&f(m) = f(1)^m = e^{\theta m},&& e^{\theta} = f(1) = f(1/n)^n.
 \end{xalignat*}
Let $q \in \Q$, then it can be written as $m/n, n \neq 0$ for some $m,n \in \Z^+$. 
Hence, it is clear that for all $q \in \Q^+$, we have $f(q) = e^{\theta q}$.
either unity or zero. Note, that $f$ is a right continuous function and is non-negative. 
%We will show that $g$ is an exponential function. That is, $g(t) = e^{\theta t}$ for some $\theta \ge 0$. We will prove this in stages. First, we show this is true for $t \in \mathbb{Z}^+$. Notice that we can obtain via induction
%\begin{eqnarray*}
%	g(2) &=& g(1) g(1) = g^{2}(1), \mathrm{ and }\\
%	g(m) &=& [g(1)]^{m}.
%\end{eqnarray*}
%Since $g(1)$ is non negative, there exists a $\beta$ such that $g(1)=e^{\beta}$ and $g(m)= e^{m \beta}, m \in \mathbb{Z}_{+}$. Next we show that for any $n \in \mathbb{Z}_{+}$,        
%\begin{align*}
%	g(1) =  g\left(\frac{1}{n}+..., +\frac{1}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{n}.
%\end{align*}
%Therefore, for same $\beta$ we used for $g(1)$, we have $g\left(\frac{1}{n}\right) = e^{\frac{\beta}{n}}$. Now, we show that $g$ is exponential for any $t \in \mathbb{Q}^+$. To this end, we see that for any $m, n \in \mathbb{Z}_{+}$, we have 
%\begin{align*}
%	g\left(\frac{m}{n}\right) = \left[g\left(\frac{1}{n}\right)\right]^{m}= e^{\frac{m \beta}{n}}.
%\end{align*}
Now, we can show that $f$ is exponential for any real positive $t$ by taking a sequence of rational numbers $\{t_n\}$ decreasing to $t$. 
From right continuity of $f$, we obtain 
\begin{align*}
f(t) &= \lim_{t_n \downarrow t} f(t_n) =   \lim_{t_n \downarrow t} e^{\beta t_{n}}= e^{\beta t}.
\end{align*}
\end{proof}
\end{document}