% !TEX spellcheck = en_US
% !TEX spellcheck = LaTeX
\documentclass[a4paper,10pt,english]{article}
\input{header}
\title{Lecture 23 : Martingales}
\author{}
\begin{document}
\maketitle
\section{Martingales}
A \textbf{filtration} is an increasing sequence of $\sigma$-fields, with $n$th $\sigma$-field denoted by $\F_n$. 
A sequence $X = \{X_n: n \in \N\}$ of random variables is said to be \textbf{adapted} to the filtration $\{\F_n: n \in \N\}$ if $X_n \in \F_n$. 
%A martingale is a type of stochastic process whose definition formalizes the concept of a fair game.
%\begin{defn}
A discrete stochastic process $\{X_n,~n \in \N \}$ is said to be a \textbf{martingale} with respect to $\{\F_n: n \in \N\}$ if 
\begin{enumerate}[i\_]
\item $\E[|X_n|]< \infty$,
\item $X_n$ is adapted to $\F_n$,
\item $\E[X_{n+1}|\F_n]=X_n$, for each $n \in \N$.
\end{enumerate}
If the equality in third condition is replaced by $\leq$ or $\geq$, then the process is called \textbf{supermartingale} or \textbf{submartingale}, respectively.
%\end{defn}
For a discrete stochastic process $X = \{X_n: n \in \N\}$, its \textbf{natural filtration} is defined as 
\eq{
\F_n \triangleq \sigma(X_1, \dots, X_n).
}
\begin{cor} For a martingale $X$ adapted to a filtration $\F$, for each $n \in \N$
\eq{
\E X_n &= \E X_1.
}
\end{cor}
%Taking expectation on both sides of part 2 of the above definition, we get $\E[Z_{n+1}]=\E[Z_n]$, and hence $\E[Z_{n+1}]=\E[Z_1]$, for all $n$.
%\end{cor}
\begin{shaded*}
\begin{exmp*}[Simple random walk]
Let $\{X_i: i \in \N\}$ be a sequence of independent random variables with mean $\E X_i = 0$ and $\E|X_i| < \infty$ for each $i \in \N$. 
Let $Z_n=\sum_{i=1}^n X_i$ and $\F_n = \sigma(X_1, \dots, X_n)$ for each $n \in \N$. 
Then, $\{Z_n,~n \in \N \}$ is a martingale with respect to the natural filtration of $X$.  
This follows, since $\E Z_n=0$ and 
\begin{align*}
\E[Z_{n+1}|\F_n] =\E[Z_{n}+X_{n+1}|\F_n] &=Z_n.
\end{align*} 
\end{exmp*}
\end{shaded*}
\begin{shaded*}
\begin{exmp*}[Product martingale]
Let $\{X_i: i \in \N\}$ be a sequence of independent random variables with mean $\E X_i = 1$ and $\E|X_i| < \infty$ for each $i \in \N$. 
Let $Z_n=\Pi_{i=1}^n X_i$ and $\F_n = \sigma(X_1, \dots, X_n)$. 
Then, $\{Z_n,~n \in \N \}$ is a martingale with respect to the natural filtration of $X$. 
This follows, since $\E Z_n=1$ and 
\begin{align*}
\E[Z_{n+1}|\F_n] =\E[Z_{n}X_{n+1}|\F_n] &=Z_n.
\end{align*} 
\end{exmp*}
\end{shaded*}

\begin{shaded*}
\begin{exmp*}[Branching process] 
Consider a population where each individual $i$ can produce an independent random number of offsprings $Z_i$ in its lifetime, given by a common distribution $P= \{P_j : j \in \N_0\}$ and mean $\mu = \sum_{j \in \N}jP_j$.  
Let $X_n$ denote the size of the $n$th generation, which is same as number of offsprings generated by $(n-1)$th generation. 
The discrete stochastic process $\{X_n \in \N_0: n \in \N\}$ is called a branching process. 
Let $X_0=1$ and $\F_n = \sigma(X_1, \dots, X_n)$, then,
\begin{align*}
X_n = \sum_{i=1}^{X_{n-1}}Z_i.
\end{align*}
%where $Z_i$ represents the number of offspring of the individual $i$ of the $(n-1)$th generation. 
Conditioning on $X_{n-1}$ yields, $\E[X_n]= \mu^n$ where $\mu$ is the mean number of offspring per individual. Then $\{Y_n = X_n / \mu^n: n \in \N\}$ is a martingale because $\E[Y_n]= 1$ and 
\begin{align*}
\E[Y_{n+1}|\F_n] %&= \frac{1}{\mu^{n+1}}\E[X_{n+1}|Y_1, \hdots Y_n]\\
&= \frac{1}{\mu^{n+1}}\E[\sum_{i=1}^{X_{n}}Z_i|\F_n]
%&=  \frac{1}{\mu^{n+1}}X_{n}\E[Z_i]
= \frac{X_n}{\mu^n}=Y_n.
\end{align*}
\end{exmp*}
\end{shaded*}

\begin{shaded*}
\begin{exmp*} [Doob's Martingale]
Let $X$ be an arbitrary random variable such that $\E[|X|]< \infty$, and $Y = \{Y_n: n \in \N\}$ be an arbitrary random sequence. 
Let $\F$ be the natural filtration associated with the stochastic process $Y$, then
\begin{align*}
X_n = \E[X|\F_n]
\end{align*}
is a martingale. 
The integrability condition can be directly verified, and
\begin{align*}
\E[X_{n+1}|\F_n]&= \E[\E[X|\F_{n+1}]|\F_{n}] = \E[X|\F_{n}] = X_n.
\end{align*} 
%Thus the result follows. The above martingale is called the Doob type martingale.
\end{exmp*}
\end{shaded*}

\begin{shaded*}
\begin{exmp*}[Centralized Doob sequence]
For any sequence of random variables $X = \{X_n: n \in \N\}$ and its natural filtration $\F$, 
the random variables $X_i-\E[X_i|\F_{i-1}]$ have zero mean, then 
\begin{align*}
Z_n =\sum_{i=1}^n (X_i -\E[X_i|\F_{i-1}])
\end{align*}
is a martingale with respect to $\F$, provided $\E |Z_n|< \infty$.  
To verify the same, 
\begin{align*}
\E[Z_{n+1}|\F_n]&= \E[Z_n+X_n-\E[X_n|\F_{n-1}]|\F_n] = Z_n+\E[X_n-\E[X_n|\F_{n-1}]]=Z_n.\\
\end{align*}
\end{exmp*}
\end{shaded*}

\begin{lem}
\label{ConvexFuncSubmart}
If $X = \{X_n: n \in \N\}$ is a martingale with respect to a filtration $\{\F_n: n \in \N\}$ and $f$ is a convex function, 
then $\{f(X_n): n \in \N\}$ is a sub martigale with respect to the same filtration.
\end{lem}
\begin{proof}
The result is a direct consequence of Jensen's inequality.
\begin{align*}
\E[f(X_{n+1})|\F_n] &\geq f(\E[X_{n+1}|\F_n])=f(X_n).
\end{align*}
\end{proof}

\begin{cor} 
Let $a \in \R$ be a constant. 
\begin{enumerate}[i\_]
\item If $\{X_n : n \in \N\}$ is a submartingale, then so is $\{(X_n - a)_+: n \in \N\}$.  
\item If $\{X_n : n \in \N\}$ is a supermartingale, then so is $\{X_n \wedge a: n \in \N\}$. 
\end{enumerate}
\end{cor}

\subsection{Stopping Times}
Consider a discrete filtration $\F = \{\F_n: n \in \N_0\}$.  
%\begin{defn}
A positive integer valued, possibly infinite, random variable $N$ is said to be a \textbf{random time} with respect to the 
filtration $\F$,  if the event $\{N=n\} \in \F_n$ for each $n \in \N$. 
%process $\{X_n: n \in \N\}$ if the event $\{N=n\}$ is determined by the random variables $X_1 \hdots X_n$. 
%That is, if $\F$ is the natural filtration associated with the discrete process $X$, then $\{N = n \} \in \F_n$. 
If $\Pr\{N < \infty\}=1$, then the random time $N$ is said to be a \textbf{stopping time}. 
%\end{defn}
%\begin{defn}
%Consider a discrete filtration $\F = \{\F_n: n \in \N\}$.  
A sequence $\{H_n: n\in \N\}$ is \textbf{predictable} with respect to the the filtration $\F$, if $H_n \in \F_{n-1}$ for each $n \in \N$. 
Further, we define 
%process $\{X_n\}$ is the one where $H_n$ is completely determined by $X_1,X_2,...,X_{n-1}$
\begin{align*}
(H\cdot X)_n &\triangleq \sum_{m=1}^{n}H_m(X_m-X_{m-1}).
\end{align*}
%\end{defn}
\begin{thm}
Let $\{X_n: n \in \N_0\}$ be a super martingale with respect to a filtration $\F$. 
If $H = \{H_n: n \in \N\}$ is predictable with respect to $\F$ and each $H_n$ is non-negative and bounded, 
then $(H \cdot X)_n$ is a super martingale w.r.t. $\F$. 
\end{thm}
%<<<<<<< HEAD
%\begin{proof} It is easy to check that $(H\cdot X)_n \in \F_n$. Thhe boundedness of the $H_n$ implies $E|(H\cdot X)_n|<\infty$ for each $n$. With this established, we can compute conditional expectations to conclude
%\begin{align*}
%E((H\cdot X)_{n+1}| \F_n)&=(H\cdot X)_n+E(H_{n+1}(X_{n+1}-X_n)| \F_n)\\
%       &=(HX)_n+H_{n+1}E(X_{n+1}-X_n| \F_n)\\
%       &=(H\cdot X)_n.\qedhere
%\end{align*}
%since $H_{n+1}\in \F_n$ and $E(X_{n+1}-X_n| \F_n)=0$
%\end{proof}
%The last theorem can be interpreted as: you can't make money by gambling on a fair game. This conclusion does not hold if we only assume that $H_n$ is optional, that is $H_n\in \mathcal(F)_n$, since then we can base our bet on the outcome of the coin we are betting on.
%
%\begin{thm} 
%Suppose $M_0,M_1,\dots $ is a martingale with respect to$\{\F_n\}$ and suppose $T$ is a stopping time. Suppose that $T$ is bounded, $T\leq K$. Then
%\begin{equation*}
%\E(M_T|\F_0)=M_0.
%\end{equation*}
%In particular, $\E(M_T)=\E(M_0)$.
% \end{thm}
%To prove this fact, we first note that the event $\{T>n\}$ is measurable with respect to $\F_n$ (since we need only the information up through time $n$ to determine if we have stopped by time $n$). Since $M_T$ is the random variable which equals $M_j$ if $T=j$ we can write
%\begin{equation*}
%M_T = \sum_{j=0}^K M_j I\{T=j\}.
%\end{equation*}
%Let us take the conditional expectation with respect to $\F_{K-1}$,
%\begin{equation*}
%\E(M_T|\mathcal(F)_{K-1})=\E(M_KI\{T=K\}|\F_{K-1})+\sum_{j=0}^{K-1} \E(M_jI\{T=j\}|\F_{K-1}).
%\end{equation*}
%For $j\leq K-1, M_jI\{T=j\}$ is $\F_{K-1}$- measurable; hence
%\begin{equation*}
%\E(M_jI\{T=j\}|\F_{K-1})=M_jI\{T=j\}.
%\end{equation*}
%Since $T$ is known to be no more than $K$, the event $\{T=K\}$ is the same as the event $\{T>K-1\}$. The latter event is measurable with respect to $\F_{K-1}$. Hence using equality
%\begin{equation*}
%\E(YZ|\F_n)= Z\E(Y|\F_n).
%\end{equation*}
%Where $Y$ is any random variable and $Z$ is a random variable that is measurable with respect to finite number of random variables $X_1,X_2,\dots,X_n$.
%
%\begin{align*}
%\E(M_KI\{T=K\}|\F_{K-1}) &=\E(M_KI\{T>K-1\}|\F_{K-1})\\
%&=I\{T>K-1\}\E(M_K|\F_{K-1})\\
%&=I\{T>K-1\}M_{K-1}.
%\end{align*}
%
%The last equality follows from the fact the $M_n$ is a martingale. Therefore,
%\begin{align*}
%\E(M_T|\F_{K-1}) &=I\{T>K-1\}M_{K-1}+\sum_{j=0}^{K-1}M_jI\{T=j\}\\
%&=I\{T>K-2\}M_{K-1}+\sum_{j=0}^{K-2}M_jI\{T=j\}.
%\end{align*}
%If we work through this argument again, this time conditioning with respect to $\F_{K-2}$, we gat
%\begin{align*}
%\E(M_T|\F_{K-2}) &=\E(\E(M_T|\F_{K-1})|\F_{K-2})\\
%&=I\{T>K-3\}M_{K-2}+\sum_{j=0}^{K-3}M_jI\{T=j\}.
%\end{align*}
%We can continue this process untill we get
%\begin{equation*}
%\E(M_T|\F_0)=M_0.
%\end{equation*}
%
%There are many examples of interest where the stopping time $T$ is not bounded. Suppose $T$ is a stopping time with $\mathbb{P}\{T<\infty\}=1$, i.e., a rule that guarantees that one stops eventually. (Note that the time associated to the martingale betting strategy satisfies this condition.) When can we conclude that $\E(M_T)=\E(M_0)?$ To investigate this consider the stopping times $T_n=min\{T,n\}$. Note that
%\begin{equation*}
%M_T=M_{T_{n}}+M_TI\{T>n\}-M_nI\{T>n\}.
%\end{equation*}
%Hence,
%\begin{equation*}
%\E(M_T)=\E(M_{T_{n}})+\E(M_TI\{T>n\})-\E(M_nI\{T>n\}).
%\end{equation*}
%
%Since $T_n$ is a bounded stopping time, it follows from the above that $\E(M_{T_{n}})=\E(M_0)$. We would like to be able to say that the other termsdo not contribute as $n\to \infty$. The second term is not much of a problem.  Since the probability of the event $\{T>n\}$ goes to 0 as $n\to \infty$, we are taking the expectation of the random variable $M_T$ restricted to a smaller and smaller set. One can show that if $\E(|M_T|)<\infty$ then $\E(|M_T|I\{T>n\}) \to 0$.
%
%The third term, if $M_n$ and $T$ are given satisfying
%\begin{equation*}
%lim_{n\to \infty}\E(|M_n|I\{T>n\})=0,
%\end{equation*}
%then we will be able to conclude that $\E(M_T)=\E(M_0)$. We summarize this as follows.\newline
%\textbf{Optional Sampling Theorem.} Suppose $M_0,M_1,\dots$ is a martingale with respect to $\{\F_n\}$ and $T$ is a stopping time satisying $\mathbb{P}\{T<\infty\}=1$,
%\begin{equation*}
%\E(|M_T|)<\infty,
%\end{equation*}
%and
%\begin{equation*}
%\lim_{n\to \infty}\E(|M_n|I\{T>n\})=0.
%\end{equation*}
%Then, $\E(M_T)=\E(M_0)$.
%
%\section{Polya's Urn Scheme}
%Suppose an urn initially contains $b_0$ black balls and $w_0$ white balls. Suppose balls are sampled from the urn one at a time, but after each draw $1$ balls of the same color are returned to the urn. If first draw is a black, then replace $b_0$ with $b_0+1$ balls in the urn and $w_0$ with $w_0+1$ for white balls. The number of black balls in the first $n$ draws would then have a $Bin(n,\frac{b_0}{b_0+w_0})$.
%Let $B_n$ be the number of black balls in urn after $n$ draws and $B_0 = b_0$. Now probability of getting a black ball in first draw is
% \begin{equation*}
%\mathbb{P}(B_1=b_0+1)= \frac{b_0}{b_0+w_0},
%\end{equation*}
%and probability of a getting white ball in first draw is
% \begin{equation*}
%\mathbb{P}(B_1=b_0)= \frac{w_0}{b_0+w_0}.
%\end{equation*}
%Similarly after two draws,
% \begin{align*}
%\mathbb{P}(B_2=b_0)= \frac{w_0}{b_0+w_0} \cdot \frac{w_0+1}{b_0+w_0+1}\\
%\mathbb{P}(B_2=b_0+1)= \frac{b_0}{b_0+w_0} \cdot \frac{w_0}{b_0+w_0+1}+\frac{w_0}{b_0+w_0} \cdot \frac{b_0}{b_0+w_0+1}\\
%\mathbb{P}(B_2=b_0)= \frac{b_0}{b_0+w_0} \cdot \frac{b_0+1}{b_0+w_0+1}.
%\end{align*}
%For first three draws,
% \begin{align*}
%\mathbb{P}(\text{first 3 draws are } bwb)= \frac{b_0}{b_0+w_0} \cdot \frac{w_0}{b_0+w_0+1} \cdot \frac{b_0+1}{b_0+w_0+2}\\
%\mathbb{P}(\text{first 3 draws are } bbw)= \frac{b_0}{b_0+w_0} \cdot \frac{b_0+1}{b_0+w_0+1} \cdot \frac{w_0}{b_0+w_0+2}\\
%\mathbb{P}(\text{first 3 draws are } wbb)= \frac{w_0}{b_0+w_0} \cdot \frac{b_0}{b_0+w_0+1} \cdot \frac{b_0+1}{b_0+w_0+2},
%\end{align*}
%where $b$ stands for black ball and $w$ stands for white ball. We observe above 3 equations, all of them are same. like wise
% \begin{align*}
%\mathbb{P}(bbwww)= \frac{b_0(b_0+1)w_0(w_0+1)(w_0+2)}{\prod_{i=0}^4 (b_0+w_0+i)}\\
%\mathbb{P}(bwwwb)= \frac{b_0w_0(w_0+1)(w_0+2)(b_0+1)}{\prod_{i=0}^4 (b_0+w_0+i)}.
%\end{align*}
%Again above two probabilities are equal.
%\begin{defn}
%An infinite sequence $\{X_i\}_{i=1}^{\infty}$ of random variables is exchangeable if $\forall$ $n=1,2,\dots$
% \begin{equation*}
%X_1,\dots,X_n=X_{\pi(1)},\dots ,X_{\pi(n)}, \forall \pi \in S(n),
%\end{equation*}
%where $S(n)$ is the symmetric group, the group of permutations.
%\end{defn}
%Polya's Urn Model is one of the examples for exchangeability. An Example would be following
% \begin{equation*}
%\mathbb{P}(bbwww)=\mathbb{P}(bwwwb)
%\end{equation*}
%If $\xi_1,\xi_2,\dots,\xi_n$ denote the sigma algebra for the color of the drawn ball i.e., $\xi_i$ represents the color of the $i^{th}$ ball, from the definition of excangeability
%\begin{equation*}
%(\xi_1,\xi_2,\xi_3,\xi_4,\xi_5)=(\xi_2,\xi_1,\xi_5,\xi_4,\xi_3).
%\end{equation*}
%\begin{note}
%Polya's Urn scheme generate exchangeable sequences.
%\end{note}
%Let 
%\begin{equation*}
%X_n=\frac{B_n}{B_n+W_n}=\frac{B_n}{b_0+w_0+n},\ 0\leq X_n \leq1,
%\end{equation*}
%represents the proportion of black balls after $n$ draws, then given the past $\xi_1,\xi_2,\dots,\xi_n$
%\begin{equation*}
%B_{n+1} = \begin{cases}
%B_n  & w.p \ (1-\frac{B_n}{B_n+W_n}) = 1 -X_n \\
%B_{n+1} & %if \ \xi_{n+1} \ 
%w.p \ \frac{B_n}{B_n+W_n} = X_n.
%\end{cases}
%\end{equation*}
%Now
%\begin{align*}
%\E[X_{n+1}|\xi_1,\xi_2,\dots\xi_n]&=\frac{1}{b_0+w_0+n+1}\E[B_{n+1}|\xi_1,\xi_2,\dots\xi_n]\\
%&=\frac{B_n(1-X_n)+(B_n+1)X_n}{B_n+W_n+1}\\
%&=\frac{B_n+X_n}{B_n+W_n+1} = \frac{B_n(B_n+W_n+1)}{(B_n+W_n+1)(B_n+W_n)}=X_n.
%\end{align*}
%That means it is a martingale.
%\begin{note}
%$X_n$ is a martingale.
%\end{note}
%\subsection{Analysis of the Polya urn model}
%\begin{thm}(De Finetti 1931)
%A binary sequence $\{X_n\}_{i=1}^{\infty}$ is exchageable iff there exixtes a distribution function $F(p)$ on $[0,1]$ such that for any $n\geq1$,
%\begin{equation*}
%\mathbb{P}(X_1=x_1,\dots,X_n=x_n)=\int_0^1 p^{S_n}(1-p)^{n-S_n}dF(p)
%\end{equation*}
%where $S_n=\sum_i x_i$.
\begin{proof}
It follows from the definition, 
\begin{align*}
\E[(H\cdot X)_{n+1}|\F_n]=&\E[H_{n+1}(X_{n+1}-X_n)+(H\cdot X)_n|\F_n] = H_{n+1}(\E[X_{n+1}|\F_n]-X_n)+(H\cdot X)_n \leq (H\cdot X)_n.
\end{align*}
\end{proof}

%Before we state and prove martingale convergence theorem, we state some results which will be used in the proof of the theorem.
\begin{lem}
\label{StoppingTimeBound}
If $\{X_i:  i \in \N \}$ is  a submartingale and $T$ is a stopping time such that $\Pr\{T \leq n\}=1$ then
\begin{align*}
 \E X_1 \leq \E X_T \leq \E X_n.
\end{align*}
\end{lem}
\begin{proof}
Since $T$ is bounded, it follows from Martingale stopping theorem, that $\E X_T  \geq \E X_1$. 
Now, since $T$ is a stopping time, we see that for $\{T = k\}$
\begin{align*}
\E[X_n1\{T = k\}|\F_T,T=k]&= %\E[Z_n1\{T = k\}|\F_k,T=k] = 
\E[X_n1\{T = k\}|\F_k] \geq X_k1\{T = k \} = X_T1\{T = k\}.
\end{align*}
%where $(a)$ follows from the fact that $N$ is  a stopping time. 
Result follows by taking expectation on both sides and summing over $k$. 
That is,
\eq{
\E X_n &= \E\sum_{k=1}^n X_n1\{T = k\} \geq \E\sum_{k=1}^nX_T1\{T=k\} = \E X_T.
}
\end{proof}

\begin{cor}  
Let $T$ be a stopping time and $\{X_n : n \in \N\}$ be a supermartingale, 
then $\{X_{T\wedge n}: n \in \N\}$ os a supermartingale. 
\end{cor}
\subsection{Stopped process}
%\begin{defn}
Consider a discrete stochastic process $X = \{X_n: n\in \N\}$ adapted to a discrete filtration $\F$. 
Let $T$ be a random time for the filtration $\F$, 
then the \textbf{stopped process} $\{X_{T\wedge n}: n \in \N\}$ is defined as 
\begin{align*}
X_{T\wedge n} = X_n1_{\{n \leq T\}} + X_T1_{\{n > T\}}.
\end{align*}
%\end{defn}
\begin{prop}
Let  $\{X_n: n\in \N\}$ be a martingale with a discrete filtration $\F$. 
If $T$ is an integer random time for the filtration $\F$, 
then the stopped process $\{X_{T\wedge n}\}$ is a martingale. 
\end{prop}
\begin{proof}
We observe that $H = \{1\{n\leq T\}: n \in \N\}$ is a non-negative, predictable, and bounded sequence, since 
\eq{
  \{n\leq T\}=\{T>n-1\}=\{T\leq n-1\}^c = (\cup_{i=0}^{n-1}\{T = i\})^c = \cap_{i=1}^{n-1}\{T \neq i\} \in \F_{n-1}. 
}
%Since, we have 
%\begin{align*}
%X_{T \wedge n}&= X_{T\wedge n-1}+1_{\{n \leq T\}}(X_n-X_{n-1}), 
%\end{align*}
In terms of the predictable and bounded sequence $H$, we can write the stopped process as
\eq{
X_{T \wedge n} &= X_0 + \sum_{m=1}^{T\wedge n}(X_{m} - X_{m-1}) = X_0 + \sum_{m=1}^{n}1\{m \leq T\}(X_{m} - X_{m-1}) = X_0 + (H \cdot X)_n. 
}
%\begin{align*}
%    X_{T\wedge n}&= (H\cdot X)_n\hspace{2 mm} when\hspace{2 mm} H_n=1_{\{n\leq T\}}\\
%    X_{T \wedge n}&= X_{T\wedge n-1}+1_{\{n \leq T\}}(X_n-X_{n-1})\\
%\end{align*}
% $n\leq T$:\hspace{2 mm}  $X_{T\wedge n}= X_n$\\
% $n>T$: Since $n>T$ gives $n-1\geq T$ therefore $T\wedge n-1\geq T$ which implies $X_{T\wedge n}=X_{T}$ \\
% \begin{align*}
%     X_{T\wedge n}= X_0 +  \sum_{m=1}^{n} 1_{\{m\leq T\}}(X_m-X_{m-1})
% \end{align*}
%     It is suffice to show $\{1_{n\leq T}\}$ is a predictable sequence which is true since 
%  \begin{align*}
%  \{n\leq T\}=\{T>n-1\}=\{T<n-1\}^c
%  \end{align*}
Therefore from the previous theorem we have 
\begin{align*}
      \E X_{T\wedge n} =\E X_{T\wedge 1} =\E X_{1}. 
\end{align*}


%We claim that 
%\begin{align*}
%\bar{Z}_n= \bar{Z}_{n-1}+1_{\{n \leq N\}}(Z_n-Z_{n-1})
%\end{align*}
%The above equation can be directly verified by considering the two cases separately by
%\begin{align*}
%\bar{Z}_n &= 
%\begin{cases}
%Z_n & n \leq N,\\
%\bar{Z}_{n-1}=Z_N, & n > N.
%\end{cases}
%\end{align*}
%Further, since $N$ is a random time, we see that 
%\begin{align*}
%\E[\bar{Z}_{n+1}|Z_1 \hdots \bar{Z}_n] %&=\E[\bar{Z}_{n}+1_{\{n \leq N\}}(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
%&=\bar{Z}_{n}+1_{\{n \leq N\}} \E[(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n] =\bar{Z}_{n}.
%\end{align*}
\end{proof}

\begin{rem}
For any martingale $\{X_n: n \in \N\}$ w.r.t. $\F$, we have $\E X_{T\wedge n} =\E X_1$, for all $n$.  
Now assume that $T$ is a stopping time w.r.t. $\F$. 
It is immediate that stopped process converges almost surely to $X_T$, i.e. 
\begin{align*}
\Pr\left\{\lim_{n\in \N}{X}_{T\wedge n} = X_T\right\} = 1.
\end{align*}
\end{rem}
We are interested in knowing under what conditions will we have convergence in mean. %, i.e.  
%\begin{align*}
%\lim_{n \in \N}\E X_{T\wedge n} = \E X_T. 
%\end{align*}
%It so turns out that the above is true under some additional regularity constraints only. %We state the following theorem without proof.
\begin{thm}[Martingale stopping theorem]
\label{MartStopThm}
Let $X$ be a martingale and $T$ be a stopping time adapted to a discrete filtration $\F$. 
Then, the random variable $X_T$ is integrable and the stopped process $X_{T \wedge n}$ converges in mean to $X_T$, i.e.
\eq{
\lim_{n \in \N}\E X_{T\wedge n} = \E X_T =\E X_1,
}
if 
%If $T$ is a stopping time for a martingale $\{X_n: n \in \N\}$ such that 
either of the following conditions holds true. 
\begin{enumerate}[(i)]
\item $T$ is bounded, 
\item $X_{T\wedge n}$ is uniformly bounded,
\item $\E T < \infty$, and for some real positive $K$, we have $\sup_{n \in \N}\E[|X_{n+1}-X_n||\F_n] < K$. 
\end{enumerate}
%then $X_T$ is integrable and $\lim_{n \in \N}\E[X_{T\wedge n}] = \E[X_T]=\E[X_1]$.
\end{thm}
\begin{proof} We show this is true for all three cases.
\begin{enumerate}[(i)] 
\item Let $K$ be the bound on $T$ then for all $n \geq K$, we have $X_{T\wedge n} = X_T$, and hence it follows that
\begin{align*}
\E X_1 = \E X_{T\wedge n} &= \E X_T, ~\forall n \geq K.
\end{align*}
\item Dominated convergence theorem implies the result. 
\item Since $T$ is integrable and  
\begin{align*} 
X_{T\wedge n} \leq |X_1| + K T,
\end{align*}
we observe that $X_{T\wedge n}$ is bounded by an integrable random variable, and hence result follows from dominated convergence theorem.
\end{enumerate}
\end{proof}

\begin{cor}[Wald's Equation] 
If $T$ is a stopping time for $\{X_i: i \in \N\}$ \textit{iid} with $\E|X|< \infty$ and $\E T < \infty$, then
\begin{align*}
\E\sum_{i=1}^{T}X_i =\E T\E X.
\end{align*}
\end{cor}
\begin{proof}
Let $\mu=\E X$. Then $\{Z_n = \sum_{i=1}^{n}(X_i-\mu): n \in \N\}$ is a martingale adapted to natural filtration for $X$,  and hence from the Martingale stopping theorem, we have $\E Z_T =\E Z_1=0$. 
But 
\begin{align*}
\E[Z_T] %&= \E[\sum_{i=1}^T (X_i-\mu)]\\
%&=\E[\sum_{i=1}^N (X_i)-N\mu)]\\
&=\E\sum_{i=1}^T X_i- \mu\E T.
\end{align*}
Observe that condition $(iii)$  for Martingale stopping theorem to hold can be directly verified. 
Hence the result follows. 
\end{proof}
%\section{ Submartingales, supermartingales and the Martingale Convergence Theorem}
%%\begin{defn}
%A stochastic process $\{Z_n,~  n \geq 1\}$ having $\E[|Z_n|]< \infty$ for all $n$ is said to be a submartingale if
%\begin{equation}
%\label{Submartingale}
%\E[Z_{n+1}|Z_1 \hdots Z_n] \geq Z_n
%\end{equation}
%and is said to be a supermartingale if
%\begin{equation}
%\label{supermartingale}
%\E[Z_{n+1}|Z_1 \hdots Z_n] \leq Z_n
%\end{equation}
%%\end{defn}
%From \ref{Submartingale}, for a submartingale
%\begin{align*}
%\E[Z_{n+1}] \geq \E[Z_n]
%\end{align*}
%where the inequality is reversed for a supermartingale. 
%\begin{thm}
%\label{Stoppingtime_theorem}
%If $N$ is a stopping time for $\{Z_n,~ n\geq 1\}$ such that any one of the following sufficient conditions is satisfied:
%\begin{enumerate}
%\item $\bar{Z}_n$ is uniformly bounded, or;
%\item $N$ is bounded, or;
%\item $\E[N]< \infty$, and there is an $M < \infty$ such that
%\begin{align*}
%\E[|Z_{n+1}-Z_n| |Z_1, \hdots Z_n]<M,
%\end{align*}
%then,
%\begin{align*}
%\E[Z_N] \geq \E[Z_1] ~ \text {for a submartingale}\\
%\E[Z_N] \leq \E[Z_1] ~ \text {for a supermartingale}.
%\end{align*}
%\end{enumerate}
%\end{thm}
%\begin{proof}
%We claim that 
%\begin{align*}
%\bar{Z}_n= \bar{Z}_{n-1}+1_{N \geq n}(Z_n-Z_{n-1})
%\end{align*}
%The above equation can be directly verified by considering the two cases separately viz. 
%\begin{enumerate}
%\item $N \geq n$: $\bar{Z}_n=Z_n$.
%\item $N < n:$ $\bar{Z}_{n-1}=\bar{Z}_{n}=Z_N$
%\end{enumerate}
%\begin{align*}
%\E[\bar{Z}_{n+1}|Z_1 \hdots \bar{Z}_n]&=\E[\bar{Z}_{n}+1_{n \leq N}(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
%&\stackrel{(a)}{=}\bar{Z}_{n}+1_{n \leq N} \E[(Z_n-Z_{n-1})|Z_1 \hdots \bar{Z}_n]\\
%& \geq \bar{Z}_{n},
%\end{align*}
%where in $(a)$ we have used the fact that $N$ is a random time. Also, we have $\E[\bar{Z}_{n}]=\E[Z_1]$, for all $n$.  Now assume that $N$ is a stopping time. It is immediate that
%\begin{align*}
% \bar{Z}_n \rightarrow Z_N ~ \text{w.p}~ 1.
%\end{align*}
%But  is it true that
%\begin{align*}
% \E[\bar{Z}_n] \rightarrow \E[Z_N] ~ \text{as n}~ \rightarrow \infty.
%\end{align*}
%which gives that 
%\begin{align*}
%\E[Z_N ] \geq \E[Z_1].
%\end{align*}
%\end{proof}
\end{document}

Before we state and prove martingale convergence theorem, we state some results which will be used in the proof of the theorem.
\begin{lem}
\label{StoppingTimeBound}
If $\{X_i:  i \in \N \}$ is  a submartingale and $T$ is a stopping time such that $\Pr\{T \leq n\}=1$ then
\begin{align*}
 \E X_1 \leq \E X_T \leq \E X_n.
\end{align*}
\end{lem}
\begin{proof}
It follows from Theorem \ref{MartStopThm} that since $T$ is bounded, $\E X_T  \geq \E X_1$. 
Now, since $T$ is a stopping time, we see that for $\{T = k\}$
\begin{align*}
\E[X_n1\{T = k\}|\F_T,T=k]&= %\E[Z_n1\{T = k\}|\F_k,T=k] = 
\E[X_n1\{T = k\}|\F_k] \geq X_k1\{T = k \} = X_T1\{T = k\}.
\end{align*}
%where $(a)$ follows from the fact that $N$ is  a stopping time. 
Result follows by taking expectation on both sides and summing over $k$. 
\end{proof}

\begin{lem}
\label{ConvexFuncSubmart}
If $X = \{X_n: n \in \N\}$ is a martingale and $f$ is a convex function, then $\{f(X_n),n \in \N\}$ is a submartigale.
\end{lem}
\begin{proof}
The result is a direct consequence of Jensen's inequality.
\begin{align*}
\E[f(X_{n+1})|\F_n] &\geq f(\E[X_{n+1}|\F_n])=f(X_n).
\end{align*}
\end{proof}

\begin{con}
Let $X = \{X_n : n\in \N_0\}$ be a sub martingale. 
Let $a< b$ and $N_0  = -1$.
\begin{xalignat*}{3}
&N_{2k-1}=\inf\{m>N_{2k-2}:X_m\leq a \}, && N_{2k}=\inf\{m>N_{2k-1}:X_m\geq b \}.
\end{xalignat*}
The above quantities $N_{2k-1}$,$N_{2k}$ are stopping times and the set containing values of $m$ in the transition from $a$ to $b$ can be defined as
\begin{align*}
H_m &\triangleq \{N_{2k-1}<m\leq N_{2k} \}= %\{N_{2k-1} \leq m-1\} \cap \{m>N_{2k}\}^{c} = 
\{m-1\geq N_{2k-1}\}\cap\{m-1\geq N_{2k}\}^{c} \in \F_{m-1}.		
\end{align*}
Clearly, the event of $X$ being in an up crossing at time $m$ is  predictable. 
The number of up crossings completed in time $n$ is
\eq{
U_n &= \sum_{k=1}^nH_k = \sup\{k: n \geq N_{2k}\}.
}
%Since the above set depends on $\{m-1\}$ values instead of $\{m\}$ values, So
%\begin{align*}
%H_m=&1_{\{N_{2k-1}<m\leq  N_{2k}\}}\\
%U_n=&\sup\{k:N_{2k}\leq n\}
%\end{align*}
%$H_m$ defines a predictable sequence and $U_n$ is the number of up crossings completed in time $n$.
\end{con}
\begin{lem}[Upcrossing inequality]
If $X$ is a sub martingale, 
then for $Y_n \triangleq a+ (X_n-a)^{+}$, we have 
\begin{align*}
(b-a)\E U_n\leq& \E Y_n-\E Y_0.
%\text{where }Y_n&:=a+ (X_n-a)^{+}
\end{align*}
\end{lem}
\begin{proof}
Since $X$ is a submartingale so is $Y$, as $Y_n$ is a convex function of $X_n$. 
Since each up crossing has a gain slightly more than $b-a$, the following inequality exists, 
\begin{align*}
(b-a)U_n\leq &(H\cdot Y)_n = \sum_{m=1}^{n}1_{\{N_{2k-1}<m\leq N_{2k}\}}(Y_{m+1}-Y_{m}) = \sum_{k=1}^{U_n}(Y_{N_{2k+1}}-Y_{N_{2k+1}}).
\end{align*}
Now let $K_m=1-H_m$, then $K$ is a predictable sequence, and
\eq{
Y_n-Y_0 &= (H\cdot Y)_n+(K\cdot Y)_n.
}
From the submartingale property of Y, it follows
\eq{
\E[(K\cdot Y)_n] &\geq \E[(K\cdot Y)_0]=0. 
} 
Therefore, it follows that 
\eq{
\E(Y_n - Y_0) &= \E(H\cdot Y)_n+\E(K\cdot Y)_n \geq \E(H\cdot Y)_n  \geq  (b-a)\E U_n.
}
\end{proof}

\begin{thm}[Martingale convergence theorem] 
\label{MartingaleConvergenceTheorem} 
If $X$ is a submartingale with ${\sup}_{n\in \N} \E X_n^{+} < \infty$ then ${\lim}_{n\in \N} X_n=X$ a.s with $\E[X]<\infty$.
\end{thm}
\begin{proof} 
Since $(X-a)^{+}\leq X^{+}+|a|$, it follows from upcrossing inequality that   		
\begin{align*}
\E U_n \leq& \frac{\E X_n^{+} +|a|}{b-a}.
\end{align*}
The number of upcrossings $U_n$ increases with $n$, however the mean $\E U_n$ is bounded above for each $n \in \N$. 
Hence, $\lim_{n \in \N}\E U_n$ exists and is finite. 
    			$\lim_{n \in \N} U_n=U$ since, $\E [X_n^{+}] < \infty$ gives $U<\infty$ a.s. This conclusion leads to  
    			\begin{align*}
    			\Pr\{_{a,b \in \mathbb{Q}}{\cup}\{{\lim \inf}_{n\in \N}  X_n<a<b<{\lim \sup}_{n\in \N}  X_n \}\} = 0.
    			\end{align*}
    		  From the above probability we have a. s.
    		  \begin{align*}
    		  	{\lim \sup}_{n\in \N}  X_n &={\lim \inf}_{n\in \N}  X_n.
    		  	\end{align*}
    		  	Now the Fatou's lemma in measure theory guarantees
    		  	\begin {align*}
    		  	\E[X^{+}]\leq {\lim \inf}_{n\in \N}\E[X_n^{+}]<\infty,
    		  \end{align*}
    		  	which implies $X<\infty$  almost sure. To see $X>-\infty$, we observe that
    		  	\begin{align*}
    		  		\E[X_n^{-}]&=\E[X_n^{+}]-\E[X_n]\\
    		  		&\leq \E[X_n^{+}]-\E[X_0].
    		  		\end{align*} 
    		  		The above inequality comes from the submartingale property of $X_n$. Now from another application of Fatou's lemma gives,
    		  		\begin{align*}
    		  		\E[X^{-}]\leq {\lim \inf} _{n\in \N}\E[X_n^{-}] \leq {\sup}_{n\in \N}\E[X_n^{+}]-\E[X_0]<\infty.
    		  		\end{align*}
    		\end{proof}

%\begin{thm}[Martingale Convergence Theorem]
%\label{MartingaleConvergenceTheorem}
%If $\{Z_n,~n \geq 1\}$ is a martingale such that for some $M< \infty$
%\begin{align*}
%\E[|Z_n|] \leq M, ~ \text{for all}~ n
%\end{align*}
%then, with probability 1, $\lim_{n \rightarrow \infty}Z_n$ exists and is finite.
%\end{thm}
%\begin{proof}
%Assume $\E[Z_n^2]< \infty$ which is stronger than $\E[|Z_n|]< \infty$ (as a consequence of %Jensen's inequality). Observe that $\{Z_n^2\}$ is a submartingale (from Lemma %\ref{ConvexFuncSubmart}). Thus $\E[Z_n^2]<\infty$ and is non-decreasing in $n$. Thus, as $n %\rightarrow \infty$, $\E[Z_n^2]$ converges and let $\mu<\infty$ be given by $\mu=\lim_{n %\rightarrow \infty}\E[Z_n^2]$.
%\begin{equation}
%\label{KolmoBound}
%\Pr(\cup_{k \leq n} \{|Z_{m+k}-Z_m|> \epsilon\} )
%\end{equation}  
%\begin{align*}
%&\stackrel{(a)}{\leq }\E[(Z_{m+n}-Z_m)^2]/\epsilon^2
%&=\E[Z_{m+n}^2-2Z_mZ_{m+n}+Z_m^2]/\epsilon^2.
%\end{align*}
%Note that 
%\begin{align*}
%\E[Z_{m+n}Z_m]&=\E[\E[Z_mZ_{m+n}|Z_m]]\\
%&=\E[Z_m\E[Z_{m+n}|Z_m]]\\
%&=\E[Z_m^2].
%\end{align*}
%From \ref{KolmoBound}, 
%\begin{align*}
%\Pr(\cup_{k \leq n} \{|Z_{m+k}-Z_m|> \epsilon\}) \leq \frac{\E[Z_{m+n}^2]-\E[Z_m^2]}{\epsilon^2}.
%\end{align*}
%Letting $n \rightarrow \infty$
%\begin{align*}
%\Pr(\cup_{k \leq 1} \{|Z_{m+k}-Z_m|> \epsilon\}) \leq \frac{\mu-\E[Z_m^2]}{\epsilon^2}.
%\end{align*}
%Hence,
%\begin{align*}
%\Pr(\cup_{k \leq n} \{|Z_{m+k}-Z_m|> \epsilon\}) \rightarrow 0 ~\text{as}~ m \rightarrow \infty.
%\end{align*}
%Thus with probability 1, $\{Z_n\}$ will be  a Cauchy sequence, and thus $\lim_{n \rightarrow %\infty}Z_n$ will exist and be finite.`
%\end{proof}
%\begin{cor}
%If $\{Z_n,~m \geq 0\}$ is a non-negative martingale, then, with probability 1, $\lim_{n %\rightarrow \infty}Z_n$ exists and is finite.
%\end{cor}
%\begin{proof}
%Since $Z_n$ is non-negative,
%\begin{align*}
%\E[|Z_n|]=\E[Z_n]=\E[Z_1].
%\end{align*}
%\end{proof}